:toc:
:toclevels: 5
:hardbreaks-option:

== 分配与释放

=== 演进
==== 伙伴分配器
https://www.kernel.org/doc/gorman/html/understand/understand009.html

==== 对象分配器

===== SLAB
2.0
slab可以看作是内存对象池
https://en.wikipedia.org/wiki/Slab_allocation

===== SLOB(Simple List Of Blocks)
2.6.16
比较精简，适用于嵌入式小内存机器
a traditional K&R/UNIX allocator with a SLAB emulation layer
smaller code and is more memory efficient. 
like all similar allocators, it scales poorly and suffers from fragmentation more than SLAB, so it's only appropriate for small systems who want to save some memory
https://lwn.net/Articles/157944/

===== SLUB(The unqueued Slab allocator)
2.6.22
default

===== 注意事项
注意: slob从6.2版本起可能将要删除。
slob删除完之后，可能要删除slab, 但从目前来看，slab的删除可能会比较困难，因为有些公司某些场景下仍然需要使用slab。
https://lwn.net/Articles/918344/

==== 连续内存分配器(CMA)
3.5
Contiguous Memory Allocator
避免预留大块内存
https://lwn.net/Kernel/Index/#Contiguous_memory_allocator

==== 碎片问题
▪ 成块回收(Lumpy Reclaim)
2.6.23, 3.5移除(2012年7月)
回收是指MM在分配内存遇到内存紧张时, 会把一部分内存页面回收。
成块回收是尝回收目标回收页相邻的页面，以形成一块满足需求的高阶连续页块。
这种方法没有考虑被连带回收的页面可能是比较热的页面。

▪ Page Clustering by Page Mobility
2.6.23
所有使用的内存页有三种情形：
1.容易回收的(easily reclaimable): 这种页面可以在系统需要时回收，比如文件缓存页，们可以轻易的丢弃掉而不会有问题(有需要时再从后备文件系统中读取); 又比如一些生命周期短的内核使用的页，如DMA缓存区。
2.难回收的(non-reclaimable): 这种页面得内核主动释放，很难回收，内核使用的很多内存页就归为此类，比如为模块分配的区域，比如一些常驻内存的重要内核结构所占的页面。
3. 可移动的(movable): 用户空间分配的页面都属于这种类型，因为用户态的页地址是由页表翻译的，移动页后只要修改页表映射就可以(这也从另一面应证了内核态的页为什么不能移动，因为它们采取直接映射)。

因此, Mel Gorman修改了伙伴分配器和分配API, 使得在分配时告知伙伴分配器页面的可移动性: 回收时, 把相同移动性的页面聚类; 分配时, 根据移动性, 从相应的聚类中分配. 聚类的好处是, 结合上述的成块回收方案, 回收页面时，就能保证回收同一类型的; 或者在迁移页面时(migrate page), 就能移动可移动类型的页面，从而腾出连续的页面块，以满足高阶的连续物理页面分配。

▪ 内存紧致化(Memory Compaction)
2.6.35
不同于成块回收回收相临页面, 内存紧致化则是更彻底, 它在回收页面时被触发, 它会在一个zone里扫描, 把已分配的页记录下来, 然后把所有这些页移动到zone的一端, 这样这把一个可能已经七零八落的zone给紧致化成一段完全未分配的区间和一段已经分配的区间, 这样就又腾出大块连续的物理页面了。内存紧致化替代了成块回收, 使得成块回收在3.5中被移除。
注意: 内存紧致化与内存压缩不是一回事，内存压缩可以参考zram。

参考: https://www.zhihu.com/question/35484429/answer/62964898

==== OOM
支持比较早，待考证
https://www.cnblogs.com/MrLiuZF/p/15229868.html
https://www.kernel.org/doc/gorman/html/understand/understand016.html
https://blog.csdn.net/reliveIT/article/details/108432119
https://lwn.net/Kernel/Index/#OOM_killer

=== 用户空间

==== 堆栈
===== 堆栈大小
一般为8M，通过ulimit -s命令可查看和设置堆栈最大值，当程序使用的堆栈超过该值时, 发生栈溢出(Stack Overflow)，程序会收到一个段错误(Segmentation Fault)。

===== 堆栈扩展
堆栈的扩展一般不是用户显示发起的，而是入栈过程中动态扩展的。
堆栈区VMA默认是VM_GROWSDOWN向下扩展的。

进程在运行的过程中，通过不断向栈区压入数据，当超出栈区容量时，会触发缺页异常(page fault)。通过异常陷入内核态后，页面异常do_user_addr_fault()异常会调用expand_stack()，进而调用acct_stack_growth()来检查是否还有合适的地方用于栈的增长。

如果栈的大小低于RLIMIT_STACK(通常为8MB，可以通过ulimit -a查看)，那么一般情况下栈会被加长，程序继续执行，感觉不到发生了什么事情，这是一种将栈扩展到所需大小的常规机制。
否则，就会发生栈溢出(stack overflow)，进程将会收到内核发出的段错误(segmentation fault)信号。

动态栈增长是唯一一种访问未映射内存区域而被允许的情形，其他任何对未映射内存区域的访问都会触发页错误，从而导致段错误。一些被映射的区域是只读的，企图写这些区域也会导致段错误。

实现:
expand_stack(): https://elixir.bootlin.com/linux/latest/source/mm/mmap.c

===== 非主线程堆栈
allocate_stack(): https://elixir.bootlin.com/glibc/latest/source/nptl/allocatestack.c

	mem = __mmap (NULL, size, (guardsize == 0) ? prot : PROT_NONE,
		MAP_PRIVATE | MAP_ANONYMOUS | MAP_STACK, -1, 0);
    __mmap(): https://elixir.bootlin.com/glibc/latest/source/sysdeps/unix/sysv/linux/mmap.c

参考: https://blog.csdn.net/yangkuanqaz85988/article/details/52403726

==== 堆
===== malloc/free/calloc/realloc/reallocarray 
malloc(3)属于c库函数，会优先使用自己的内存池，只有在需要时才会使用系统调用brk/mmap，这样做既避免了系统调用的开销，又在上层尽量规避了内存碎片。

当申请小内存时，malloc使用brk分配内存；
当申请大内存时，使用mmap函数申请内存；
这个大小为M_MMAP_THRESHOLD，默认为128K，可以通过mallopt(3)来设置。
这只是分配了虚拟内存，还没有映射到物理内存；当访问申请的内存时，会发生缺页异常，此时内核为之分配物理内存。

Q: malloc一次最大能申请多大的内存？
https://www.zhihu.com/question/20836462

参考: https://man7.org/linux/man-pages/man3/malloc.3.html
参考: https://man7.org/linux/man-pages/man3/mallopt.3.html

===== brk/sbrk
link:./相关系统调用.asc#brk[系统调用brk]

    int brk(void *addr);
    void *sbrk(intptr_t increment);

brk()是系统调用函数

sbrk()是c库在brk()基础上实现的一个库函数:
https://elixir.bootlin.com/linux/latest/source/arch/um/include/shared/kern.h

注意事项：
尽量避免使用brk()与sbrk()，使用malloc(3)。

==== 映射

===== mmap/munmap
link:./相关系统调用.asc#mmap/munmap[系统调用mmap/munmap]

===== remap_pfn_range
外设物理地址映射到用户空间。
[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/memory.c
----
/**
 * remap_pfn_range - remap kernel memory to userspace
 * @vma: user vma to map to
 * @addr: target page aligned user address to start at
 * @pfn: page frame number of kernel physical memory address
 * @size: size of mapping area
 * @prot: page protection flags for this mapping
 *
 * Note: this is only safe if the mm semaphore is held when called.
 *
 * Return: %0 on success, negative error code otherwise.
 */
int remap_pfn_range(struct vm_area_struct *vma, unsigned long addr,
		    unsigned long pfn, unsigned long size, pgprot_t prot)
{
	int err;

	err = track_pfn_remap(vma, &prot, pfn, addr, PAGE_ALIGN(size));
	if (err)
		return -EINVAL;

	err = remap_pfn_range_notrack(vma, addr, pfn, size, prot);
	if (err)
		untrack_pfn(vma, pfn, PAGE_ALIGN(size));
	return err;
}
----

使用void vm_area_free(struct vm_area_struct *)释放。

=== 内核空间

==== 页分配与页释放
link:./页与页表.asc#页分配[页分配]
link:./页与页表.asc#页释放[页释放]

==== cma_alloc/cma_release
有些功能模块都需要预留大量连续内存，例如GPU，Camera，HDMI等，这部分内存平时不用，但是一般的做法是先预留着。通过CMA分配机制，可以做到不预留内存，这些内存平时是可用的，只有当需要的时候才被分配给GPU，Camera，HDMI等设备。

[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/cma.h
----
struct cma_kobject {
	struct kobject kobj;
	struct cma *cma;
};

struct cma {
	unsigned long   base_pfn;
	unsigned long   count;
	unsigned long   *bitmap;
	unsigned int order_per_bit; /* Order of pages represented by one bit */
	spinlock_t	lock;
#ifdef CONFIG_CMA_DEBUGFS
	struct hlist_head mem_head;
	spinlock_t mem_head_lock;
	struct debugfs_u32_array dfs_bitmap;
#endif
	char name[CMA_MAX_NAME];
#ifdef CONFIG_CMA_SYSFS
	/* the number of CMA page successful allocations */
	atomic64_t nr_pages_succeeded;
	/* the number of CMA page allocation failures */
	atomic64_t nr_pages_failed;
	/* kobject requires dynamic object */
	struct cma_kobject *cma_kobj;
#endif
	bool reserve_pages_on_error;
};

extern struct cma cma_areas[MAX_CMA_AREAS];
extern unsigned cma_area_count;
----
unsigned long   base_pfn: CMA区域物理地址的起始page frame number(页帧号)。
unsigned long   count: CMA区域的页面数量
unsigned long   *bitmap: CMA区域页面的分配情况，1表示已分配，0表示空闲。
unsigned int order_per_bit: bitmap中一个bit代表的页面数量

[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/cma.h
----
extern struct page *cma_alloc(struct cma *cma, unsigned long count, unsigned int align,
			      bool no_warn);
extern bool cma_pages_valid(struct cma *cma, const struct page *pages, unsigned long count);
extern bool cma_release(struct cma *cma, const struct page *pages, unsigned long count);
----

参考:
https://lwn.net/Kernel/Index/#Contiguous_memory_allocator
https://lwn.net/Articles/447405/
https://lwn.net/Articles/486301/
https://blog.51cto.com/u_15015138/2556751

==== slab
slab机制出现的背景:
1. 内核使用的对象种类繁多，应该采用一种统一的高效管理方法。
2. 内核对某些对象(如task_struct, inode)的使用是非常频繁的。
3. 充分重用已被释放的对象甚至使得下次分配时无需初始化，可以大幅提升性能。
4. 分配器对内核对象缓冲区的组织和管理必须充分考虑对硬件高速缓存，多处理器，NUMA等硬件特性。

• SLOB: K&R allocator (1991-1999)
• SLAB: Solaris type allocator (1999-2008)
• SLUB: Unqueued allocator (2008-today)

设计哲学:
• SLOB: As compact as possible
• SLAB: As cache friendly as possible. Benchmark friendly.
• SLUB: Simple and instruction cost counts. Superior Debugging. Defragmentation. Execution time friendly.

slab vs. slub
https://stackoverflow.com/questions/15470560/what-to-choose-between-slab-and-slub-allocator-in-linux-kernel

参考:
Christoph Lameter: SL[AUO]B: Kernel memory allocator design and philosophy: https://www.youtube.com/watch?v=h0VMLXavx30
https://events.static.linuxfound.org/sites/events/files/slides/slaballocators.pdf
https://events.static.linuxfound.org/images/stories/pdf/klf2012_kim.pdf
https://lwn.net/Articles/223411/
https://lwn.net/Articles/229096/
https://lwn.net/Articles/229984/

==== kmalloc/kfree
kmalloc是slab内存的申请接口，申请到的是kernel虚拟地址空间，虚拟地址与物理地址均连续，是线性映射，因此也不需要修改页表。

https://elixir.bootlin.com/linux/latest/source/include/linux/slab.h

    void *kmalloc(size_t size, gfp_t flags)
    void *kmalloc_node(size_t size, gfp_t flags, int node)
    void kfree(const void *objp)

==== vmalloc/vfree
vmalloc提供了内核虚拟地址连续但物理地址不一定连续的区域。
vmalloc从内核的虚存空间分配一块虚存以及相应的物理内存，vmalloc分配的空间不会被kswapd换出，kswapd只扫描各个进程的用户空间，通过vmalloc分配的页面表项对其不可见。

void *vmalloc(unsigned long size): https://elixir.bootlin.com/linux/latest/source/mm/vmalloc.c

	__vmalloc_node(size, 1, GFP_KERNEL, NUMA_NO_NODE, __builtin_return_address(0));
		__vmalloc_node_range(size, align, VMALLOC_START, VMALLOC_END, gfp_mask, PAGE_KERNEL, 0, node, caller);

[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/vmalloc.c
----
/**
 * __vmalloc_node_range - allocate virtually contiguous memory
 * @size:		  allocation size
 * @align:		  desired alignment
 * @start:		  vm area range start
 * @end:		  vm area range end
 * @gfp_mask:		  flags for the page level allocator
 * @prot:		  protection mask for the allocated pages
 * @vm_flags:		  additional vm area flags (e.g. %VM_NO_GUARD)
 * @node:		  node to use for allocation or NUMA_NO_NODE
 * @caller:		  caller's return address
 *
 * Allocate enough pages to cover @size from the page level
 * allocator with @gfp_mask flags. Please note that the full set of gfp
 * flags are not supported. GFP_KERNEL, GFP_NOFS and GFP_NOIO are all
 * supported.
 * Zone modifiers are not supported. From the reclaim modifiers
 * __GFP_DIRECT_RECLAIM is required (aka GFP_NOWAIT is not supported)
 * and only __GFP_NOFAIL is supported (i.e. __GFP_NORETRY and
 * __GFP_RETRY_MAYFAIL are not supported).
 *
 * __GFP_NOWARN can be used to suppress failures messages.
 *
 * Map them into contiguous kernel virtual space, using a pagetable
 * protection of @prot.
 *
 * Return: the address of the area or %NULL on failure
 */
void *__vmalloc_node_range(unsigned long size, unsigned long align,
			unsigned long start, unsigned long end, gfp_t gfp_mask,
			pgprot_t prot, unsigned long vm_flags, int node,
			const void *caller)
{
	struct vm_struct *area;
	void *ret;
	kasan_vmalloc_flags_t kasan_flags = KASAN_VMALLOC_NONE;
	unsigned long real_size = size;
	unsigned long real_align = align;
	unsigned int shift = PAGE_SHIFT;

	if (WARN_ON_ONCE(!size))
		return NULL;

	if ((size >> PAGE_SHIFT) > totalram_pages()) {
		warn_alloc(gfp_mask, NULL,
			"vmalloc error: size %lu, exceeds total pages",
			real_size);
		return NULL;
	}

	if (vmap_allow_huge && (vm_flags & VM_ALLOW_HUGE_VMAP)) {
		unsigned long size_per_node;

		/*
		 * Try huge pages. Only try for PAGE_KERNEL allocations,
		 * others like modules don't yet expect huge pages in
		 * their allocations due to apply_to_page_range not
		 * supporting them.
		 */

		size_per_node = size;
		if (node == NUMA_NO_NODE)
			size_per_node /= num_online_nodes();
		if (arch_vmap_pmd_supported(prot) && size_per_node >= PMD_SIZE)
			shift = PMD_SHIFT;
		else
			shift = arch_vmap_pte_supported_shift(size_per_node);

		align = max(real_align, 1UL << shift);
		size = ALIGN(real_size, 1UL << shift);
	}

again:
	area = __get_vm_area_node(real_size, align, shift, VM_ALLOC |
				  VM_UNINITIALIZED | vm_flags, start, end, node,
				  gfp_mask, caller);
	if (!area) {
		bool nofail = gfp_mask & __GFP_NOFAIL;
		warn_alloc(gfp_mask, NULL,
			"vmalloc error: size %lu, vm_struct allocation failed%s",
			real_size, (nofail) ? ". Retrying." : "");
		if (nofail) {
			schedule_timeout_uninterruptible(1);
			goto again;
		}
		goto fail;
	}

	/*
	 * Prepare arguments for __vmalloc_area_node() and
	 * kasan_unpoison_vmalloc().
	 */
	if (pgprot_val(prot) == pgprot_val(PAGE_KERNEL)) {
		if (kasan_hw_tags_enabled()) {
			/*
			 * Modify protection bits to allow tagging.
			 * This must be done before mapping.
			 */
			prot = arch_vmap_pgprot_tagged(prot);

			/*
			 * Skip page_alloc poisoning and zeroing for physical
			 * pages backing VM_ALLOC mapping. Memory is instead
			 * poisoned and zeroed by kasan_unpoison_vmalloc().
			 */
			gfp_mask |= __GFP_SKIP_KASAN_UNPOISON | __GFP_SKIP_ZERO;
		}

		/* Take note that the mapping is PAGE_KERNEL. */
		kasan_flags |= KASAN_VMALLOC_PROT_NORMAL;
	}

	/* Allocate physical pages and map them into vmalloc space. */
	ret = __vmalloc_area_node(area, gfp_mask, prot, shift, node);
	if (!ret)
		goto fail;

	/*
	 * Mark the pages as accessible, now that they are mapped.
	 * The condition for setting KASAN_VMALLOC_INIT should complement the
	 * one in post_alloc_hook() with regards to the __GFP_SKIP_ZERO check
	 * to make sure that memory is initialized under the same conditions.
	 * Tag-based KASAN modes only assign tags to normal non-executable
	 * allocations, see __kasan_unpoison_vmalloc().
	 */
	kasan_flags |= KASAN_VMALLOC_VM_ALLOC;
	if (!want_init_on_free() && want_init_on_alloc(gfp_mask) &&
	    (gfp_mask & __GFP_SKIP_ZERO))
		kasan_flags |= KASAN_VMALLOC_INIT;
	/* KASAN_VMALLOC_PROT_NORMAL already set if required. */
	area->addr = kasan_unpoison_vmalloc(area->addr, real_size, kasan_flags);

	/*
	 * In this function, newly allocated vm_struct has VM_UNINITIALIZED
	 * flag. It means that vm_struct is not fully initialized.
	 * Now, it is fully initialized, so remove this flag here.
	 */
	clear_vm_uninitialized_flag(area);

	size = PAGE_ALIGN(size);
	if (!(vm_flags & VM_DEFER_KMEMLEAK))
		kmemleak_vmalloc(area, size, gfp_mask);

	return area->addr;

fail:
	if (shift > PAGE_SHIFT) {
		shift = PAGE_SHIFT;
		align = real_align;
		size = real_size;
		goto again;
	}

	return NULL;
}
----

VMALLOC_START与VMALLOC_END:
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/pgtable_32_areas.h
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/pgtable_64_types.h

参考: https://www.cnblogs.com/LoyenWang/p/11965787.html

==== kvmalloc/kvfree
kvmalloc: 优先使用kmalloc分配，如果失败，则使用vmalloc进行分配。
kvfree: 如果是vmalloc分配的，调用vfree释放，否则使用kfree。
参考: https://lwn.net/Articles/711653/

[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/slab.h
----
extern void *kvmalloc_node(size_t size, gfp_t flags, int node) __alloc_size(1);
static inline __alloc_size(1) void *kvmalloc(size_t size, gfp_t flags)
{
	return kvmalloc_node(size, flags, NUMA_NO_NODE);
}
----

[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/util.c
----
/**
 * kvmalloc_node - attempt to allocate physically contiguous memory, but upon
 * failure, fall back to non-contiguous (vmalloc) allocation.
 * @size: size of the request.
 * @flags: gfp mask for the allocation - must be compatible (superset) with GFP_KERNEL.
 * @node: numa node to allocate from
 *
 * Uses kmalloc to get the memory but if the allocation fails then falls back
 * to the vmalloc allocator. Use kvfree for freeing the memory.
 *
 * GFP_NOWAIT and GFP_ATOMIC are not supported, neither is the __GFP_NORETRY modifier.
 * __GFP_RETRY_MAYFAIL is supported, and it should be used only if kmalloc is
 * preferable to the vmalloc fallback, due to visible performance drawbacks.
 *
 * Return: pointer to the allocated memory of %NULL in case of failure
 */
void *kvmalloc_node(size_t size, gfp_t flags, int node)
{
	gfp_t kmalloc_flags = flags;
	void *ret;

	/*
	 * We want to attempt a large physically contiguous block first because
	 * it is less likely to fragment multiple larger blocks and therefore
	 * contribute to a long term fragmentation less than vmalloc fallback.
	 * However make sure that larger requests are not too disruptive - no
	 * OOM killer and no allocation failure warnings as we have a fallback.
	 */
	if (size > PAGE_SIZE) {
		kmalloc_flags |= __GFP_NOWARN;

		if (!(kmalloc_flags & __GFP_RETRY_MAYFAIL))
			kmalloc_flags |= __GFP_NORETRY;

		/* nofail semantic is implemented by the vmalloc fallback */
		kmalloc_flags &= ~__GFP_NOFAIL;
	}

	ret = kmalloc_node(size, kmalloc_flags, node);

	/*
	 * It doesn't really make sense to fallback to vmalloc for sub page
	 * requests
	 */
	if (ret || size <= PAGE_SIZE)
		return ret;

	/* non-sleeping allocations are not supported by vmalloc */
	if (!gfpflags_allow_blocking(flags))
		return NULL;

	/* Don't even allow crazy sizes */
	if (unlikely(size > INT_MAX)) {
		WARN_ON_ONCE(!(flags & __GFP_NOWARN));
		return NULL;
	}

	/*
	 * kvmalloc() can always use VM_ALLOW_HUGE_VMAP,
	 * since the callers already cannot assume anything
	 * about the resulting pointer, and cannot play
	 * protection games.
	 */
	return __vmalloc_node_range(size, 1, VMALLOC_START, VMALLOC_END,
			flags, PAGE_KERNEL, VM_ALLOW_HUGE_VMAP,
			node, __builtin_return_address(0));
}
//...
void kvfree(const void *addr)
{
	if (is_vmalloc_addr(addr))
		vfree(addr);
	else
		kfree(addr);
}
----

==== vmap/vunmap
vmap: 在vmalloc虚拟地址空间中找到一个空闲区域，然后将page页面数组对应的物理内存映射到该区域，最终返回映射的虚拟起始地址。
vunmap: 从vmap_area_root/vmap_area_list中查找vmap_area区域，取消页表映射，再从vmap_area_root/vmap_area_list中删除掉vmap_area，页面返还给伙伴系统等。因为映射关系有改动，所以还需要进行TLB的刷新，而频繁的TLB刷新会降低性能，故需要延迟进行处理，即lazy TLB。

==== ioremap/iounmap
ioremap is used to map physical memory into virtual address space of the kernel. The return value of ioremap is a special virtual address that can be used to access the specified physical address range. ioremap is most useful for mapping the (physical) address of a PCI buffer to (virtual) kernel space.

参考:
https://lwn.net/Articles/653585/
https://www.quora.com/How-are-mmap-ioremap-and-kmap-different

==== kmap/kunmap
https://elixir.bootlin.com/linux/latest/source/include/linux/highmem.h

kmap(). This function has been deprecated; use kmap_local_page().

参考: https://lwn.net/Articles/836144/
参考: https://github.com/torvalds/linux/blob/master/Documentation/mm/highmem.rst

==== kmap_atomic/kunmap_atomic
https://elixir.bootlin.com/linux/latest/source/include/linux/highmem.h

kmap_atomic(). This function has been deprecated; use kmap_local_page().

参考: https://github.com/torvalds/linux/blob/master/Documentation/mm/highmem.rst

=== OOM
https://elixir.bootlin.com/linux/latest/source/mm/oom_kill.c

=== 小结

==== malloc/free系列函数

[format="csv", options="header", separator=|]
|===
function  |  区域	  |  连续性	         |      大小	   |  释放函数  |  优势
kmalloc	  | 内核空间  |  物理地址连续	  |  最大值128K-16  |	kfree	|  性能更佳
vmalloc	  | 内核空间  |  虚拟地址连续	  |  更大	        |   vfree	|  更易分配大内存
kvmalloc  | 内核空间  |  kmalloc+vmalloc |  更大           |   kvfree  |  兼具
malloc	  | 用户空间  |  虚拟地址连续	  |  更大	        |   free    |  用户空间
|===

==== map/unmap系列函数

=== 参考
https://www.cnblogs.com/arnoldlu/p/8251333.html
https://mp.weixin.qq.com/s/0Ss_A1aeoca2n-KENOL0mA
https://linux-kernel-labs.github.io/refs/heads/master/labs/memory_mapping.html