:toc:
:toclevels: 6
:hardbreaks-option:

== 页与页表

=== 概念
==== 页面种类
▪ 虚拟页(Vitual Page)
    
    虚拟空间中的页

▪ 物理页(Physical Page)
    
    物理内存中的页

▪ 磁盘页(Disk Page)

    磁盘上的页

注意: 磁盘页也可以认为是一种物理页，实际上除了磁盘页，其它的物理介质也可能存在物理页。

在交换或映射时发生转换。

▪ 用户空间页面

    ▪ 普通用户空间页面，例如进程的代码段，数据段，堆栈段以及堆
    ▪ 通过系统调用mmap()映射到用户空间的内容
    ▪ 进程间的共享内存区

▪ 系统空间页面

    ▪ 不会被换出，但是会回收，周转
    ▪ 一类是使用完毕可以立即释放回，另一类是使用完毕经过一段时间条件允许才会回收

==== 页面大小
一般来说4K
4M大小: https://en.wikipedia.org/wiki/Page_Size_Extension

过小的页面大小会带来较多的页表项从而增加寻址的查找时间和额外开销
过大的页面大小会更容易造成内存碎片，降低内存的利用率

==== 为什么需要分页
内存管理的几大需求: 隔离 效率 便利

分段没有解决效率的问题，也不够灵活便利:
▪ 分段大小更大，容易产生碎片
▪ 分段大小不固定，不利于磁盘的换入换出
▪ 分页则用大小相同的更小的页取代了大小不同的更大的段

==== 为什么需要分级
多级分页是为了节省物理内存，其代价是寻址的时候需要多次转换，会稍微慢一点

==== 需要几级分页
linux内核最开始是2级分页，之后为了支持PAE(Physical Address Extention, 2.3.23, 32bit扩展为36bit支持64G物理内存)扩展为3级，再之后为了更好的支持64位CPU, 2.6.11扩展为四级的通用页表。

=== 演进
==== 页表管理
▪ 四级页表
2.6.11
https://lwn.net/Articles/106177/

▪ 延迟页表缓存冲刷(Lazy-TLB flushing)

==== 页面预读
▪ 原始预读
内核发现可能在进行顺序读操作时, 把后面的128KB的页面也读进来。

▪ 按需预读(On-demand Readahead)
2.6.23

如果内存紧张, 预读其实是浪费预读的页面可能还没被访问就被踢出去了。
如果进程频繁且内存也足够宽裕, 128KB显得不够。
按需预读要考虑这些情况，它采用一种启发式的算法，决定预读窗口的大小和哪一页做为异步预读的开始。
对于两个进程在同一个文件上的交替预读, 2.6.24增强了该算法, 使其能很好地侦测这一行为。

==== 页面回收
▪ 改进的LRU算法
2.6前
经典的LRU算法没能体现页面的使用频率。
为此Linux引入了两个链表: active list和inactive list

▪ active与inactive链表拆分
2.6.28

▪ 拆分出被锁页的链表
2.6.28

▪ 代码文件缓存页优化
2.6.31

▪ 工作集大小探测
3.15

==== 页面写回

==== 大内存页(Huge Page)
▪ 作用

    减少页表(Page Table)大小
    由于页表数量的减少，使得CPU中的TLB(可理解为CPU对页表的CACHE)的命中率大大提高
    Huge Page内存只能锁定在物理内存中，不能被交换到交换区，这样避免了交换引起的性能影响

▪ HUGETLB
https://www.kernel.org/doc/html/latest/admin-guide/mm/hugetlbpage.html
https://lwn.net/Articles/375098/

▪ 透明大页(Transparent Huge Pages)
2.6.38
缺页中断发生时, 内核会尝试分配一个大页。

=== 页表
▪ 页表用于建立用户进程的虚拟地址空间和物理内存之间的关联
▪ 内核内存管理总是假定使用四级页表，而不管底层处理器是否如此, 例如IA-32默认情况下只使用两级分页系统(不使用PAE扩展的情况下), 此时第三和第四级页表必须由特定于体系结构的代码模拟

https://www.kernel.org/doc/html/latest/x86/x86_64/mm.html

https://elixir.bootlin.com/linux/latest/source/include/asm-generic/page.h
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/pgtable_64.h

▪ PGD PUD PMD PTE OFFSET:

    ▪ pgd_t用于全局页目录项(Page Global Directory)
    ▪ pud_t用于上层页目录项(Page Upper Directory)
    ▪ pmd_t用于中间页目录项(Page Middle Directory)
    ▪ pte_t用于直接页表项(Page Table Entry)
    ▪ PTE的相关信息: 例如_PAGE_DIRTY等
    https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/pgtable_types.h

▪ CR3 register
CR3 register contains the physical address of the base address of the page directory table. This value is unique for each running process, since every process has it’s own page table.

▪ 创建与释放
例如pgd_alloc
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/pgalloc.h

▪ swapper_pg_dir
内核维持着一组自己使用的页表，即主内核页全局目录。当内核在初始化完成后，其存放在swapper_pg_dir中，之后在init进程启动后就成了idle内核线程的页目录指针。
不只是为idle进程指示内存映射信息，同时作为一个内核空间的内存映射模板而存在，任何进程在内核空间就不分彼此了，所有的进程都会共用一份内核空间的内存映射。
参考: https://www.kernel.org/doc/Documentation/arm64/memory.rst

==== 参考
https://cs61.seas.harvard.edu/site/2018/Kernel3/

=== 分配页
▪ alloc_pages(): struct page *alloc_pages(gfp_t gfp, unsigned order)
https://elixir.bootlin.com/linux/latest/source/include/linux/gfp.h
https://elixir.bootlin.com/linux/latest/source/mm/mempolicy.c

    __alloc_pages(): https://elixir.bootlin.com/linux/latest/source/mm/page_alloc.c  伙伴分配算法核心函数
        get_page_from_freelist(): https://elixir.bootlin.com/linux/latest/source/mm/page_alloc.c

▪ 分配掩码: ___GFP_DMA等  gfp(GFP)代表get_free_pages

▪ https://elixir.bootlin.com/linux/latest/source/include/linux/gfp.h
▪ 进程中也有影响分配的一些标志: 例如PF_MEMALLOC等 https://elixir.bootlin.com/linux/latest/source/include/linux/sched.h

=== 释放页
▪ free_page函数
▪ https://elixir.bootlin.com/linux/latest/source/include/linux/gfp.h

=== 交换
利用磁盘空间作为扩展内存，从而增大了可用的内存。

▪ 命令:

	vmstat
		si: Amount of memory swapped in from disk (/s).
		so: Amount of memory swapped to disk (/s).

注意，不是所有的内存页面都是可以交换出去的:
只有映射到用户空间的页面才会被换出，内核即系统空间的页面则不会。

[source,c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/swap.h
----
/*
 * The in-memory structure used to track swap areas.
 */
struct swap_info_struct {
	struct percpu_ref users;	/* indicate and keep swap device valid. */
	unsigned long	flags;		/* SWP_USED etc: see above */
	signed short	prio;		/* swap priority of this type */
	struct plist_node list;		/* entry in swap_active_head */
	signed char	type;		/* strange name for an index */
	unsigned int	max;		/* extent of the swap_map */
	unsigned char *swap_map;	/* vmalloc'ed array of usage counts */
	struct swap_cluster_info *cluster_info; /* cluster info. Only for SSD */
	struct swap_cluster_list free_clusters; /* free clusters list */
	unsigned int lowest_bit;	/* index of first free in swap_map */
	unsigned int highest_bit;	/* index of last free in swap_map */
	unsigned int pages;		/* total of usable pages of swap */
	unsigned int inuse_pages;	/* number of those currently in use */
	unsigned int cluster_next;	/* likely index for next allocation */
	unsigned int cluster_nr;	/* countdown to next cluster search */
	unsigned int __percpu *cluster_next_cpu; /*percpu index for next allocation */
	struct percpu_cluster __percpu *percpu_cluster; /* per cpu's swap location */
	struct rb_root swap_extent_root;/* root of the swap extent rbtree */
	struct block_device *bdev;	/* swap device or bdev of swap file */
	struct file *swap_file;		/* seldom referenced */
	unsigned int old_block_size;	/* seldom referenced */
	struct completion comp;		/* seldom referenced */
#ifdef CONFIG_FRONTSWAP
	unsigned long *frontswap_map;	/* frontswap in-use, one bit per page */
	atomic_t frontswap_pages;	/* frontswap pages in-use counter */
#endif
	spinlock_t lock;		/*
					 * protect map scan related fields like
					 * swap_map, lowest_bit, highest_bit,
					 * inuse_pages, cluster_next,
					 * cluster_nr, lowest_alloc,
					 * highest_alloc, free/discard cluster
					 * list. other fields are only changed
					 * at swapon/swapoff, so are protected
					 * by swap_lock. changing flags need
					 * hold this lock and swap_lock. If
					 * both locks need hold, hold swap_lock
					 * first.
					 */
	spinlock_t cont_lock;		/*
					 * protect swap count continuation page
					 * list.
					 */
	struct work_struct discard_work; /* discard worker */
	struct swap_cluster_list discard_clusters; /* discard clusters list */
	struct plist_node avail_lists[]; /*
					   * entries in swap_avail_heads, one
					   * entry per node.
					   * Must be last as the number of the
					   * array is nr_node_ids, which is not
					   * a fixed value so have to allocate
					   * dynamically.
					   * And it has to be an array so that
					   * plist_for_each_* can work.
					   */
};
----

[source,c]
.https://elixir.bootlin.com/linux/latest/source/mm/swapfile.c
----
struct swap_info_struct *swap_info[MAX_SWAPFILES];
----

https://elixir.bootlin.com/linux/latest/source/mm/swap.c

==== 定期换出
避免在缺页异常的时候，临时搜索可供换出的页面并加以换出，因此有必要定期检查并预先将某些页面换出以便腾出空间，从而减少缺页异常发生时的处理负担。

承担这一任务的就是kswapd内核线程:
kswapd_init(): https://elixir.bootlin.com/linux/latest/source/mm/vmscan.c
    kswapd_run()

[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/vmscan.c
----
/*
 * The background pageout daemon, started as a kernel thread
 * from the init process.
 *
 * This basically trickles out pages so that we have _some_
 * free memory available even if there is no other activity
 * that frees anything up. This is needed for things like routing
 * etc, where we otherwise might have all activity going on in
 * asynchronous contexts that cannot page things out.
 *
 * If there are applications that are active memory-allocators
 * (most normal use), this basically shouldn't matter.
 */
static int kswapd(void *p)
{
	unsigned int alloc_order, reclaim_order;
	unsigned int highest_zoneidx = MAX_NR_ZONES - 1;
	pg_data_t *pgdat = (pg_data_t *)p;
	struct task_struct *tsk = current;
	const struct cpumask *cpumask = cpumask_of_node(pgdat->node_id);

	if (!cpumask_empty(cpumask))
		set_cpus_allowed_ptr(tsk, cpumask);

	/*
	 * Tell the memory management that we're a "memory allocator",
	 * and that if we need more memory we should get access to it
	 * regardless (see "__alloc_pages()"). "kswapd" should
	 * never get caught in the normal page freeing logic.
	 *
	 * (Kswapd normally doesn't need memory anyway, but sometimes
	 * you need a small amount of memory in order to be able to
	 * page out something else, and this flag essentially protects
	 * us from recursively trying to free more memory as we're
	 * trying to free the first piece of memory in the first place).
	 */
	tsk->flags |= PF_MEMALLOC | PF_KSWAPD;
	set_freezable();

	WRITE_ONCE(pgdat->kswapd_order, 0);
	WRITE_ONCE(pgdat->kswapd_highest_zoneidx, MAX_NR_ZONES);
	atomic_set(&pgdat->nr_writeback_throttled, 0);
	for ( ; ; ) {
		bool ret;

		alloc_order = reclaim_order = READ_ONCE(pgdat->kswapd_order);
		highest_zoneidx = kswapd_highest_zoneidx(pgdat,
							highest_zoneidx);

kswapd_try_sleep:
		kswapd_try_to_sleep(pgdat, alloc_order, reclaim_order,
					highest_zoneidx);

		/* Read the new order and highest_zoneidx */
		alloc_order = READ_ONCE(pgdat->kswapd_order);
		highest_zoneidx = kswapd_highest_zoneidx(pgdat,
							highest_zoneidx);
		WRITE_ONCE(pgdat->kswapd_order, 0);
		WRITE_ONCE(pgdat->kswapd_highest_zoneidx, MAX_NR_ZONES);

		ret = try_to_freeze();
		if (kthread_should_stop())
			break;

		/*
		 * We can speed up thawing tasks if we don't call balance_pgdat
		 * after returning from the refrigerator
		 */
		if (ret)
			continue;

		/*
		 * Reclaim begins at the requested order but if a high-order
		 * reclaim fails then kswapd falls back to reclaiming for
		 * order-0. If that happens, kswapd will consider sleeping
		 * for the order it finished reclaiming at (reclaim_order)
		 * but kcompactd is woken to compact for the original
		 * request (alloc_order).
		 */
		trace_mm_vmscan_kswapd_wake(pgdat->node_id, highest_zoneidx,
						alloc_order);
		reclaim_order = balance_pgdat(pgdat, alloc_order,
						highest_zoneidx);
		if (reclaim_order < alloc_order)
			goto kswapd_try_sleep;
	}

	tsk->flags &= ~(PF_MEMALLOC | PF_KSWAPD);

	return 0;
}
----

休眠: kswapd_try_to_sleep()
休眠时机:

唤醒: wakeup_kswapd()
唤醒时机:

[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/swap.c
----
/* How many pages do we try to swap or page in/out together? */
int page_cluster;
----

参考: https://zhuanlan.zhihu.com/p/574462887

==== 换入

===== page falut(缺页异常)
▪ 发生page falut(缺页异常)的时机

    ▪ page table中找不到对应的PTE
        ▪ 无效地址(通过地址的addr来找vma，如果没找到说明地址无效，段错误，内核panic掉/相应的页面目录或页表项为空即线性地址与物理地址尚未建立映射关系，或者已经撤销)
        ▪ 有效地址但是没有载入主存(对应的页面不在内存中)
            ▪ 首次访问，发生调页。
            ▪ 如果当前page的present=0，说明不在主存中被swap out了，需要从外存调入主存。
            ▪ COW时访问语义冲突，比如PTE不可写，但是做了写操作，会触发COW机制，在copy page中write
    ▪ 对应虚拟地址的PTE拒绝访问(页面权限不符，例如试图写一个只读页面)

参考: https://bbs.pediy.com/thread-269149.htm

====== x86
DEFINE_IDTENTRY_RAW_ERRORCODE(exc_page_fault): https://elixir.bootlin.com/linux/latest/source/arch/x86/mm/fault.c

	handle_page_fault(regs, error_code, address);
        do_kern_addr_fault(regs, error_code, address);
		do_user_addr_fault(regs, error_code, address);

    do_kern_addr_fault(): https://elixir.bootlin.com/linux/latest/source/arch/x86/mm/fault.c

    do_user_addr_fault(): https://elixir.bootlin.com/linux/latest/source/arch/x86/mm/fault.c
        handle_mm_fault(): https://elixir.bootlin.com/linux/latest/source/mm/memory.c
            __handle_mm_fault()
                handle_pte_fault()
                    do_swap_page()

===== 流程

	do_swap_page(): https://elixir.bootlin.com/linux/latest/source/mm/memory.c
		swapin_readahead(): https://elixir.bootlin.com/linux/latest/source/mm/swap_state.c

==== 页面抖动(pagethrashing)

=== 回收
参考: https://zhuanlan.zhihu.com/p/480428225

=== 回写
用于将内存映射被修改的内容与底层的块设备同步，也称为数据回写。

=== 页帧迁移

==== 迁移属性

==== 自动NUMA平衡

=== 大页
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/pgtable.h
pte_mkhuge(): 4M
pmd_mkhuge(): 1G
参考: https://zhuanlan.zhihu.com/p/503738975

=== 参考
https://www.kernel.org/doc/gorman/html/understand/understand006.html
https://puqiyuan.github.io/kernel/mm/mm_series/ptm.html
