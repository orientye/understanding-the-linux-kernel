:toc:
:toclevels: 6
:hardbreaks-option:

== 页与页表

=== 概念
==== 页面种类
▪ 虚拟页(Vitual Page)
    
    虚拟空间中的页

▪ 物理页(Physical Page)
    
    物理内存中的页

▪ 磁盘页(Disk Page)

    磁盘上的页

注意: 磁盘页也可以认为是一种物理页，实际上除了磁盘页，其它的物理介质也可能存在物理页。

在交换或映射时发生转换。

▪ 用户空间页面

    ▪ 普通用户空间页面，例如进程的代码段，数据段，堆栈段以及堆
    ▪ 通过系统调用mmap()映射到用户空间的内容
    ▪ 进程间的共享内存区

▪ 系统空间页面

    ▪ 不会被换出，但是会回收，周转
    ▪ 一类是使用完毕可以立即释放回收，另一类是使用完毕经过一段时间满足条件才会回收

==== 页面大小
一般来说4K
4M大小: https://en.wikipedia.org/wiki/Page_Size_Extension

过小的页面大小会带来较多的页表项从而增加寻址的查找时间和额外开销
过大的页面大小会更容易造成内存碎片，降低内存的利用率

==== 为什么需要分页
内存管理的几大需求: 隔离 效率 便利

分段没有解决效率的问题，也不够灵活便利:
▪ 分段大小更大，容易产生碎片
▪ 分段大小不固定，不利于磁盘的换入换出
▪ 分页则用大小相同的更小的页取代了大小不同的更大的段

==== 为什么需要分级
多级分页是为了节省物理内存，其代价是寻址的时候需要多次转换，会稍微慢一点

==== 需要几级分页
linux内核最开始是2级分页，之后为了支持PAE(Physical Address Extention, 2.3.23, 32bit扩展为36bit支持64G物理内存)扩展为3级，再之后为了更好的支持64位CPU, 2.6.11扩展为四级的通用页表。

=== 演进
==== 页表管理
▪ 四级页表
2.6.11
https://lwn.net/Articles/106177/

▪ 延迟页表缓存冲刷(Lazy-TLB flushing)

==== 页面预读
▪ 原始预读
内核发现可能在进行顺序读操作时, 把后面的128KB的页面也读进来。

▪ 按需预读(On-demand Readahead)
2.6.23

如果内存紧张, 预读其实是浪费预读的页面可能还没被访问就被踢出去了。
如果进程频繁且内存也足够宽裕, 128KB显得不够。
按需预读要考虑这些情况，它采用一种启发式的算法，决定预读窗口的大小和哪一页做为异步预读的开始。
对于两个进程在同一个文件上的交替预读, 2.6.24增强了该算法, 使其能很好地侦测这一行为。

==== 页面回收
▪ 改进的LRU算法
2.6前
经典的LRU算法没能体现页面的使用频率。
为此Linux引入了两个链表: active list和inactive list

▪ active与inactive链表拆分
2.6.28

▪ 拆分出被锁页的链表
2.6.28

▪ 代码文件缓存页优化
2.6.31

▪ 工作集大小探测
3.15

==== 页面写回

==== 大内存页(Huge Page)
▪ 作用

    减少页表(Page Table)大小
    由于页表数量的减少，使得CPU中的TLB(可理解为CPU对页表的CACHE)的命中率大大提高
    Huge Page内存只能锁定在物理内存中，不能被交换到交换区，这样避免了交换引起的性能影响

▪ HUGETLB
https://www.kernel.org/doc/html/latest/admin-guide/mm/hugetlbpage.html
https://lwn.net/Articles/375098/

▪ 透明大页(Transparent Huge Pages)
2.6.38
缺页中断发生时, 内核会尝试分配一个大页。

=== 页表
▪ 页表用于建立用户进程的虚拟地址空间和物理内存之间的关联
▪ 内核内存管理总是假定使用四级页表，而不管底层处理器是否如此, 例如IA-32默认情况下只使用两级分页系统(不使用PAE扩展的情况下), 此时第三和第四级页表必须由特定于体系结构的代码模拟

https://www.kernel.org/doc/html/latest/x86/x86_64/mm.html

https://elixir.bootlin.com/linux/latest/source/include/asm-generic/page.h
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/pgtable_64.h

▪ PGD PUD PMD PTE OFFSET:

    ▪ pgd_t用于全局页目录项(Page Global Directory)
    ▪ pud_t用于上层页目录项(Page Upper Directory)
    ▪ pmd_t用于中间页目录项(Page Middle Directory)
    ▪ pte_t用于直接页表项(Page Table Entry)
    ▪ PTE的相关信息: 例如_PAGE_DIRTY等
    https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/pgtable_types.h

▪ CR3 register
CR3 register contains the physical address of the base address of the page directory table. This value is unique for each running process, since every process has it’s own page table.

▪ 创建与释放
例如pgd_alloc
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/pgalloc.h

▪ swapper_pg_dir
内核维持着一组自己使用的页表，即主内核页全局目录。当内核在初始化完成后，其存放在swapper_pg_dir中，之后在init进程启动后就成了idle内核线程的页目录指针。
不只是为idle进程指示内存映射信息，同时作为一个内核空间的内存映射模板而存在，任何进程在内核空间就不分彼此了，所有的进程都会共用一份内核空间的内存映射。
参考: https://www.kernel.org/doc/Documentation/arm64/memory.rst

==== 参考
https://cs61.seas.harvard.edu/site/2018/Kernel3/

=== 分配页
==== alloc_pages
▪ 有许多用于分配页面的函数
▪ 最终都会调用struct page *alloc_pages(gfp_t gfp, unsigned order)
▪ alloc_pages内部进而调用__alloc_pages() //伙伴分配算法核心函数
▪ 参数: gfp表示采用哪一种分配策略，order表示所需物理块的大小为2的多少次个页面。

▪ alloc_pages(): struct page *alloc_pages(gfp_t gfp, unsigned order)
https://elixir.bootlin.com/linux/latest/source/include/linux/gfp.h //如果没有定义CONFIG_NUMA
https://elixir.bootlin.com/linux/latest/source/mm/mempolicy.c      //Simple NUMA memory policy for the Linux kernel

==== __alloc_pages
[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/page_alloc.c
----
/*
 * This is the 'heart' of the zoned buddy allocator.
 */
struct page *__alloc_pages(gfp_t gfp, unsigned int order, int preferred_nid,
							nodemask_t *nodemask)
{
	struct page *page;
	unsigned int alloc_flags = ALLOC_WMARK_LOW;
	gfp_t alloc_gfp; /* The gfp_t that was actually used for allocation */
	struct alloc_context ac = { };

	/*
	 * There are several places where we assume that the order value is sane
	 * so bail out early if the request is out of bound.
	 */
	if (WARN_ON_ONCE_GFP(order >= MAX_ORDER, gfp))
		return NULL;

	gfp &= gfp_allowed_mask;
	/*
	 * Apply scoped allocation constraints. This is mainly about GFP_NOFS
	 * resp. GFP_NOIO which has to be inherited for all allocation requests
	 * from a particular context which has been marked by
	 * memalloc_no{fs,io}_{save,restore}. And PF_MEMALLOC_PIN which ensures
	 * movable zones are not used during allocation.
	 */
	gfp = current_gfp_context(gfp);
	alloc_gfp = gfp;
	if (!prepare_alloc_pages(gfp, order, preferred_nid, nodemask, &ac,
			&alloc_gfp, &alloc_flags))
		return NULL;

	/*
	 * Forbid the first pass from falling back to types that fragment
	 * memory until all local zones are considered.
	 */
	alloc_flags |= alloc_flags_nofragment(ac.preferred_zoneref->zone, gfp);

	/* First allocation attempt */
	page = get_page_from_freelist(alloc_gfp, order, alloc_flags, &ac);
	if (likely(page))
		goto out;

	alloc_gfp = gfp;
	ac.spread_dirty_pages = false;

	/*
	 * Restore the original nodemask if it was potentially replaced with
	 * &cpuset_current_mems_allowed to optimize the fast-path attempt.
	 */
	ac.nodemask = nodemask;

	page = __alloc_pages_slowpath(alloc_gfp, order, &ac);

out:
	if (memcg_kmem_enabled() && (gfp & __GFP_ACCOUNT) && page &&
	    unlikely(__memcg_kmem_charge_page(page, gfp, order) != 0)) {
		__free_pages(page, order);
		page = NULL;
	}

	trace_mm_page_alloc(page, order, alloc_gfp, ac.migratetype);
	kmsan_alloc_page(page, order, alloc_gfp);

	return page;
}
----

如果get_page_from_freelist()能分配成功，则返回结果；
否则调用__alloc_pages_slowpath()。

整体流程:
需要保证申请的order不能超过伙伴系统预设的范围[0, MAX_ORDER - 1]（通常是[0, 10]）,如果超过了，则直接返回NULL。
填充alloc_context。作为内存分配的上下文，其记录了内存分配的行为，包括分配标记，order、node id，期待zone等。
通过gfp_mask获得可分配最大zone下标、zonelist、页面迁移类型等。
如果系统开启了cpuset配置，则根据配置决定本次申请应该在哪个node上进行。
如果本次分配gfp_flags标记了__GFP_WRITE，表示本次是为了分配可写的页面，则需要做脏页平衡。所谓脏页平衡是说如果一个zone的脏页数量超过预设的阈值，则本次不在该zone上分配。
注意：极少场景会出现组装失败的情况，至于什么场景，目前还不得而知...
通过快路径尝试分配内存。为了凸显“快”的特点，这种方式分配内存会尽可能减少内存整理或者回收回收，理想的手段是从pcplist或者free_area中直接获取所需的连续内存块。
从期待zone开始，遍历系统中所有的zone，找到一个能满足分配连续内存的zone。所谓能满足分配，指的是分配完连续内存后，zone还能满足某个水位线的要求，具体需要满足哪个水位线，则根据gfp_flags的设置而定。同时还要考虑cpuset、脏页平衡等因素。
如果找不到合适的zone，则尝试执行快速的内存回收方式来获得内存，最终还是需要根据水位情况来判定该zone是否合适。
如果能找到合适的zone，需要区分两种场景：本次只需要分配1页，则在pcplist上进行分配；如果本次需要分配超过1页，则从free_area中获取。如果最终该zone无法申请到足够的连续内存，尝试从其他迁移类型中“偷”取足够的连续内存块。
上述快路径已经无计可施了，只能进入慢路径的方式了。这里说明下，所谓慢路径的方式只是多了一些内存碎片整理、内存回收、OOM等耗时动作，而这些耗时动作都只是为了让伙伴系统获得足够的空闲内存，最终还是通过快路径的方式来分配内存的。
参考: https://zhuanlan.zhihu.com/p/468892596

===== get_page_from_freelist
▪ get_page_from_freelist()
[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/page_alloc.c
----
/*
 * get_page_from_freelist goes through the zonelist trying to allocate
 * a page.
 */
static struct page *
get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,
						const struct alloc_context *ac)
{
	struct zoneref *z;
	struct zone *zone;
	struct pglist_data *last_pgdat = NULL;
	bool last_pgdat_dirty_ok = false;
	bool no_fallback;

retry:
	/*
	 * Scan zonelist, looking for a zone with enough free.
	 * See also __cpuset_node_allowed() comment in kernel/cgroup/cpuset.c.
	 */
	no_fallback = alloc_flags & ALLOC_NOFRAGMENT;
	z = ac->preferred_zoneref;
	for_next_zone_zonelist_nodemask(zone, z, ac->highest_zoneidx,
					ac->nodemask) {
		struct page *page;
		unsigned long mark;

		if (cpusets_enabled() &&
			(alloc_flags & ALLOC_CPUSET) &&
			!__cpuset_zone_allowed(zone, gfp_mask))
				continue;
		/*
		 * When allocating a page cache page for writing, we
		 * want to get it from a node that is within its dirty
		 * limit, such that no single node holds more than its
		 * proportional share of globally allowed dirty pages.
		 * The dirty limits take into account the node's
		 * lowmem reserves and high watermark so that kswapd
		 * should be able to balance it without having to
		 * write pages from its LRU list.
		 *
		 * XXX: For now, allow allocations to potentially
		 * exceed the per-node dirty limit in the slowpath
		 * (spread_dirty_pages unset) before going into reclaim,
		 * which is important when on a NUMA setup the allowed
		 * nodes are together not big enough to reach the
		 * global limit.  The proper fix for these situations
		 * will require awareness of nodes in the
		 * dirty-throttling and the flusher threads.
		 */
		if (ac->spread_dirty_pages) {
			if (last_pgdat != zone->zone_pgdat) {
				last_pgdat = zone->zone_pgdat;
				last_pgdat_dirty_ok = node_dirty_ok(zone->zone_pgdat);
			}

			if (!last_pgdat_dirty_ok)
				continue;
		}

		if (no_fallback && nr_online_nodes > 1 &&
		    zone != ac->preferred_zoneref->zone) {
			int local_nid;

			/*
			 * If moving to a remote node, retry but allow
			 * fragmenting fallbacks. Locality is more important
			 * than fragmentation avoidance.
			 */
			local_nid = zone_to_nid(ac->preferred_zoneref->zone);
			if (zone_to_nid(zone) != local_nid) {
				alloc_flags &= ~ALLOC_NOFRAGMENT;
				goto retry;
			}
		}

		mark = wmark_pages(zone, alloc_flags & ALLOC_WMARK_MASK);
		if (!zone_watermark_fast(zone, order, mark,
				       ac->highest_zoneidx, alloc_flags,
				       gfp_mask)) {
			int ret;

#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
			/*
			 * Watermark failed for this zone, but see if we can
			 * grow this zone if it contains deferred pages.
			 */
			if (static_branch_unlikely(&deferred_pages)) {
				if (_deferred_grow_zone(zone, order))
					goto try_this_zone;
			}
#endif
			/* Checked here to keep the fast path fast */
			BUILD_BUG_ON(ALLOC_NO_WATERMARKS < NR_WMARK);
			if (alloc_flags & ALLOC_NO_WATERMARKS)
				goto try_this_zone;

			if (!node_reclaim_enabled() ||
			    !zone_allows_reclaim(ac->preferred_zoneref->zone, zone))
				continue;

			ret = node_reclaim(zone->zone_pgdat, gfp_mask, order);
			switch (ret) {
			case NODE_RECLAIM_NOSCAN:
				/* did not scan */
				continue;
			case NODE_RECLAIM_FULL:
				/* scanned but unreclaimable */
				continue;
			default:
				/* did we reclaim enough */
				if (zone_watermark_ok(zone, order, mark,
					ac->highest_zoneidx, alloc_flags))
					goto try_this_zone;

				continue;
			}
		}

try_this_zone:
		page = rmqueue(ac->preferred_zoneref->zone, zone, order,
				gfp_mask, alloc_flags, ac->migratetype);
		if (page) {
			prep_new_page(page, order, gfp_mask, alloc_flags);

			/*
			 * If this is a high-order atomic allocation then check
			 * if the pageblock should be reserved for the future
			 */
			if (unlikely(order && (alloc_flags & ALLOC_HARDER)))
				reserve_highatomic_pageblock(page, zone, order);

			return page;
		} else {
#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
			/* Try again if zone has deferred pages */
			if (static_branch_unlikely(&deferred_pages)) {
				if (_deferred_grow_zone(zone, order))
					goto try_this_zone;
			}
#endif
		}
	}

	/*
	 * It's possible on a UMA machine to get through all zones that are
	 * fragmented. If avoiding fragmentation, reset and try again.
	 */
	if (no_fallback) {
		alloc_flags &= ~ALLOC_NOFRAGMENT;
		goto retry;
	}

	return NULL;
}
----

▪ rmqueue()
[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/page_alloc.c
----
/*
 * Allocate a page from the given zone.
 * Use pcplists for THP or "cheap" high-order allocations.
 */

/*
 * Do not instrument rmqueue() with KMSAN. This function may call
 * __msan_poison_alloca() through a call to set_pfnblock_flags_mask().
 * If __msan_poison_alloca() attempts to allocate pages for the stack depot, it
 * may call rmqueue() again, which will result in a deadlock.
 */
__no_sanitize_memory
static inline
struct page *rmqueue(struct zone *preferred_zone,
			struct zone *zone, unsigned int order,
			gfp_t gfp_flags, unsigned int alloc_flags,
			int migratetype)
{
	struct page *page;

	/*
	 * We most definitely don't want callers attempting to
	 * allocate greater than order-1 page units with __GFP_NOFAIL.
	 */
	WARN_ON_ONCE((gfp_flags & __GFP_NOFAIL) && (order > 1));

	if (likely(pcp_allowed_order(order))) {
		/*
		 * MIGRATE_MOVABLE pcplist could have the pages on CMA area and
		 * we need to skip it when CMA area isn't allowed.
		 */
		if (!IS_ENABLED(CONFIG_CMA) || alloc_flags & ALLOC_CMA ||
				migratetype != MIGRATE_MOVABLE) {
			page = rmqueue_pcplist(preferred_zone, zone, order,
					migratetype, alloc_flags);
			if (likely(page))
				goto out;
		}
	}

	page = rmqueue_buddy(preferred_zone, zone, order, alloc_flags,
							migratetype);

out:
	/* Separate test+clear to avoid unnecessary atomics */
	if (unlikely(test_bit(ZONE_BOOSTED_WATERMARK, &zone->flags))) {
		clear_bit(ZONE_BOOSTED_WATERMARK, &zone->flags);
		wakeup_kswapd(zone, 0, 0, zone_idx(zone));
	}

	VM_BUG_ON_PAGE(page && bad_range(zone, page), page);
	return page;
}
----

===== __alloc_pages_slowpath
▪ __alloc_pages_slowpath()
[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/page_alloc.c
----
static inline struct page *
__alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,
						struct alloc_context *ac)
{
	bool can_direct_reclaim = gfp_mask & __GFP_DIRECT_RECLAIM;
	const bool costly_order = order > PAGE_ALLOC_COSTLY_ORDER;
	struct page *page = NULL;
	unsigned int alloc_flags;
	unsigned long did_some_progress;
	enum compact_priority compact_priority;
	enum compact_result compact_result;
	int compaction_retries;
	int no_progress_loops;
	unsigned int cpuset_mems_cookie;
	unsigned int zonelist_iter_cookie;
	int reserve_flags;

	/*
	 * We also sanity check to catch abuse of atomic reserves being used by
	 * callers that are not in atomic context.
	 */
	if (WARN_ON_ONCE((gfp_mask & (__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)) ==
				(__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)))
		gfp_mask &= ~__GFP_ATOMIC;

restart:
	compaction_retries = 0;
	no_progress_loops = 0;
	compact_priority = DEF_COMPACT_PRIORITY;
	cpuset_mems_cookie = read_mems_allowed_begin();
	zonelist_iter_cookie = zonelist_iter_begin();

	/*
	 * The fast path uses conservative alloc_flags to succeed only until
	 * kswapd needs to be woken up, and to avoid the cost of setting up
	 * alloc_flags precisely. So we do that now.
	 */
	alloc_flags = gfp_to_alloc_flags(gfp_mask);

	/*
	 * We need to recalculate the starting point for the zonelist iterator
	 * because we might have used different nodemask in the fast path, or
	 * there was a cpuset modification and we are retrying - otherwise we
	 * could end up iterating over non-eligible zones endlessly.
	 */
	ac->preferred_zoneref = first_zones_zonelist(ac->zonelist,
					ac->highest_zoneidx, ac->nodemask);
	if (!ac->preferred_zoneref->zone)
		goto nopage;

	/*
	 * Check for insane configurations where the cpuset doesn't contain
	 * any suitable zone to satisfy the request - e.g. non-movable
	 * GFP_HIGHUSER allocations from MOVABLE nodes only.
	 */
	if (cpusets_insane_config() && (gfp_mask & __GFP_HARDWALL)) {
		struct zoneref *z = first_zones_zonelist(ac->zonelist,
					ac->highest_zoneidx,
					&cpuset_current_mems_allowed);
		if (!z->zone)
			goto nopage;
	}

	if (alloc_flags & ALLOC_KSWAPD)
		wake_all_kswapds(order, gfp_mask, ac);

	/*
	 * The adjusted alloc_flags might result in immediate success, so try
	 * that first
	 */
	page = get_page_from_freelist(gfp_mask, order, alloc_flags, ac);
	if (page)
		goto got_pg;

	/*
	 * For costly allocations, try direct compaction first, as it's likely
	 * that we have enough base pages and don't need to reclaim. For non-
	 * movable high-order allocations, do that as well, as compaction will
	 * try prevent permanent fragmentation by migrating from blocks of the
	 * same migratetype.
	 * Don't try this for allocations that are allowed to ignore
	 * watermarks, as the ALLOC_NO_WATERMARKS attempt didn't yet happen.
	 */
	if (can_direct_reclaim &&
			(costly_order ||
			   (order > 0 && ac->migratetype != MIGRATE_MOVABLE))
			&& !gfp_pfmemalloc_allowed(gfp_mask)) {
		page = __alloc_pages_direct_compact(gfp_mask, order,
						alloc_flags, ac,
						INIT_COMPACT_PRIORITY,
						&compact_result);
		if (page)
			goto got_pg;

		/*
		 * Checks for costly allocations with __GFP_NORETRY, which
		 * includes some THP page fault allocations
		 */
		if (costly_order && (gfp_mask & __GFP_NORETRY)) {
			/*
			 * If allocating entire pageblock(s) and compaction
			 * failed because all zones are below low watermarks
			 * or is prohibited because it recently failed at this
			 * order, fail immediately unless the allocator has
			 * requested compaction and reclaim retry.
			 *
			 * Reclaim is
			 *  - potentially very expensive because zones are far
			 *    below their low watermarks or this is part of very
			 *    bursty high order allocations,
			 *  - not guaranteed to help because isolate_freepages()
			 *    may not iterate over freed pages as part of its
			 *    linear scan, and
			 *  - unlikely to make entire pageblocks free on its
			 *    own.
			 */
			if (compact_result == COMPACT_SKIPPED ||
			    compact_result == COMPACT_DEFERRED)
				goto nopage;

			/*
			 * Looks like reclaim/compaction is worth trying, but
			 * sync compaction could be very expensive, so keep
			 * using async compaction.
			 */
			compact_priority = INIT_COMPACT_PRIORITY;
		}
	}

retry:
	/* Ensure kswapd doesn't accidentally go to sleep as long as we loop */
	if (alloc_flags & ALLOC_KSWAPD)
		wake_all_kswapds(order, gfp_mask, ac);

	reserve_flags = __gfp_pfmemalloc_flags(gfp_mask);
	if (reserve_flags)
		alloc_flags = gfp_to_alloc_flags_cma(gfp_mask, reserve_flags) |
					  (alloc_flags & ALLOC_KSWAPD);

	/*
	 * Reset the nodemask and zonelist iterators if memory policies can be
	 * ignored. These allocations are high priority and system rather than
	 * user oriented.
	 */
	if (!(alloc_flags & ALLOC_CPUSET) || reserve_flags) {
		ac->nodemask = NULL;
		ac->preferred_zoneref = first_zones_zonelist(ac->zonelist,
					ac->highest_zoneidx, ac->nodemask);
	}

	/* Attempt with potentially adjusted zonelist and alloc_flags */
	page = get_page_from_freelist(gfp_mask, order, alloc_flags, ac);
	if (page)
		goto got_pg;

	/* Caller is not willing to reclaim, we can't balance anything */
	if (!can_direct_reclaim)
		goto nopage;

	/* Avoid recursion of direct reclaim */
	if (current->flags & PF_MEMALLOC)
		goto nopage;

	/* Try direct reclaim and then allocating */
	page = __alloc_pages_direct_reclaim(gfp_mask, order, alloc_flags, ac,
							&did_some_progress);
	if (page)
		goto got_pg;

	/* Try direct compaction and then allocating */
	page = __alloc_pages_direct_compact(gfp_mask, order, alloc_flags, ac,
					compact_priority, &compact_result);
	if (page)
		goto got_pg;

	/* Do not loop if specifically requested */
	if (gfp_mask & __GFP_NORETRY)
		goto nopage;

	/*
	 * Do not retry costly high order allocations unless they are
	 * __GFP_RETRY_MAYFAIL
	 */
	if (costly_order && !(gfp_mask & __GFP_RETRY_MAYFAIL))
		goto nopage;

	if (should_reclaim_retry(gfp_mask, order, ac, alloc_flags,
				 did_some_progress > 0, &no_progress_loops))
		goto retry;

	/*
	 * It doesn't make any sense to retry for the compaction if the order-0
	 * reclaim is not able to make any progress because the current
	 * implementation of the compaction depends on the sufficient amount
	 * of free memory (see __compaction_suitable)
	 */
	if (did_some_progress > 0 &&
			should_compact_retry(ac, order, alloc_flags,
				compact_result, &compact_priority,
				&compaction_retries))
		goto retry;


	/*
	 * Deal with possible cpuset update races or zonelist updates to avoid
	 * a unnecessary OOM kill.
	 */
	if (check_retry_cpuset(cpuset_mems_cookie, ac) ||
	    check_retry_zonelist(zonelist_iter_cookie))
		goto restart;

	/* Reclaim has failed us, start killing things */
	page = __alloc_pages_may_oom(gfp_mask, order, ac, &did_some_progress);
	if (page)
		goto got_pg;

	/* Avoid allocations with no watermarks from looping endlessly */
	if (tsk_is_oom_victim(current) &&
	    (alloc_flags & ALLOC_OOM ||
	     (gfp_mask & __GFP_NOMEMALLOC)))
		goto nopage;

	/* Retry as long as the OOM killer is making progress */
	if (did_some_progress) {
		no_progress_loops = 0;
		goto retry;
	}

nopage:
	/*
	 * Deal with possible cpuset update races or zonelist updates to avoid
	 * a unnecessary OOM kill.
	 */
	if (check_retry_cpuset(cpuset_mems_cookie, ac) ||
	    check_retry_zonelist(zonelist_iter_cookie))
		goto restart;

	/*
	 * Make sure that __GFP_NOFAIL request doesn't leak out and make sure
	 * we always retry
	 */
	if (gfp_mask & __GFP_NOFAIL) {
		/*
		 * All existing users of the __GFP_NOFAIL are blockable, so warn
		 * of any new users that actually require GFP_NOWAIT
		 */
		if (WARN_ON_ONCE_GFP(!can_direct_reclaim, gfp_mask))
			goto fail;

		/*
		 * PF_MEMALLOC request from this context is rather bizarre
		 * because we cannot reclaim anything and only can loop waiting
		 * for somebody to do a work for us
		 */
		WARN_ON_ONCE_GFP(current->flags & PF_MEMALLOC, gfp_mask);

		/*
		 * non failing costly orders are a hard requirement which we
		 * are not prepared for much so let's warn about these users
		 * so that we can identify them and convert them to something
		 * else.
		 */
		WARN_ON_ONCE_GFP(costly_order, gfp_mask);

		/*
		 * Help non-failing allocations by giving them access to memory
		 * reserves but do not use ALLOC_NO_WATERMARKS because this
		 * could deplete whole memory reserves which would just make
		 * the situation worse
		 */
		page = __alloc_pages_cpuset_fallback(gfp_mask, order, ALLOC_HARDER, ac);
		if (page)
			goto got_pg;

		cond_resched();
		goto retry;
	}
fail:
	warn_alloc(gfp_mask, ac->nodemask,
			"page allocation failure: order:%u", order);
got_pg:
	return page;
}
----

==== gfp_t
▪ 分配掩码: ___GFP_DMA等  gfp(GFP)代表get_free_pages
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/gfp_types.h
----
/* The typedef is in types.h but we want the documentation here */
#if 0
/**
 * typedef gfp_t - Memory allocation flags.
 *
 * GFP flags are commonly used throughout Linux to indicate how memory
 * should be allocated.  The GFP acronym stands for get_free_pages(),
 * the underlying memory allocation function.  Not every GFP flag is
 * supported by every function which may allocate memory.  Most users
 * will want to use a plain ``GFP_KERNEL``.
 */
typedef unsigned int __bitwise gfp_t;
#endif

/*
 * In case of changes, please don't forget to update
 * include/trace/events/mmflags.h and tools/perf/builtin-kmem.c
 */

/* Plain integer GFP bitmasks. Do not use this directly. */
#define ___GFP_DMA		0x01u
#define ___GFP_HIGHMEM		0x02u
#define ___GFP_DMA32		0x04u
#define ___GFP_MOVABLE		0x08u
#define ___GFP_RECLAIMABLE	0x10u
#define ___GFP_HIGH		0x20u
#define ___GFP_IO		0x40u
#define ___GFP_FS		0x80u
#define ___GFP_ZERO		0x100u
#define ___GFP_ATOMIC		0x200u
#define ___GFP_DIRECT_RECLAIM	0x400u
#define ___GFP_KSWAPD_RECLAIM	0x800u
#define ___GFP_WRITE		0x1000u
#define ___GFP_NOWARN		0x2000u
#define ___GFP_RETRY_MAYFAIL	0x4000u
#define ___GFP_NOFAIL		0x8000u
#define ___GFP_NORETRY		0x10000u
#define ___GFP_MEMALLOC		0x20000u
#define ___GFP_COMP		0x40000u
#define ___GFP_NOMEMALLOC	0x80000u
#define ___GFP_HARDWALL		0x100000u
#define ___GFP_THISNODE		0x200000u
#define ___GFP_ACCOUNT		0x400000u
#define ___GFP_ZEROTAGS		0x800000u
#ifdef CONFIG_KASAN_HW_TAGS
#define ___GFP_SKIP_ZERO		0x1000000u
#define ___GFP_SKIP_KASAN_UNPOISON	0x2000000u
#define ___GFP_SKIP_KASAN_POISON	0x4000000u
#else
#define ___GFP_SKIP_ZERO		0
#define ___GFP_SKIP_KASAN_UNPOISON	0
#define ___GFP_SKIP_KASAN_POISON	0
#endif
#ifdef CONFIG_LOCKDEP
#define ___GFP_NOLOCKDEP	0x8000000u
#else
#define ___GFP_NOLOCKDEP	0
#endif
----

▪ https://elixir.bootlin.com/linux/latest/source/include/linux/gfp.h
▪ 进程中也有影响分配的一些标志: 例如PF_MEMALLOC等 https://elixir.bootlin.com/linux/latest/source/include/linux/sched.h

=== 释放页
如同__alloc_pages作为页面分配的核心函数，__free_pages则是页面释放的核心函数。
[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/page_alloc.c
----
/**
 * __free_pages - Free pages allocated with alloc_pages().
 * @page: The page pointer returned from alloc_pages().
 * @order: The order of the allocation.
 *
 * This function can free multi-page allocations that are not compound
 * pages.  It does not check that the @order passed in matches that of
 * the allocation, so it is easy to leak memory.  Freeing more memory
 * than was allocated will probably emit a warning.
 *
 * If the last reference to this page is speculative, it will be released
 * by put_page() which only frees the first page of a non-compound
 * allocation.  To prevent the remaining pages from being leaked, we free
 * the subsequent pages here.  If you want to use the page's reference
 * count to decide when to free the allocation, you should allocate a
 * compound page, and use put_page() instead of __free_pages().
 *
 * Context: May be called in interrupt context or while holding a normal
 * spinlock, but not in NMI context or while holding a raw spinlock.
 */
void __free_pages(struct page *page, unsigned int order)
{
	if (put_page_testzero(page))
		free_the_page(page, order);
	else if (!PageHead(page))
		while (order-- > 0)
			free_the_page(page + (1 << order), order);
}
----

=== 交换

==== 概念
页面交换: 利用磁盘空间作为扩展内存，从而增大了可用的内存。
换出: 从内存到磁盘，称为换出；
换入: 从磁盘到内存，称为换入。

▪ 命令:

	vmstat
		si: Amount of memory swapped in from disk (/s).
		so: Amount of memory swapped to disk (/s).

注意，不是所有的内存页面都是可以交换出去的。
那么，哪些页面能够交换出去呢？

==== 数据结构
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/swap.h
----
/*
 * The in-memory structure used to track swap areas.
 */
struct swap_info_struct {
	struct percpu_ref users;	/* indicate and keep swap device valid. */
	unsigned long	flags;		/* SWP_USED etc: see above */
	signed short	prio;		/* swap priority of this type */
	struct plist_node list;		/* entry in swap_active_head */
	signed char	type;		/* strange name for an index */
	unsigned int	max;		/* extent of the swap_map */
	unsigned char *swap_map;	/* vmalloc'ed array of usage counts */
	struct swap_cluster_info *cluster_info; /* cluster info. Only for SSD */
	struct swap_cluster_list free_clusters; /* free clusters list */
	unsigned int lowest_bit;	/* index of first free in swap_map */
	unsigned int highest_bit;	/* index of last free in swap_map */
	unsigned int pages;		/* total of usable pages of swap */
	unsigned int inuse_pages;	/* number of those currently in use */
	unsigned int cluster_next;	/* likely index for next allocation */
	unsigned int cluster_nr;	/* countdown to next cluster search */
	unsigned int __percpu *cluster_next_cpu; /*percpu index for next allocation */
	struct percpu_cluster __percpu *percpu_cluster; /* per cpu's swap location */
	struct rb_root swap_extent_root;/* root of the swap extent rbtree */
	struct block_device *bdev;	/* swap device or bdev of swap file */
	struct file *swap_file;		/* seldom referenced */
	unsigned int old_block_size;	/* seldom referenced */
	struct completion comp;		/* seldom referenced */
#ifdef CONFIG_FRONTSWAP
	unsigned long *frontswap_map;	/* frontswap in-use, one bit per page */
	atomic_t frontswap_pages;	/* frontswap pages in-use counter */
#endif
	spinlock_t lock;		/*
					 * protect map scan related fields like
					 * swap_map, lowest_bit, highest_bit,
					 * inuse_pages, cluster_next,
					 * cluster_nr, lowest_alloc,
					 * highest_alloc, free/discard cluster
					 * list. other fields are only changed
					 * at swapon/swapoff, so are protected
					 * by swap_lock. changing flags need
					 * hold this lock and swap_lock. If
					 * both locks need hold, hold swap_lock
					 * first.
					 */
	spinlock_t cont_lock;		/*
					 * protect swap count continuation page
					 * list.
					 */
	struct work_struct discard_work; /* discard worker */
	struct swap_cluster_list discard_clusters; /* discard clusters list */
	struct plist_node avail_lists[]; /*
					   * entries in swap_avail_heads, one
					   * entry per node.
					   * Must be last as the number of the
					   * array is nr_node_ids, which is not
					   * a fixed value so have to allocate
					   * dynamically.
					   * And it has to be an array so that
					   * plist_for_each_* can work.
					   */
};
----

[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/swapfile.c
----
struct swap_info_struct *swap_info[MAX_SWAPFILES];
----

https://elixir.bootlin.com/linux/latest/source/mm/swap.c

==== 定期换出
在缺页异常的时候，临时搜索可供换出的页面并加以换出，其代价比较大，因此有必要定期检查并预先将某些页面换出以便腾出空间，从而减少缺页异常发生时的处理负担。
承担这一任务的就是kswapd内核线程。

- kswapd_init
[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/vmscan.c
----
static int __init kswapd_init(void)
{
	int nid;

	swap_setup();
	for_each_node_state(nid, N_MEMORY)
 		kswapd_run(nid);
	return 0;
}

module_init(kswapd_init)
----

- kswapd_init()->swap_setup()
[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/swap.c
----
/*
 * Perform any setup for the swap system
 */
void __init swap_setup(void)
{
	unsigned long megs = totalram_pages() >> (20 - PAGE_SHIFT);

	/* Use a smaller cluster for small-memory machines */
	if (megs < 16)
		page_cluster = 2;
	else
		page_cluster = 3;
	/*
	 * Right now other parts of the system means that we
	 * _really_ don't want to cluster much more
	 */
}
----

- kswapd_init()->kswapd_run()
[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/vmscan.c
----
/*
 * The background pageout daemon, started as a kernel thread
 * from the init process.
 *
 * This basically trickles out pages so that we have _some_
 * free memory available even if there is no other activity
 * that frees anything up. This is needed for things like routing
 * etc, where we otherwise might have all activity going on in
 * asynchronous contexts that cannot page things out.
 *
 * If there are applications that are active memory-allocators
 * (most normal use), this basically shouldn't matter.
 */
static int kswapd(void *p)
{
	unsigned int alloc_order, reclaim_order;
	unsigned int highest_zoneidx = MAX_NR_ZONES - 1;
	pg_data_t *pgdat = (pg_data_t *)p;
	struct task_struct *tsk = current;
	const struct cpumask *cpumask = cpumask_of_node(pgdat->node_id);

	if (!cpumask_empty(cpumask))
		set_cpus_allowed_ptr(tsk, cpumask);

	/*
	 * Tell the memory management that we're a "memory allocator",
	 * and that if we need more memory we should get access to it
	 * regardless (see "__alloc_pages()"). "kswapd" should
	 * never get caught in the normal page freeing logic.
	 *
	 * (Kswapd normally doesn't need memory anyway, but sometimes
	 * you need a small amount of memory in order to be able to
	 * page out something else, and this flag essentially protects
	 * us from recursively trying to free more memory as we're
	 * trying to free the first piece of memory in the first place).
	 */
	tsk->flags |= PF_MEMALLOC | PF_KSWAPD;
	set_freezable();

	WRITE_ONCE(pgdat->kswapd_order, 0);
	WRITE_ONCE(pgdat->kswapd_highest_zoneidx, MAX_NR_ZONES);
	atomic_set(&pgdat->nr_writeback_throttled, 0);
	for ( ; ; ) {
		bool ret;

		alloc_order = reclaim_order = READ_ONCE(pgdat->kswapd_order);
		highest_zoneidx = kswapd_highest_zoneidx(pgdat,
							highest_zoneidx);

kswapd_try_sleep:
		kswapd_try_to_sleep(pgdat, alloc_order, reclaim_order,
					highest_zoneidx);

		/* Read the new order and highest_zoneidx */
		alloc_order = READ_ONCE(pgdat->kswapd_order);
		highest_zoneidx = kswapd_highest_zoneidx(pgdat,
							highest_zoneidx);
		WRITE_ONCE(pgdat->kswapd_order, 0);
		WRITE_ONCE(pgdat->kswapd_highest_zoneidx, MAX_NR_ZONES);

		ret = try_to_freeze();
		if (kthread_should_stop())
			break;

		/*
		 * We can speed up thawing tasks if we don't call balance_pgdat
		 * after returning from the refrigerator
		 */
		if (ret)
			continue;

		/*
		 * Reclaim begins at the requested order but if a high-order
		 * reclaim fails then kswapd falls back to reclaiming for
		 * order-0. If that happens, kswapd will consider sleeping
		 * for the order it finished reclaiming at (reclaim_order)
		 * but kcompactd is woken to compact for the original
		 * request (alloc_order).
		 */
		trace_mm_vmscan_kswapd_wake(pgdat->node_id, highest_zoneidx,
						alloc_order);
		reclaim_order = balance_pgdat(pgdat, alloc_order,
						highest_zoneidx);
		if (reclaim_order < alloc_order)
			goto kswapd_try_sleep;
	}

	tsk->flags &= ~(PF_MEMALLOC | PF_KSWAPD);

	return 0;
}
//...
/*
 * This kswapd start function will be called by init and node-hot-add.
 */
void kswapd_run(int nid)
{
	pg_data_t *pgdat = NODE_DATA(nid);

	pgdat_kswapd_lock(pgdat);
	if (!pgdat->kswapd) {
		pgdat->kswapd = kthread_run(kswapd, pgdat, "kswapd%d", nid);
		if (IS_ERR(pgdat->kswapd)) {
			/* failure at boot is fatal */
			BUG_ON(system_state < SYSTEM_RUNNING);
			pr_err("Failed to start kswapd on node %d\n", nid);
			pgdat->kswapd = NULL;
		}
	}
	pgdat_kswapd_unlock(pgdat);
}
----

休眠: kswapd_try_to_sleep()
休眠时机:

唤醒: wakeup_kswapd()
唤醒时机:

[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/swap.c
----
/* How many pages do we try to swap or page in/out together? */
int page_cluster;
----

参考: https://zhuanlan.zhihu.com/p/574462887

==== 换入

===== page falut(缺页异常)
▪ 发生page falut(缺页异常)的时机

    ▪ page table中找不到对应的PTE
        ▪ 无效地址(通过地址的addr来找vma，如果没找到说明地址无效，段错误，内核panic掉/相应的页面目录或页表项为空即线性地址与物理地址尚未建立映射关系，或者已经撤销)
        ▪ 有效地址但是没有载入主存(对应的页面不在内存中)
            ▪ 首次访问，发生调页。
            ▪ 如果当前page的present=0，说明不在主存中被swap out了，需要从外存调入主存。
            ▪ COW时访问语义冲突，比如PTE不可写，但是做了写操作，会触发COW机制，在copy page中write
    ▪ 对应虚拟地址的PTE拒绝访问(页面权限不符，例如试图写一个只读页面)

参考: https://bbs.pediy.com/thread-269149.htm

====== x86
DEFINE_IDTENTRY_RAW_ERRORCODE(exc_page_fault): https://elixir.bootlin.com/linux/latest/source/arch/x86/mm/fault.c

	handle_page_fault(regs, error_code, address);
        do_kern_addr_fault(regs, error_code, address);
		do_user_addr_fault(regs, error_code, address);

    do_kern_addr_fault(): https://elixir.bootlin.com/linux/latest/source/arch/x86/mm/fault.c

    do_user_addr_fault(): https://elixir.bootlin.com/linux/latest/source/arch/x86/mm/fault.c
        handle_mm_fault(): https://elixir.bootlin.com/linux/latest/source/mm/memory.c
            __handle_mm_fault()
                handle_pte_fault()
                    do_swap_page()

===== 流程

	do_swap_page(): https://elixir.bootlin.com/linux/latest/source/mm/memory.c
		swapin_readahead(): https://elixir.bootlin.com/linux/latest/source/mm/swap_state.c

==== 页面抖动(pagethrashing)
刚刚换出的页面马上又要换入主存，刚刚换入的页面马上就要换出主存，这种糟糕的行为称为抖动或颠簸。

=== 回收
快速内存回收:
在快路径中触发。快路径分配页框时，会对待分配的zone判断是否满足水位要求，这个水位可能是high/low/min中任一个，与分配用途相关。如果没有达到分配的水位要求，则会触发快速内存回收。

直接内存回收:
在慢路径中触发。当zonelist中的所有zone都无法以min水线进行页框分配时，系统会根据情况启动一次针对全zonelist进行直接内存回收。这是非常耗时的回收方式，非必要时，系统优先采用其它两种方式。该方式有可能唤醒flush内核线程处理页面回写。

kswapd内存回收:
每个node都对应着一个kswapd内核线程，换句话说，kswapd内存回收只针对特定的node。当node中所有zone都满足分配后回到高水位时，kswapd内核线程就结束回收操作，进入阻塞等到。由于直接内存回收非常耗时，系统更愿意通过这种方式进行内存回收，只有当kswapd的方式超过阈值次数没有回收到页框，才会考虑使用直接内存回收方式。

参考: https://zhuanlan.zhihu.com/p/480428225
参考: https://www.cnblogs.com/linhaostudy/p/12679296.html

=== 页帧迁移
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/migrate_mode.h
----
/*
 * MIGRATE_ASYNC means never block
 * MIGRATE_SYNC_LIGHT in the current implementation means to allow blocking
 *	on most operations but not ->writepage as the potential stall time
 *	is too significant
 * MIGRATE_SYNC will block when migrating pages
 * MIGRATE_SYNC_NO_COPY will block when migrating pages but will not copy pages
 *	with the CPU. Instead, page copy happens outside the migratepage()
 *	callback and is likely using a DMA engine. See migrate_vma() and HMM
 *	(mm/hmm.c) for users of this mode.
 */
enum migrate_mode {
	MIGRATE_ASYNC,
	MIGRATE_SYNC_LIGHT,
	MIGRATE_SYNC,
	MIGRATE_SYNC_NO_COPY,
};
----

异步模式(MIGRATE_ASYNC):
在该模式不允许进行任何阻塞操作，当需要阻塞或者调度的时候，则停止内存碎片整理。在该模式下只会处理MIGRATE_MOVABLE、MIGRATE_CMA类型的页框，而不会处理MIRGATE_RECLAIMABLE类型的页框，因为该类型的页框大多数是文件页，对文件页进行内存碎片整理，有可能涉及脏页回写，这会引起阻塞。

轻同步模式(MIGRATE_SYNC_LIGHT):
该模式允许绝大部分的阻塞操作，但是不阻塞等待脏文件页的回写操作，因为回写时间可能很长。

同步模式(MIGRATE_SYNC):
该模式允许在迁移页框时允许阻塞，也就是允许页回写完成才返回结果，这是最耗时的模式。该模式会整zone扫描，并且不会跳过标记为PG_migrate_skip标志的pageblock。

非拷贝同步模式(MIGRATE_SYNC_NO_COPY):
与同步模式类似，在迁移页框时允许阻塞，但不会进行页框拷贝。

只有三种类型的页框支持内存碎片整理: MIGRATE_MOVABLE、MIGRATE_CMA和MIRGATE_RECLAIMABLE。

碎片整理时机:
"快路径"无法分配到连续内存，进入"慢路径"时会进行内存碎片整理;
kswapd任务中，进行内存回收后会进行内存碎片整理;
手动触发，往/proc/sys/vm/compact_memory中写入1时;
指定范围分配连续页框时，而该范围的页框有部分已经被使用，则需要通过碎片整理的方式进行迁移。

[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/page_alloc.c
----
/*
 * This array describes the order lists are fallen back to when
 * the free lists for the desirable migrate type are depleted
 *
 * The other migratetypes do not have fallbacks.
 */
static int fallbacks[MIGRATE_TYPES][3] = {
	[MIGRATE_UNMOVABLE]   = { MIGRATE_RECLAIMABLE, MIGRATE_MOVABLE,   MIGRATE_TYPES },
	[MIGRATE_MOVABLE]     = { MIGRATE_RECLAIMABLE, MIGRATE_UNMOVABLE, MIGRATE_TYPES },
	[MIGRATE_RECLAIMABLE] = { MIGRATE_UNMOVABLE,   MIGRATE_MOVABLE,   MIGRATE_TYPES },
};
----

void set_pageblock_migratetype(struct page *page, int migratetype)
初始化的时候都是MIGRATE_MOVABLE

__alloc_pages_direct_compact(): https://elixir.bootlin.com/linux/latest/source/mm/page_alloc.c

参考: https://www.zhihu.com/column/c_1444822980567805952
参考: https://zhuanlan.zhihu.com/p/473369120

=== 大页
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/pgtable.h
pte_mkhuge(): 4M
pmd_mkhuge(): 1G
参考: https://zhuanlan.zhihu.com/p/503738975

=== 回写
用于将内存映射被修改的内容与底层的块设备同步，也称为数据回写。

=== 参考
https://www.kernel.org/doc/gorman/html/understand/understand006.html
https://puqiyuan.github.io/kernel/mm/mm_series/ptm.html
