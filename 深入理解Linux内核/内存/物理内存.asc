:toc:
:toclevels: 5
:hardbreaks-option:

== 物理内存

=== 内存模型

==== 概念
为了管理物理内存页，内核提出了内存模型的概念。
其主要解决的问题是:
不连续内存的支持，这主要体现在NUMA和内存热插拔；
性能，主要体现为使用尽量少的内存去管理物理页面，以及pfn_to_page和page_to_pfn的转换效率。

PFN(page frame number): page frame是针对物理内存而言的，把物理内存分成一个个page size区域，并且给每一个page进行编号，这个编号就是PFN。

==== 演进
https://elixir.bootlin.com/linux/latest/source/include/asm-generic/memory_model.h

▪ FLATMEM
0.11就开始存在
连续

▪ DISCONTIGMEM
从2.3.99pre6开始支持，5.14被废弃
支持不连续

▪ SPARSEMEM
2.6.13开始支持SPARSEMEM
2.6.24开始支持SPARSEMEM_VMEMMAP
支持不连续
支持内存热插拔
默认使用SPARSEMEM内存模型

在sparse memory内存模型下，连续的地址空间按照SECTION(例如1G)被分成了一段一段，其中每一section都是hotplug的，因此内存地址空间可以被切分的更细，支持更离散的discontiguous memory。此外，在sparse memory没有出现之前，NUMA和discontiguous memory总是剪不断，理还乱的关系：NUMA并没有规定其内存的连续性，而discontiguous memory系统也并非一定是NUMA系统，但是这两种配置都是multi node的。有了sparse memory之后，可以把内存的连续性和NUMA的概念剥离开来：一个NUMA系统可以是flat memory，也可以是sparse memory，而一个sparse memory系统可以是NUMA，也可以是UMA的。

==== FLATMEM
CONFIG_FLATMEM

通过简单的线性映射将物理内存页与一个mem_map数组对应起来。
优点: 简单高效
缺点: 存在大量空洞内存的场景下，mem_map数组可能会很大，比较浪费内存。

==== DISCONTIGMEM
CONFIG_DISCONTIGMEM

DISCONTIGMEM本质上是一个node上的FLATMEM, 在node的增加或者内存热插拔的场景下，同一个node内也可能出现大量不连续内存，导致DISCONTIGMEM开销越来越大。

==== SPARSEMEM
===== 经典SPARSEMEM
CONFIG_SPARSEMEM

===== SPARSEMEM_EXTREME
CONFIG_SPARSEMEM_EXTREME
在SPARSEMEM加入到linux几个月后，SPARSEMEM_EXTREME又被引入到kernel，这个特性是针对极度稀疏物理内存对SPARSEMEM模型进行的一种扩展。这种扩展的理由在于SPARSEMEM模型中使用了一个长度为NR_MEM_SECTIONS的struct mem_section数组来表示所有可能的mem sections。对于一些极度稀疏的物理内存，并不会用到这么多的mem sections，因而是一种浪费。
参考: https://www.cnblogs.com/liuhailong0112/p/14515466.html
参考: https://lwn.net/Articles/147285/
参考: https://lwn.net/Articles/147286/

===== SPARSEMEM_VMEMMAP
CONFIG_SPARSEMEM_VMEMMAP
2007年，引入这个特性是因为经典SPARSEMEM不仅在进行pfn_to_page()和page_to_pfn()时颇为复杂，而且需要page->flags维护section索引。
SPARSEMEM_VMEMMAP的实现思路非常简洁：在虚拟地址空间中划分出一个连续地址区域用于和物理页框号一一映射，这样一旦这个虚拟区域的首地址确定下来，系统中所有物理页框对应的struct page也就确定下来了。
参考: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=8f6aac419bd

===== 实现
struct mem_section:
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/mmzone.h
----
struct mem_section_usage {
#ifdef CONFIG_SPARSEMEM_VMEMMAP
	DECLARE_BITMAP(subsection_map, SUBSECTIONS_PER_SECTION);
#endif
	/* See declaration of similar field in struct zone */
	unsigned long pageblock_flags[0];
};
//...
struct mem_section {
	/*
	 * This is, logically, a pointer to an array of struct
	 * pages.  However, it is stored with some other magic.
	 * (see sparse.c::sparse_init_one_section())
	 *
	 * Additionally during early boot we encode node id of
	 * the location of the section here to guide allocation.
	 * (see sparse.c::memory_present())
	 *
	 * Making it a UL at least makes someone do a cast
	 * before using it wrong.
	 */
	unsigned long section_mem_map;

	struct mem_section_usage *usage;
#ifdef CONFIG_PAGE_EXTENSION
	/*
	 * If SPARSEMEM, pgdat doesn't have page_ext pointer. We use
	 * section. (see page_ext.h about this.)
	 */
	struct page_ext *page_ext;
	unsigned long pad;
#endif
	/*
	 * WARNING: mem_section must be a power-of-2 in size for the
	 * calculation and use of SECTION_ROOT_MASK to make sense.
	 */
};
----

[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/mmzone.h
----
//一个mem_section root对应的mem_section结构体个数
#ifdef CONFIG_SPARSEMEM_EXTREME
#define SECTIONS_PER_ROOT       (PAGE_SIZE / sizeof (struct mem_section))
#else
#define SECTIONS_PER_ROOT	1
#endif
//...
#define PFN_SECTION_SHIFT	(SECTION_SIZE_BITS - PAGE_SHIFT)
//每个mem_section对应的struct page结构体个数
#define PAGES_PER_SECTION       (1UL << PFN_SECTION_SHIFT)
----

[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/sparsemem.h
----
//一个mem_section对应的物理地址范围(128M)
# define SECTION_SIZE_BITS	27 /* matt - 128 is convenient right now */
----

void __init sparse_init():
https://elixir.bootlin.com/linux/latest/source/mm/sparse.c

参考: https://lwn.net/Articles/134804/
参考: https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/include/asm-x86/mmzone_64.h?h=linux-2.6.25.y&id=b263295dbffd33b0fbff670720fa178c30e3392a
参考: https://zhuanlan.zhihu.com/p/220068494

==== page_to_pfn与pfn_to_page
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/asm-generic/memory_model.h
----
/*
 * supports 3 memory models.
 */
#if defined(CONFIG_FLATMEM)

#ifndef ARCH_PFN_OFFSET
#define ARCH_PFN_OFFSET		(0UL)
#endif

#define (pfn)	(mem_map + ((pfn) - ARCH_PFN_OFFSET))
#define __page_to_pfn(page)	((unsigned long)((page) - mem_map) + \
				 ARCH_PFN_OFFSET)

#elif defined(CONFIG_SPARSEMEM_VMEMMAP)

/* memmap is virtually contiguous.  */
#define __pfn_to_page(pfn)	(vmemmap + (pfn))
#define __page_to_pfn(page)	(unsigned long)((page) - vmemmap)

#elif defined(CONFIG_SPARSEMEM)
/*
 * Note: section's mem_map is encoded to reflect its start_pfn.
 * section[i].section_mem_map == mem_map's address - start_pfn;
 */
#define __page_to_pfn(pg)					\
({	const struct page *__pg = (pg);				\
	int __sec = page_to_section(__pg);			\
	(unsigned long)(__pg - __section_mem_map_addr(__nr_to_section(__sec)));	\
})

#define __pfn_to_page(pfn)				\
({	unsigned long __pfn = (pfn);			\
	struct mem_section *__sec = __pfn_to_section(__pfn);	\
	__section_mem_map_addr(__sec) + __pfn;		\
})
#endif /* CONFIG_FLATMEM/SPARSEMEM */

/*
 * Convert a physical address to a Page Frame Number and back
 */
#define	__phys_to_pfn(paddr)	PHYS_PFN(paddr)
#define	__pfn_to_phys(pfn)	PFN_PHYS(pfn)

#define page_to_pfn __page_to_pfn
#define pfn_to_page __pfn_to_page

#endif /* __ASSEMBLY__ */
----

==== 参考
https://github.com/torvalds/linux/blob/master/Documentation/mm/memory-model.rst
https://lwn.net/Articles/789304/
https://www.zhihu.com/column/c_1444822980567805952
http://www.wowotech.net/memory_management/memory_model.html

=== 内存大小
x86:
https://elixir.bootlin.com/linux/latest/source/arch/x86/boot/memory.c
    
    detect_memory():
        通过BIOS 0x15中断来实现

命令:

    cat /proc/meminfo
    MemTotal:        8105804 kB

=== 初始化

==== 初期: memblock
Memblock用于boot期间管理内存，通常这个时候内核常用的分配器还没有运行起来。
Memblock视系统内存为连续区域。 

https://elixir.bootlin.com/linux/latest/source/mm/memblock.c

https://zhuanlan.zhihu.com/p/444511088
https://github.com/0xAX/linux-insides/blob/master/MM/linux-mm-1.md

==== start_kernel
[source, c]
.https://elixir.bootlin.com/linux/latest/source/init/main.c
----
asmlinkage __visible void __init __no_sanitize_address start_kernel(void)
{
    //...
	build_all_zonelists(NULL);
	page_alloc_init();
    //...
}
----

=== buddy

==== 空闲页帧
▪ 数据结构
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/mmzone.h
----
    struct free_area {
        struct list_head	free_list[MIGRATE_TYPES];
        unsigned long		nr_free;
    };

    struct zone {
        ......
        /* free areas of different sizes */
        struct free_area	free_area[MAX_ORDER];//MAX_ORDER一般为11
        ......
    }
----

free_area[]数组下标也称为阶，对应链表中的连续内存区包含多少个页帧。
例如第0个链表包含的内存区为单页(2°=1), 第1个链表管理的内存区为2页, 第3个管理的内存区为4页, 依次类推。

▪ 系统命令
cat /proc/buddyinfo
cat /proc/pagetypeinfo

==== 分配与释放
▪ alloc_pages/free_pages
参考: link:./页与页表.asc#分配页[分配页]
参考: link:./页与页表.asc#释放页[释放页]

=== slab
==== 概念
slab机制针对小内存的申请与释放。
slab是slob/slub的基础，但其本身对内存消耗较大，不适合用于小内存模型的嵌入式设备，因此又开发了slob。
slob中只有三个slob管理器，分别满足1~4K范围内的内存申请。
但由于slob效率不高，于是又提供了增强版的slub。

通常slub为系统的默认机制: CONFIG_SLUB=y。

slub的改进:
slab有三个弊端：1、每个node节点有三个链表，分别记录空闲slab、部分空闲slab和非空闲slab。当回收操作来不及时，三个链表记录的页框会较长时间停留到slab管理器中，不利于提高内存的使用率。针对这点，slub只保留了一个链表，就是部分空闲slub。2、每个cpu私有数据记录的是object的地址，这些object可能来自不同的slab，那么不利于slab的回收。slub改成记录一个实际可用的slub，不会影响其他slub的回收。3、shared共享链表可能导致一个slab持有较多slab，无法即使释放给伙伴系统。slub去掉了该链表。可见，slub出现的主要目的是为了减少slab的数量，提高内存的使用率。同时，出于对内存使用率的极致追求，slub去除了slab的着色做法，取而代之是slub复用，通过slub复用减轻cache冲突的情况。

==== 系统命令
sudo cat /proc/slabinfo
sudo slabtop -s l

==== 数据结构
===== struct kmem_cache
参考: link:./数据结构.asc#kmem_cache[kmem_cache]

===== 分配
[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/slab_common.c
----
/**
 * kmem_cache_create_usercopy - Create a cache with a region suitable
 * for copying to userspace
 * @name: A string which is used in /proc/slabinfo to identify this cache.
 * @size: The size of objects to be created in this cache.
 * @align: The required alignment for the objects.
 * @flags: SLAB flags
 * @useroffset: Usercopy region offset
 * @usersize: Usercopy region size
 * @ctor: A constructor for the objects.
 *
 * Cannot be called within a interrupt, but can be interrupted.
 * The @ctor is run when new pages are allocated by the cache.
 *
 * The flags are
 *
 * %SLAB_POISON - Poison the slab with a known test pattern (a5a5a5a5)
 * to catch references to uninitialised memory.
 *
 * %SLAB_RED_ZONE - Insert `Red` zones around the allocated memory to check
 * for buffer overruns.
 *
 * %SLAB_HWCACHE_ALIGN - Align the objects in this cache to a hardware
 * cacheline.  This can be beneficial if you're counting cycles as closely
 * as davem.
 *
 * Return: a pointer to the cache on success, NULL on failure.
 */
struct kmem_cache *
kmem_cache_create_usercopy(const char *name,
		  unsigned int size, unsigned int align,
		  slab_flags_t flags,
		  unsigned int useroffset, unsigned int usersize,
		  void (*ctor)(void *))
{
	struct kmem_cache *s = NULL;
	const char *cache_name;
	int err;

#ifdef CONFIG_SLUB_DEBUG
	/*
	 * If no slub_debug was enabled globally, the static key is not yet
	 * enabled by setup_slub_debug(). Enable it if the cache is being
	 * created with any of the debugging flags passed explicitly.
	 * It's also possible that this is the first cache created with
	 * SLAB_STORE_USER and we should init stack_depot for it.
	 */
	if (flags & SLAB_DEBUG_FLAGS)
		static_branch_enable(&slub_debug_enabled);
	if (flags & SLAB_STORE_USER)
		stack_depot_init();
#endif

	mutex_lock(&slab_mutex);

	err = kmem_cache_sanity_check(name, size);
	if (err) {
		goto out_unlock;
	}

	/* Refuse requests with allocator specific flags */
	if (flags & ~SLAB_FLAGS_PERMITTED) {
		err = -EINVAL;
		goto out_unlock;
	}

	/*
	 * Some allocators will constraint the set of valid flags to a subset
	 * of all flags. We expect them to define CACHE_CREATE_MASK in this
	 * case, and we'll just provide them with a sanitized version of the
	 * passed flags.
	 */
	flags &= CACHE_CREATE_MASK;

	/* Fail closed on bad usersize of useroffset values. */
	if (WARN_ON(!usersize && useroffset) ||
	    WARN_ON(size < usersize || size - usersize < useroffset))
		usersize = useroffset = 0;

	if (!usersize)
		s = __kmem_cache_alias(name, size, align, flags, ctor);
	if (s)
		goto out_unlock;

	cache_name = kstrdup_const(name, GFP_KERNEL);
	if (!cache_name) {
		err = -ENOMEM;
		goto out_unlock;
	}

	s = create_cache(cache_name, size,
			 calculate_alignment(flags, align, size),
			 flags, useroffset, usersize, ctor, NULL);
	if (IS_ERR(s)) {
		err = PTR_ERR(s);
		kfree_const(cache_name);
	}

out_unlock:
	mutex_unlock(&slab_mutex);

	if (err) {
		if (flags & SLAB_PANIC)
			panic("%s: Failed to create slab '%s'. Error %d\n",
				__func__, name, err);
		else {
			pr_warn("%s(%s) failed with error %d\n",
				__func__, name, err);
			dump_stack();
		}
		return NULL;
	}
	return s;
}
EXPORT_SYMBOL(kmem_cache_create_usercopy);

/**
 * kmem_cache_create - Create a cache.
 * @name: A string which is used in /proc/slabinfo to identify this cache.
 * @size: The size of objects to be created in this cache.
 * @align: The required alignment for the objects.
 * @flags: SLAB flags
 * @ctor: A constructor for the objects.
 *
 * Cannot be called within a interrupt, but can be interrupted.
 * The @ctor is run when new pages are allocated by the cache.
 *
 * The flags are
 *
 * %SLAB_POISON - Poison the slab with a known test pattern (a5a5a5a5)
 * to catch references to uninitialised memory.
 *
 * %SLAB_RED_ZONE - Insert `Red` zones around the allocated memory to check
 * for buffer overruns.
 *
 * %SLAB_HWCACHE_ALIGN - Align the objects in this cache to a hardware
 * cacheline.  This can be beneficial if you're counting cycles as closely
 * as davem.
 *
 * Return: a pointer to the cache on success, NULL on failure.
 */
struct kmem_cache *
kmem_cache_create(const char *name, unsigned int size, unsigned int align,
		slab_flags_t flags, void (*ctor)(void *))
{
	return kmem_cache_create_usercopy(name, size, align, flags, 0, 0,
					  ctor);
}
EXPORT_SYMBOL(kmem_cache_create);
----

===== 释放

删除一个slab缓存时会先把cpu_slab指向的slab归还给node，然后尝试释放slab中的所有对象。如果所有对象均被释放，则释放page到buddy系统中。

[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/slab_common.c
----
void kmem_cache_destroy(struct kmem_cache *s)
{
	int refcnt;
	bool rcu_set;

	if (unlikely(!s) || !kasan_check_byte(s))
		return;

	cpus_read_lock();
	mutex_lock(&slab_mutex);

	rcu_set = s->flags & SLAB_TYPESAFE_BY_RCU;

	refcnt = --s->refcount;
	if (refcnt)
		goto out_unlock;

	WARN(shutdown_cache(s),
	     "%s %s: Slab cache still has objects when called from %pS",
	     __func__, s->name, (void *)_RET_IP_);
out_unlock:
	mutex_unlock(&slab_mutex);
	cpus_read_unlock();
	if (!refcnt && !rcu_set)
		kmem_cache_release(s);
}
----

==== 回收
[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/slab.c
----
/*
 * Initiate the reap timer running on the target CPU.  We run at around 1 to 2Hz
 * via the workqueue/eventd.
 * Add the CPU number into the expiration time to minimize the possibility of
 * the CPUs getting into lockstep and contending for the global cache chain
 * lock.
 */
static void start_cpu_timer(int cpu)
{
	struct delayed_work *reap_work = &per_cpu(slab_reap_work, cpu);

	if (reap_work->work.func == NULL) {
		init_reap_node(cpu);
		INIT_DEFERRABLE_WORK(reap_work, cache_reap);
		schedule_delayed_work_on(cpu, reap_work,
					__round_jiffies_relative(HZ, cpu));
	}
}
----

==== 参考
https://lwn.net/Articles/881039/
https://zhuanlan.zhihu.com/p/490588193

=== kmalloc/kfree
kmalloc/kfree基于slab机制。

==== kmalloc
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/slab.h
----
/**
 * kmalloc - allocate memory
 * @size: how many bytes of memory are required.
 * @flags: the type of memory to allocate.
 *
 * kmalloc is the normal method of allocating memory
 * for objects smaller than page size in the kernel.
 *
 * The allocated object address is aligned to at least ARCH_KMALLOC_MINALIGN
 * bytes. For @size of power of two bytes, the alignment is also guaranteed
 * to be at least to the size.
 *
 * The @flags argument may be one of the GFP flags defined at
 * include/linux/gfp.h and described at
 * :ref:`Documentation/core-api/mm-api.rst <mm-api-gfp-flags>`
 *
 * The recommended usage of the @flags is described at
 * :ref:`Documentation/core-api/memory-allocation.rst <memory_allocation>`
 *
 * Below is a brief outline of the most useful GFP flags
 *
 * %GFP_KERNEL
 *	Allocate normal kernel ram. May sleep.
 *
 * %GFP_NOWAIT
 *	Allocation will not sleep.
 *
 * %GFP_ATOMIC
 *	Allocation will not sleep.  May use emergency pools.
 *
 * %GFP_HIGHUSER
 *	Allocate memory from high memory on behalf of user.
 *
 * Also it is possible to set different flags by OR'ing
 * in one or more of the following additional @flags:
 *
 * %__GFP_HIGH
 *	This allocation has high priority and may use emergency pools.
 *
 * %__GFP_NOFAIL
 *	Indicate that this allocation is in no way allowed to fail
 *	(think twice before using).
 *
 * %__GFP_NORETRY
 *	If memory is not immediately available,
 *	then give up at once.
 *
 * %__GFP_NOWARN
 *	If allocation fails, don't issue any warnings.
 *
 * %__GFP_RETRY_MAYFAIL
 *	Try really hard to succeed the allocation but fail
 *	eventually.
 */
static __always_inline __alloc_size(1) void *kmalloc(size_t size, gfp_t flags)
{
	if (__builtin_constant_p(size)) {
#ifndef CONFIG_SLOB
		unsigned int index;
#endif
		if (size > KMALLOC_MAX_CACHE_SIZE)
			return kmalloc_large(size, flags);
#ifndef CONFIG_SLOB
		index = kmalloc_index(size);

		if (!index)
			return ZERO_SIZE_PTR;

		return kmalloc_trace(
				kmalloc_caches[kmalloc_type(flags)][index],
				flags, size);
#endif
	}
	return __kmalloc(size, flags);
}
----

[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/slab_common.c
----
static __always_inline
void *__do_kmalloc_node(size_t size, gfp_t flags, int node, unsigned long caller)
{
	struct kmem_cache *s;
	void *ret;

	if (unlikely(size > KMALLOC_MAX_CACHE_SIZE)) {
		ret = __kmalloc_large_node(size, flags, node);
		trace_kmalloc(caller, ret, size,
			      PAGE_SIZE << get_order(size), flags, node);
		return ret;
	}

	s = kmalloc_slab(size, flags);

	if (unlikely(ZERO_OR_NULL_PTR(s)))
		return s;

	ret = __kmem_cache_alloc_node(s, flags, node, size, caller);
	ret = kasan_kmalloc(s, ret, size, flags);
	trace_kmalloc(caller, ret, size, s->size, flags, node);
	return ret;
}
//...
void *__kmalloc(size_t size, gfp_t flags)
{
	return __do_kmalloc_node(size, flags, NUMA_NO_NODE, _RET_IP_);
}
----

==== kfree
[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/slab_common.c
----
/**
 * kfree - free previously allocated memory
 * @object: pointer returned by kmalloc.
 *
 * If @object is NULL, no operation is performed.
 *
 * Don't free memory not originally allocated by kmalloc()
 * or you will run into trouble.
 */
void kfree(const void *object)
{
	struct folio *folio;
	struct slab *slab;
	struct kmem_cache *s;

	trace_kfree(_RET_IP_, object);

	if (unlikely(ZERO_OR_NULL_PTR(object)))
		return;

	folio = virt_to_folio(object);
	if (unlikely(!folio_test_slab(folio))) {
		free_large_kmalloc(folio, (void *)object);
		return;
	}

	slab = folio_slab(folio);
	s = slab->slab_cache;
	__kmem_cache_free(s, (void *)object, _RET_IP_);
}
----

=== 热插拔
https://elixir.bootlin.com/linux/latest/source/mm/Kconfig
MEMORY_HOTPLUG
MEMORY_HOTPLUG_DEFAULT_ONLINE
MEMORY_HOTREMOVE
需要SPARSEMEM支持

=== v0.12
v0.12默认最多支持16M物理内存

16M物理内存划分:

    内核区: 0-end
    高速缓冲区: 3段 end-640K | 640K-1M(显示内存与BIOS ROM) | 1M-4M
    虚拟盘: 4M-4.5M
    主内存区: 4.5M-16M

=== 参考
https://www.kernel.org/doc/gorman/html/understand/understand005.html
https://www.ilinuxkernel.com/files/Linux_Physical_Memory_Description.pdf
https://bbs.pediy.com/thread-269149.htm
https://www.cnblogs.com/binlovetech/p/16914715.html
