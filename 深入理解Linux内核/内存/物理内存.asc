:toc:
:toclevels: 5
:hardbreaks-option:

== 物理内存

=== 内存模型
==== 演进
https://elixir.bootlin.com/linux/latest/source/include/asm-generic/memory_model.h

▪ FLATMEM
0.11就开始存在
连续

▪ DISCONTIGMEM
从2.3.99pre6开始支持，5.14被废弃
支持不连续

▪ SPARSEMEM
2.6.13开始支持SPARSEMEM
2.6.24开始支持SPARSEMEM_VMEMMAP
支持不连续
支持内存热插拔

在sparse memory内存模型下，连续的地址空间按照SECTION（例如1G）被分成了一段一段的，其中每一section都是hotplug的，因此内存地址空间可以被切分的更细，支持更离散的Discontiguous memory。此外，在sparse memory没有出现之前，NUMA和Discontiguous memory总是剪不断，理还乱的关系：NUMA并没有规定其内存的连续性，而Discontiguous memory系统也并非一定是NUMA系统，但是这两种配置都是multi node的。有了sparse memory之后，可以把内存的连续性和NUMA的概念剥离开来：一个NUMA系统可以是flat memory，也可以是sparse memory，而一个sparse memory系统可以是NUMA，也可以是UMA的。

==== FLATMEM
CONFIG_FLATMEM

==== DISCONTIGMEM
CONFIG_DISCONTIGMEM
DISCONTIGMEM本质上是一个node上的FLATMEM, 在node的增加或者内存热插拔的场景下，同一个node内也可能出现大量不连续内存，导致DISCONTIGMEM开销越来越大。

==== SPARSEMEM
CONFIG_SPARSEMEM
CONFIG_SPARSEMEM_VMEMMAP

struct mem_section:
[source,c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/mmzone.h
----
struct mem_section_usage {
#ifdef CONFIG_SPARSEMEM_VMEMMAP
	DECLARE_BITMAP(subsection_map, SUBSECTIONS_PER_SECTION);
#endif
	/* See declaration of similar field in struct zone */
	unsigned long pageblock_flags[0];
};
//...
struct mem_section {
	/*
	 * This is, logically, a pointer to an array of struct
	 * pages.  However, it is stored with some other magic.
	 * (see sparse.c::sparse_init_one_section())
	 *
	 * Additionally during early boot we encode node id of
	 * the location of the section here to guide allocation.
	 * (see sparse.c::memory_present())
	 *
	 * Making it a UL at least makes someone do a cast
	 * before using it wrong.
	 */
	unsigned long section_mem_map;

	struct mem_section_usage *usage;
#ifdef CONFIG_PAGE_EXTENSION
	/*
	 * If SPARSEMEM, pgdat doesn't have page_ext pointer. We use
	 * section. (see page_ext.h about this.)
	 */
	struct page_ext *page_ext;
	unsigned long pad;
#endif
	/*
	 * WARNING: mem_section must be a power-of-2 in size for the
	 * calculation and use of SECTION_ROOT_MASK to make sense.
	 */
};
----

#ifdef CONFIG_SPARSEMEM_EXTREME
#define SECTIONS_PER_ROOT       (PAGE_SIZE / sizeof (struct mem_section))
#else
#define SECTIONS_PER_ROOT	1
#endif

https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/sparsemem.h
# define SECTION_SIZE_BITS	27 /* matt - 128 is convenient right now */

void __init sparse_init():
https://elixir.bootlin.com/linux/latest/source/mm/sparse.c

参考: https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/include/asm-x86/mmzone_64.h?h=linux-2.6.25.y&id=b263295dbffd33b0fbff670720fa178c30e3392a

==== ZONE_DEVICE
基于SPARSEMEM_VMEMMAP

参考: https://github.com/torvalds/linux/blob/master/Documentation/mm/memory-model.rst
参考: https://lwn.net/Articles/789304/
参考: https://www.zhihu.com/column/c_1444822980567805952

==== page_to_pfn与pfn_to_page
[source,c]
.https://elixir.bootlin.com/linux/latest/source/include/asm-generic/memory_model.h
----
/*
 * supports 3 memory models.
 */
#if defined(CONFIG_FLATMEM)

#ifndef ARCH_PFN_OFFSET
#define ARCH_PFN_OFFSET		(0UL)
#endif

#define __pfn_to_page(pfn)	(mem_map + ((pfn) - ARCH_PFN_OFFSET))
#define __page_to_pfn(page)	((unsigned long)((page) - mem_map) + \
				 ARCH_PFN_OFFSET)

#elif defined(CONFIG_SPARSEMEM_VMEMMAP)

/* memmap is virtually contiguous.  */
#define __pfn_to_page(pfn)	(vmemmap + (pfn))
#define __page_to_pfn(page)	(unsigned long)((page) - vmemmap)

#elif defined(CONFIG_SPARSEMEM)
/*
 * Note: section's mem_map is encoded to reflect its start_pfn.
 * section[i].section_mem_map == mem_map's address - start_pfn;
 */
#define __page_to_pfn(pg)					\
({	const struct page *__pg = (pg);				\
	int __sec = page_to_section(__pg);			\
	(unsigned long)(__pg - __section_mem_map_addr(__nr_to_section(__sec)));	\
})

#define __pfn_to_page(pfn)				\
({	unsigned long __pfn = (pfn);			\
	struct mem_section *__sec = __pfn_to_section(__pfn);	\
	__section_mem_map_addr(__sec) + __pfn;		\
})
#endif /* CONFIG_FLATMEM/SPARSEMEM */

/*
 * Convert a physical address to a Page Frame Number and back
 */
#define	__phys_to_pfn(paddr)	PHYS_PFN(paddr)
#define	__pfn_to_phys(pfn)	PFN_PHYS(pfn)

#define page_to_pfn __page_to_pfn
#define pfn_to_page __pfn_to_page

#endif /* __ASSEMBLY__ */
----

参考: http://www.wowotech.net/memory_management/memory_model.html

=== 内存大小
x86:
https://elixir.bootlin.com/linux/latest/source/arch/x86/boot/memory.c
    
    detect_memory():
        通过BIOS 0x15中断来实现

命令:

    cat /proc/meminfo
    MemTotal:        8105804 kB

=== 初始化

==== 初期: memblock
Memblock用于boot期间管理内存，通常这个时候内核常用的分配器还没有运行起来。
Memblock视系统内存为连续区域。 

https://elixir.bootlin.com/linux/latest/source/mm/memblock.c

https://zhuanlan.zhihu.com/p/444511088
https://github.com/0xAX/linux-insides/blob/master/MM/linux-mm-1.md

==== start_kernel
[source,c]
.https://elixir.bootlin.com/linux/latest/source/init/main.c
----
asmlinkage __visible void __init __no_sanitize_address start_kernel(void)
{
    //...
	build_all_zonelists(NULL);
	page_alloc_init();
    //...
}
----

=== buddy

==== 空闲页帧
▪ 数据结构
[source,c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/mmzone.h
----
    struct free_area {
        struct list_head	free_list[MIGRATE_TYPES];
        unsigned long		nr_free;
    };

    struct zone {
        ......
        /* free areas of different sizes */
        struct free_area	free_area[MAX_ORDER];//MAX_ORDER一般为11
        ......
    }
----

free_area[]数组下标也称为阶，对应链表中的连续内存区包含多少个页帧。
例如第0个链表包含的内存区为单页(2°=1), 第1个链表管理的内存区为2页, 第3个管理的内存区为4页, 依次类推。

▪ 系统命令
cat /proc/buddyinfo

==== 分配与释放
▪ alloc_pages/free_pages
参考: link:./页与页表.asc#分配页[分配页]
参考: link:./页与页表.asc#释放页[释放页]

=== slab
==== 概念
slab机制针对小内存的申请与释放。
slab是slob/slub的基础，但其本身对内存消耗较大，不适合用于小内存模型的嵌入式设备，因此又开发了slob。
slob中只有三个slob管理器，分别满足1~4K范围内的内存申请。
但由于slob效率不高，于是又提供了增强版的slub。

通常slub为系统的默认机制: CONFIG_SLUB=y。

slub的改进:
slab有三个弊端：1、每个node节点有三个链表，分别记录空闲slab、部分空闲slab和非空闲slab。当回收操作来不及时，三个链表记录的页框会较长时间停留到slab管理器中，不利于提高内存的使用率。针对这点，slub只保留了一个链表，就是部分空闲slub。2、每个cpu私有数据记录的是object的地址，这些object可能来自不同的slab，那么不利于slab的回收。slub改成记录一个实际可用的slub，不会影响其他slub的回收。3、shared共享链表可能导致一个slab持有较多slab，无法即使释放给伙伴系统。slub去掉了该链表。可见，slub出现的主要目的是为了减少slab的数量，提高内存的使用率。同时，出于对内存使用率的极致追求，slub去除了slab的着色做法，取而代之是slub复用，通过slub复用减轻cache冲突的情况。

==== 系统命令
sudo cat /proc/slabinfo
sudo slabtop -s l

==== 数据结构
===== struct kmem_cache
参考: link:./数据结构.asc#kmem_cache[kmem_cache]

==== 接口

==== 分配与释放
kmalloc/kzalloc和kmem_cache_alloc, 最终调用slab_alloc
kfree和kmem_cache_free, 最终调用slab_free

==== 参考
https://zhuanlan.zhihu.com/p/490588193

=== kmalloc/kfree
kmalloc/kfree基于slab机制。

==== kmalloc
[source,c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/slab.h
----
/**
 * kmalloc - allocate memory
 * @size: how many bytes of memory are required.
 * @flags: the type of memory to allocate.
 *
 * kmalloc is the normal method of allocating memory
 * for objects smaller than page size in the kernel.
 *
 * The allocated object address is aligned to at least ARCH_KMALLOC_MINALIGN
 * bytes. For @size of power of two bytes, the alignment is also guaranteed
 * to be at least to the size.
 *
 * The @flags argument may be one of the GFP flags defined at
 * include/linux/gfp.h and described at
 * :ref:`Documentation/core-api/mm-api.rst <mm-api-gfp-flags>`
 *
 * The recommended usage of the @flags is described at
 * :ref:`Documentation/core-api/memory-allocation.rst <memory_allocation>`
 *
 * Below is a brief outline of the most useful GFP flags
 *
 * %GFP_KERNEL
 *	Allocate normal kernel ram. May sleep.
 *
 * %GFP_NOWAIT
 *	Allocation will not sleep.
 *
 * %GFP_ATOMIC
 *	Allocation will not sleep.  May use emergency pools.
 *
 * %GFP_HIGHUSER
 *	Allocate memory from high memory on behalf of user.
 *
 * Also it is possible to set different flags by OR'ing
 * in one or more of the following additional @flags:
 *
 * %__GFP_HIGH
 *	This allocation has high priority and may use emergency pools.
 *
 * %__GFP_NOFAIL
 *	Indicate that this allocation is in no way allowed to fail
 *	(think twice before using).
 *
 * %__GFP_NORETRY
 *	If memory is not immediately available,
 *	then give up at once.
 *
 * %__GFP_NOWARN
 *	If allocation fails, don't issue any warnings.
 *
 * %__GFP_RETRY_MAYFAIL
 *	Try really hard to succeed the allocation but fail
 *	eventually.
 */
static __always_inline __alloc_size(1) void *kmalloc(size_t size, gfp_t flags)
{
	if (__builtin_constant_p(size)) {
#ifndef CONFIG_SLOB
		unsigned int index;
#endif
		if (size > KMALLOC_MAX_CACHE_SIZE)
			return kmalloc_large(size, flags);
#ifndef CONFIG_SLOB
		index = kmalloc_index(size);

		if (!index)
			return ZERO_SIZE_PTR;

		return kmalloc_trace(
				kmalloc_caches[kmalloc_type(flags)][index],
				flags, size);
#endif
	}
	return __kmalloc(size, flags);
}
----

[source,c]
.https://elixir.bootlin.com/linux/latest/source/mm/slab_common.c
----
static __always_inline
void *__do_kmalloc_node(size_t size, gfp_t flags, int node, unsigned long caller)
{
	struct kmem_cache *s;
	void *ret;

	if (unlikely(size > KMALLOC_MAX_CACHE_SIZE)) {
		ret = __kmalloc_large_node(size, flags, node);
		trace_kmalloc(caller, ret, size,
			      PAGE_SIZE << get_order(size), flags, node);
		return ret;
	}

	s = kmalloc_slab(size, flags);

	if (unlikely(ZERO_OR_NULL_PTR(s)))
		return s;

	ret = __kmem_cache_alloc_node(s, flags, node, size, caller);
	ret = kasan_kmalloc(s, ret, size, flags);
	trace_kmalloc(caller, ret, size, s->size, flags, node);
	return ret;
}
//...
void *__kmalloc(size_t size, gfp_t flags)
{
	return __do_kmalloc_node(size, flags, NUMA_NO_NODE, _RET_IP_);
}
----

==== kfree
[source,c]
.https://elixir.bootlin.com/linux/latest/source/mm/slab_common.c
----
/**
 * kfree - free previously allocated memory
 * @object: pointer returned by kmalloc.
 *
 * If @object is NULL, no operation is performed.
 *
 * Don't free memory not originally allocated by kmalloc()
 * or you will run into trouble.
 */
void kfree(const void *object)
{
	struct folio *folio;
	struct slab *slab;
	struct kmem_cache *s;

	trace_kfree(_RET_IP_, object);

	if (unlikely(ZERO_OR_NULL_PTR(object)))
		return;

	folio = virt_to_folio(object);
	if (unlikely(!folio_test_slab(folio))) {
		free_large_kmalloc(folio, (void *)object);
		return;
	}

	slab = folio_slab(folio);
	s = slab->slab_cache;
	__kmem_cache_free(s, (void *)object, _RET_IP_);
}
----

=== 碎片避免
[source,c]
.https://elixir.bootlin.com/linux/latest/source/mm/page_alloc.c
----
/*
 * This array describes the order lists are fallen back to when
 * the free lists for the desirable migrate type are depleted
 *
 * The other migratetypes do not have fallbacks.
 */
static int fallbacks[MIGRATE_TYPES][3] = {
	[MIGRATE_UNMOVABLE]   = { MIGRATE_RECLAIMABLE, MIGRATE_MOVABLE,   MIGRATE_TYPES },
	[MIGRATE_MOVABLE]     = { MIGRATE_RECLAIMABLE, MIGRATE_UNMOVABLE, MIGRATE_TYPES },
	[MIGRATE_RECLAIMABLE] = { MIGRATE_UNMOVABLE,   MIGRATE_MOVABLE,   MIGRATE_TYPES },
};
----

void set_pageblock_migratetype(struct page *page, int migratetype)
初始化的时候都是MIGRATE_MOVABLE

参考: https://www.zhihu.com/column/c_1444822980567805952

=== 热插拔
https://elixir.bootlin.com/linux/latest/source/mm/Kconfig
MEMORY_HOTPLUG
MEMORY_HOTPLUG_DEFAULT_ONLINE
MEMORY_HOTREMOVE
需要SPARSEMEM支持

=== v0.12
v0.12默认最多支持16M物理内存

16M物理内存划分:

    内核区: 0-end
    高速缓冲区: 3段 end-640K | 640K-1M(显示内存与BIOS ROM) | 1M-4M
    虚拟盘: 4M-4.5M
    主内存区: 4.5M-16M

=== 参考
https://www.kernel.org/doc/gorman/html/understand/understand005.html
https://www.ilinuxkernel.com/files/Linux_Physical_Memory_Description.pdf
https://bbs.pediy.com/thread-269149.htm
