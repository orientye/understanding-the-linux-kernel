:toc:
:toclevels: 5
:hardbreaks-option:

== 虚拟内存

=== 作用
- Caching

    It uses mainmemory efficiently by treating it as a cache for an address space stored on disk, 
    keeping only the active areas in main memory and transferring data back and forth between disk and memory as needed. 

- Memory Management

    It simplifies memory management by providing each process with a uniform address space.
    简化: 链接, 加载, 共享, 内存分配

- Memory Protection

    It protects the address space of each process from corruption by other processes.
	如果一条指令违反权限，会触发异常，发送一个SIGSEGV signal，linux称之为segmentation fault

=== 空间划分
==== 概念
虚拟空间划分为用户空间与内核空间:
不同的用户进程，虚拟空间不同
内核空间则是一样的，所有进程共享一个内核空间

以TASK_SIZE大小划分用户空间与内核空间

==== 32位虚拟空间划分
32位下的用户空间
32位下: 0xC0000000 即3G
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/page_32_types.h

32位下的内核空间
1. 3G - 3G+896M
直接进行映射的896MB物理内存其实又分为两个区域, 在低于16MB的区域, ISA设备可以做DMA, 所以该区域为DMA区域(内核为了保证ISA驱动在申请DMA缓冲区的时候, 通过GFP_DMA标记可以确保申请到16MB以内的内存, 所以必须把这个区域列为一个单独的区域管理); 16MB~896MB之间的为常规区域。 
2. 3G+896M - 4G(即high memory)
这一部分又可以划分为三部分: vmalloc区，固定映射区，临时映射区。

https://www.kernel.org/doc/html/latest/mm/highmem.html
https://linux-kernel-labs.github.io/refs/heads/master/lectures/address-space.html
ARM: https://www.arm.linux.org.uk/developer/memory.txt

==== 64位虚拟空间划分
64位下用户空间: 1<<47, 即128T(4级页表下)
[source,c]
.https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/page_64_types.h
----
#ifdef CONFIG_X86_5LEVEL
#define __VIRTUAL_MASK_SHIFT	(pgtable_l5_enabled() ? 56 : 47)
/* See task_size_max() in <asm/page_64.h> */
#else
#define __VIRTUAL_MASK_SHIFT	47
#define task_size_max()		((_AC(1,UL) << __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)
#endif
----

https://github.com/torvalds/linux/blob/master/Documentation/x86/x86_64/mm.rst

==== 虚拟空间与多进程多线程

=== 用户空间
struct mm_struct: https://elixir.bootlin.com/linux/latest/source/include/linux/mm_types.h
系统中每个进程拥有一个mm_struct

当前运行代码的二进制代码，即text段
程序使用的动态库代码
存储全局变量和动态产生的数据的堆
保存局部变量和实现函数/过程调用的栈
环境变量和命令行参数的段
将文件内容映射到虚拟地址空间中的内存映射

Q: 环境变量和命令行参数存在哪里？
https://unix.stackexchange.com/questions/75939/where-is-the-environment-string-actual-stored

布局的建立:
load_elf_binary()(execve系统调用使用了此函数): https://elixir.bootlin.com/linux/latest/source/fs/binfmt_elf.c

==== 空间切换[[空间切换]]
[source,c]
.https://elixir.bootlin.com/linux/latest/source/arch/x86/mm/tlb.c
----
void switch_mm(struct mm_struct *prev, struct mm_struct *next,
	       struct task_struct *tsk)
{
	unsigned long flags;

	local_irq_save(flags);
	switch_mm_irqs_off(prev, next, tsk);//切换地址空间
	local_irq_restore(flags);
}

void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,
			struct task_struct *tsk)
{
	struct mm_struct *real_prev = this_cpu_read(cpu_tlbstate.loaded_mm);
	u16 prev_asid = this_cpu_read(cpu_tlbstate.loaded_mm_asid);
	bool was_lazy = this_cpu_read(cpu_tlbstate_shared.is_lazy);
	unsigned cpu = smp_processor_id();
	u64 next_tlb_gen;
	bool need_flush;
	u16 new_asid;

	/*
	 * NB: The scheduler will call us with prev == next when switching
	 * from lazy TLB mode to normal mode if active_mm isn't changing.
	 * When this happens, we don't assume that CR3 (and hence
	 * cpu_tlbstate.loaded_mm) matches next.
	 *
	 * NB: leave_mm() calls us with prev == NULL and tsk == NULL.
	 */

	/* We don't want flush_tlb_func() to run concurrently with us. */
	if (IS_ENABLED(CONFIG_PROVE_LOCKING))
		WARN_ON_ONCE(!irqs_disabled());

	/*
	 * Verify that CR3 is what we think it is.  This will catch
	 * hypothetical buggy code that directly switches to swapper_pg_dir
	 * without going through leave_mm() / switch_mm_irqs_off() or that
	 * does something like write_cr3(read_cr3_pa()).
	 *
	 * Only do this check if CONFIG_DEBUG_VM=y because __read_cr3()
	 * isn't free.
	 */
#ifdef CONFIG_DEBUG_VM
	if (WARN_ON_ONCE(__read_cr3() != build_cr3(real_prev->pgd, prev_asid))) {
		/*
		 * If we were to BUG here, we'd be very likely to kill
		 * the system so hard that we don't see the call trace.
		 * Try to recover instead by ignoring the error and doing
		 * a global flush to minimize the chance of corruption.
		 *
		 * (This is far from being a fully correct recovery.
		 *  Architecturally, the CPU could prefetch something
		 *  back into an incorrect ASID slot and leave it there
		 *  to cause trouble down the road.  It's better than
		 *  nothing, though.)
		 */
		__flush_tlb_all();
	}
#endif
	if (was_lazy)
		this_cpu_write(cpu_tlbstate_shared.is_lazy, false);

	/*
	 * The membarrier system call requires a full memory barrier and
	 * core serialization before returning to user-space, after
	 * storing to rq->curr, when changing mm.  This is because
	 * membarrier() sends IPIs to all CPUs that are in the target mm
	 * to make them issue memory barriers.  However, if another CPU
	 * switches to/from the target mm concurrently with
	 * membarrier(), it can cause that CPU not to receive an IPI
	 * when it really should issue a memory barrier.  Writing to CR3
	 * provides that full memory barrier and core serializing
	 * instruction.
	 */
	if (real_prev == next) {
		VM_WARN_ON(this_cpu_read(cpu_tlbstate.ctxs[prev_asid].ctx_id) !=
			   next->context.ctx_id);

		/*
		 * Even in lazy TLB mode, the CPU should stay set in the
		 * mm_cpumask. The TLB shootdown code can figure out from
		 * cpu_tlbstate_shared.is_lazy whether or not to send an IPI.
		 */
		if (WARN_ON_ONCE(real_prev != &init_mm &&
				 !cpumask_test_cpu(cpu, mm_cpumask(next))))
			cpumask_set_cpu(cpu, mm_cpumask(next));

		/*
		 * If the CPU is not in lazy TLB mode, we are just switching
		 * from one thread in a process to another thread in the same
		 * process. No TLB flush required.
		 */
		if (!was_lazy)
			return;

		/*
		 * Read the tlb_gen to check whether a flush is needed.
		 * If the TLB is up to date, just use it.
		 * The barrier synchronizes with the tlb_gen increment in
		 * the TLB shootdown code.
		 */
		smp_mb();
		next_tlb_gen = atomic64_read(&next->context.tlb_gen);
		if (this_cpu_read(cpu_tlbstate.ctxs[prev_asid].tlb_gen) ==
				next_tlb_gen)
			return;

		/*
		 * TLB contents went out of date while we were in lazy
		 * mode. Fall through to the TLB switching code below.
		 */
		new_asid = prev_asid;
		need_flush = true;
	} else {
		/*
		 * Apply process to process speculation vulnerability
		 * mitigations if applicable.
		 */
		cond_mitigation(tsk);

		/*
		 * Stop remote flushes for the previous mm.
		 * Skip kernel threads; we never send init_mm TLB flushing IPIs,
		 * but the bitmap manipulation can cause cache line contention.
		 */
		if (real_prev != &init_mm) {
			VM_WARN_ON_ONCE(!cpumask_test_cpu(cpu,
						mm_cpumask(real_prev)));
			cpumask_clear_cpu(cpu, mm_cpumask(real_prev));
		}

		/*
		 * Start remote flushes and then read tlb_gen.
		 */
		if (next != &init_mm)
			cpumask_set_cpu(cpu, mm_cpumask(next));
		next_tlb_gen = atomic64_read(&next->context.tlb_gen);

		choose_new_asid(next, next_tlb_gen, &new_asid, &need_flush);

		/* Let nmi_uaccess_okay() know that we're changing CR3. */
		this_cpu_write(cpu_tlbstate.loaded_mm, LOADED_MM_SWITCHING);
		barrier();
	}

	if (need_flush) {
		this_cpu_write(cpu_tlbstate.ctxs[new_asid].ctx_id, next->context.ctx_id);
		this_cpu_write(cpu_tlbstate.ctxs[new_asid].tlb_gen, next_tlb_gen);
		load_new_mm_cr3(next->pgd, new_asid, true);

		trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
	} else {
		/* The new ASID is already up to date. */
		load_new_mm_cr3(next->pgd, new_asid, false);

		trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, 0);
	}

	/* Make sure we write CR3 before loaded_mm. */
	barrier();

	this_cpu_write(cpu_tlbstate.loaded_mm, next);
	this_cpu_write(cpu_tlbstate.loaded_mm_asid, new_asid);

	if (next != real_prev) {
		cr4_update_pce_mm(next);
		switch_ldt(real_prev, next);
	}
}
----

CR3 register:
CR3 register contains the physical address of the base address of the page directory table. This value is unique for each running process, since every process has it’s own page table.

http://www.wowotech.net/process_management/context-switch-arch.html

=== 内核空间
==== 概要
内核空间不属于任何一个特定的进程，因此，单独设置了一个内核专用的mm_struct, 即init_mm。

==== vmalloc/vfree[[vmallocvfree]]
vmalloc提供了内核虚拟地址连续但物理地址不一定连续的区域。
vmalloc从内核的虚存空间分配一块虚存以及相应的物理内存，vmalloc分配的空间不会被kswapd换出，kswapd只扫描各个进程的用户空间，通过vmalloc分配的页面表项对其不可见。

https://elixir.bootlin.com/linux/latest/source/include/linux/vmalloc.h

VMALLOC_START与VMALLOC_END:
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/pgtable_32_areas.h
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/pgtable_64_types.h

参考: https://www.cnblogs.com/LoyenWang/p/11965787.html

=== 内存映射
==== 作用与原理
Linux initializes the contents of a virtual memory area by associating it with an object on disk, a process known as memory mapping. 

Areas can be mapped to one of two types of objects:

1. Regular file in the Linux file system: An area can be mapped to a contiguous section of a regular disk file, such as an executable object file. The file section is divided into page-size pieces, with each piece containing the initial contents of a virtual page. Because of demand paging, none of these virtual pages is actually swapped into physical memory until the CPU first touches the page (i.e., issues a virtual address that falls within that page’s region of the address space). If the area is larger than the file section, then the area is padded with zeros. 适合于很大的文件

2. Anonymous file: An area can also be mapped to an anonymous file, created by the kernel, that contains all binary zeros. The first time the CPU touches a virtual page in such an area, the kernel finds an appropriate victim page in physical memory, swaps out the victim page if it is dirty, overwrites the victim page with binary zeros, and updates the page table to mark the page as resident. Notice that no data are actually transferred between disk and memory. For this reason, pages in areas that are mapped to anonymous files are sometimes called demand-zero pages. 适合于创建进程间通信的共享内存。

In either case, once a virtual page is initialized, it is swapped back and forthbetween a special swap file maintained by the kernel. The swap file is also known as the swap space or the swap area. An important point to realize is that at any point in time, the swap space bounds the total amount of virtual pages that can be allocated by the currently running processes.

内存映射是一种重要的抽象，在内核和应用程序中均大量使用。内存映射将数据映射到进程的虚拟地址空间中。作为映射目标的地址空间区域，对改区域的内存修改都会自动同步到数据源。例如，文件的内容映射到内存中，处理只需要读取相应的内存即可访问文件内容，或向内存写入数据来修改文件的内容，内核保证任何修改都会自动同步到文件中。内核在实现设备驱动程序时直接使用了内存映射，外设的输入/输出可以映射到虚拟地址空间的区域中，对相关内存区域的都写会由系统重定向到设备，因而大大简化了驱动程序的实现。

▪ 问题
内存映射文件需要在进程上占用一块很大的连续地址空间。对于Intel的IA-32的4G逻辑地址空间，可用的连续地址空间远远小于2-3G。
相关联的文件的I/O错误(如可拔出驱动器或光驱被弹出，磁盘满时写操作等)的内存映射文件会向应用程序报告SIGSEGV/SIGBUS信号(POSIX环境)或EXECUTE_IN_PAGE_ERROR结构化异常(Windows环境)。通常的内存操作是无需考虑这些异常的。
有内存管理单元(MMU)才支持内存映射文件。

参考: CSAPP 3rd, 9.8
参考: https://linux-kernel-labs.github.io/refs/heads/master/labs/memory_mapping.html

==== 用户空间映射

==== 内核空间映射[[内核空间映射]]
===== 持久内核映射
kmap(): https://elixir.bootlin.com/linux/latest/source/include/linux/highmem-internal.h
kmap函数不能用于中断处理程序, 因为它可能进入睡眠状态

===== 固定内存映射/临时内核映射
kmap_atomic(): https://elixir.bootlin.com/linux/latest/source/include/linux/highmem-internal.h
kmap_atomic函数不能用于可能进入睡眠的代码

===== 没有高端内存的计算机上的映射函数
许多体系结构不支持高端内存, 例如64位体系结构: 内核提供了兼容宏CONFIG_HIGHMEM

===== 外部设备存储空间的地址映射
一般来说，对外部设备的访问有两种不同的形式，一种叫做内存映射式(memory mapped), 另一种叫做IO映射式(I/O mapped)。

对于内存映射式，外部设备的存储单元例如控制寄存器，状态寄存器，数据寄存器等，是作为内存的一部分出现在系统中的。CPU可以像访问一个内存单元一样访问外部设备的内存单元，因此不需要专门用于外设I/O的指令。

对于IO映射式，外部设备的存储单元与内存属于两个不同的体系，访问内存的指令不能用于访问外部设备的存储单元，例如X86的IN和OUT指令， 但通常用于IO指令的地址空间相对来说是很小的。可以说，IO映射式只适合与早期的计算机技术，彼时外设通常只有几个寄存器，通过这几个寄存器就能完成对外设的所有操作。随着计算机技术的发展，单纯的IO映射方式显然已经不能满足需求了。

尤其随着显卡以及PCI总线的出现，不管是采用IO映射还是内存映射，都需要将外设上的存储器映射到内存空间。在linux内核中，通过函数ioremap()来建立这个映射。

https://elixir.bootlin.com/linux/latest/source/arch/x86/mm/ioremap.c

    ioremap()
    iounmap()

=== v0.12
v0.12默认最多支持64M逻辑地址空间

=== 参考
https://lwn.net/Articles/253361/
https://people.freebsd.org/~lstewart/articles/cpumemory.pdf