:toc:
:toclevels: 5
:hardbreaks-option:

== 虚拟内存

=== 作用
- Caching

    It uses mainmemory efficiently by treating it as a cache for an address space stored on disk, 
    keeping only the active areas in main memory and transferring data back and forth between disk and memory as needed. 

- Memory Management

    It simplifies memory management by providing each process with a uniform address space.
    简化: 链接, 加载, 共享, 内存分配

- Memory Protection

    It protects the address space of each process from corruption by other processes.
    如果一条指令违反权限，会触发异常，发送一个SIGSEGV signal，linux称之为segmentation fault

=== 地址转换

==== 历史与现状
Early PCs used physical addressing, and systems such as digital signal processors, embedded microcontrollers, and Cray supercomputers continue to do so. However, modern processors use a form of addressing known as virtual addressing.

==== 流程

===== MMU
.Image source: CSAPP chapter9
image::img/address-translation.png[]

===== TLB
.Image source: CSAPP chapter9
image::img/address-translation-tlb.png[]

===== 加速
▪ TLB
▪ 大页
▪ 内核空间上的线性映射

===== The Intel Core i7/Linux Memory System
.Image source: CSAPP chapter9
image::img/core-i7-memory-system.png[]

.Image source: CSAPP chapter9
image::img/core-i7-address-translation.png[]

Q: 为什么x86-64下虚拟地址比物理地址少4位？
https://stackoverflow.com/questions/46509152/why-in-x86-64-the-virtual-address-are-4-bits-shorter-than-physical-48-bits-vs

==== 虚拟地址
▪ x86_64:
虚拟地址: |PGD|PUD|PMD|PTE|OFFSET|
48bit: |9bit|9bit|9bit|9bit|12bit|
https://github.com/torvalds/linux/blob/master/Documentation/x86/x86_64/mm.rst
5级页表: https://en.wikipedia.org/wiki/Intel_5-level_paging

Q: 用户空间能获得某个虚拟地址的物理地址吗？
https://kongkongk.github.io/2020/06/30/address-translation/
Q: 内核空间能获得某个虚拟地址的物理地址吗？
__pa(x)与__va(x)
[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/page.h
----
#ifndef __pa
#define __pa(x)		__phys_addr((unsigned long)(x))
#endif

#ifndef __va
#define __va(x)			((void *)((unsigned long)(x)+PAGE_OFFSET))
#endif
----

[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/page_64.h
----
static __always_inline unsigned long __phys_addr_nodebug(unsigned long x)
{
	unsigned long y = x - __START_KERNEL_map;

	/* use the carry flag to determine if x was < __START_KERNEL_map */
	x = y + ((x > y) ? phys_base : (__START_KERNEL_map - PAGE_OFFSET));

	return x;
}

#ifdef CONFIG_DEBUG_VIRTUAL
extern unsigned long __phys_addr(unsigned long);
extern unsigned long __phys_addr_symbol(unsigned long);
#else
#define __phys_addr(x)		__phys_addr_nodebug(x)
#define __phys_addr_symbol(x) \
	((unsigned long)(x) - __START_KERNEL_map + phys_base)
#endif
----

[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/page_64_types.h
----
#define __START_KERNEL_map	_AC(0xffffffff80000000, UL)
----

==== 参考
https://linux-kernel-labs.github.io/refs/heads/master/lectures/address-space.html

=== 空间划分
==== 概念
虚拟空间划分为用户空间与内核空间:
1. 不同的用户进程，虚拟空间不同
2. 内核空间则是一样的，所有进程共享一个内核空间
3. 以TASK_SIZE大小划分用户空间与内核空间

==== 32位虚拟空间划分
32位下的用户空间
32位下: 0xC0000000 即3G
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/page_32_types.h

32位下的内核空间
1. 3G - 3G+896M
直接进行映射的896MB物理内存其实又分为两个区域, 在低于16MB的区域, ISA设备可以做DMA, 所以该区域为DMA区域(内核为了保证ISA驱动在申请DMA缓冲区的时候, 通过GFP_DMA标记可以确保申请到16MB以内的内存, 所以必须把这个区域列为一个单独的区域管理); 16MB~896MB之间的为常规区域。 
2. 3G+896M - 4G(即high memory)
这一部分又可以划分为三部分:
VMALLOC_START ~ VMALLOC_END
KMAP_BASE ~ FIXADDR_START
FIXADDR_START ~ 0xFFFFFFFF

参考: https://www.kernel.org/doc/html/latest/mm/highmem.html

==== 64位虚拟空间划分
64位下用户空间: 1<<47, 即128T(4级页表下)
[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/page_64_types.h
----
#ifdef CONFIG_X86_5LEVEL
#define __VIRTUAL_MASK_SHIFT	(pgtable_l5_enabled() ? 56 : 47)
/* See task_size_max() in <asm/page_64.h> */
#else
#define __VIRTUAL_MASK_SHIFT	47
#define task_size_max()		((_AC(1,UL) << __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)
#endif
----

随机性: CONFIG_DYNAMIC_MEMORY_LAYOUT
Note that if CONFIG_RANDOMIZE_MEMORY is enabled, the direct mapping of all physical memory, vmalloc/ioremap space and virtual memory map are randomized. Their order is preserved but their base will be offset early at boot time.

参考: https://github.com/torvalds/linux/blob/master/Documentation/x86/x86_64/mm.rst
参考: https://github.com/torvalds/linux/blob/master/Documentation/arm64/memory.rst

==== 参考
https://linux-kernel-labs.github.io/refs/heads/master/lectures/address-space.html
https://www.arm.linux.org.uk/developer/memory.txt

=== 用户空间

==== 数据结构
每个用户进程拥有一个struct mm_struct
一个用户进程下的多线程，共享进程的虚拟空间

==== 布局
布局:
当前运行代码的二进制代码
程序使用的动态库代码
存储全局变量和动态产生的数据的堆
保存局部变量和实现函数/过程调用的栈
环境变量和命令行参数
将文件内容映射到虚拟地址空间中的内存映射

布局的建立:
load_elf_binary()(execve系统调用使用了此函数): https://elixir.bootlin.com/linux/latest/source/fs/binfmt_elf.c

Q: 环境变量和命令行参数存在哪里？
https://unix.stackexchange.com/questions/75939/where-is-the-environment-string-actual-stored

==== 多线程
对于用户进程，主线程堆栈也称进程栈；非主线程堆栈也叫线程栈。
对于同一个用户进程下的多线程，共享进程的虚拟空间，主线程的堆栈就是进程的堆栈。
非主线程的堆栈又是如何分配的呢？
一般是从heap的顶部附近向下分配8M大小，多个(非主)线程之间会有少量间隔填充。

==== 空间切换[[空间切换]]
void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next, struct task_struct *tsk): 切换地址空间

===== x86
[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/x86/mm/tlb.c
----
void switch_mm(struct mm_struct *prev, struct mm_struct *next,
	       struct task_struct *tsk)
{
	unsigned long flags;

	local_irq_save(flags);
	switch_mm_irqs_off(prev, next, tsk);
	local_irq_restore(flags);
}

void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,
			struct task_struct *tsk)
{
	struct mm_struct *real_prev = this_cpu_read(cpu_tlbstate.loaded_mm);
	u16 prev_asid = this_cpu_read(cpu_tlbstate.loaded_mm_asid);
	bool was_lazy = this_cpu_read(cpu_tlbstate_shared.is_lazy);
	unsigned cpu = smp_processor_id();
	u64 next_tlb_gen;
	bool need_flush;
	u16 new_asid;

	/*
	 * NB: The scheduler will call us with prev == next when switching
	 * from lazy TLB mode to normal mode if active_mm isn't changing.
	 * When this happens, we don't assume that CR3 (and hence
	 * cpu_tlbstate.loaded_mm) matches next.
	 *
	 * NB: leave_mm() calls us with prev == NULL and tsk == NULL.
	 */

	/* We don't want flush_tlb_func() to run concurrently with us. */
	if (IS_ENABLED(CONFIG_PROVE_LOCKING))
		WARN_ON_ONCE(!irqs_disabled());

	/*
	 * Verify that CR3 is what we think it is.  This will catch
	 * hypothetical buggy code that directly switches to swapper_pg_dir
	 * without going through leave_mm() / switch_mm_irqs_off() or that
	 * does something like write_cr3(read_cr3_pa()).
	 *
	 * Only do this check if CONFIG_DEBUG_VM=y because __read_cr3()
	 * isn't free.
	 */
#ifdef CONFIG_DEBUG_VM
	if (WARN_ON_ONCE(__read_cr3() != build_cr3(real_prev->pgd, prev_asid))) {
		/*
		 * If we were to BUG here, we'd be very likely to kill
		 * the system so hard that we don't see the call trace.
		 * Try to recover instead by ignoring the error and doing
		 * a global flush to minimize the chance of corruption.
		 *
		 * (This is far from being a fully correct recovery.
		 *  Architecturally, the CPU could prefetch something
		 *  back into an incorrect ASID slot and leave it there
		 *  to cause trouble down the road.  It's better than
		 *  nothing, though.)
		 */
		__flush_tlb_all();
	}
#endif
	if (was_lazy)
		this_cpu_write(cpu_tlbstate_shared.is_lazy, false);

	/*
	 * The membarrier system call requires a full memory barrier and
	 * core serialization before returning to user-space, after
	 * storing to rq->curr, when changing mm.  This is because
	 * membarrier() sends IPIs to all CPUs that are in the target mm
	 * to make them issue memory barriers.  However, if another CPU
	 * switches to/from the target mm concurrently with
	 * membarrier(), it can cause that CPU not to receive an IPI
	 * when it really should issue a memory barrier.  Writing to CR3
	 * provides that full memory barrier and core serializing
	 * instruction.
	 */
	if (real_prev == next) {
		VM_WARN_ON(this_cpu_read(cpu_tlbstate.ctxs[prev_asid].ctx_id) !=
			   next->context.ctx_id);

		/*
		 * Even in lazy TLB mode, the CPU should stay set in the
		 * mm_cpumask. The TLB shootdown code can figure out from
		 * cpu_tlbstate_shared.is_lazy whether or not to send an IPI.
		 */
		if (WARN_ON_ONCE(real_prev != &init_mm &&
				 !cpumask_test_cpu(cpu, mm_cpumask(next))))
			cpumask_set_cpu(cpu, mm_cpumask(next));

		/*
		 * If the CPU is not in lazy TLB mode, we are just switching
		 * from one thread in a process to another thread in the same
		 * process. No TLB flush required.
		 */
		if (!was_lazy)
			return;

		/*
		 * Read the tlb_gen to check whether a flush is needed.
		 * If the TLB is up to date, just use it.
		 * The barrier synchronizes with the tlb_gen increment in
		 * the TLB shootdown code.
		 */
		smp_mb();
		next_tlb_gen = atomic64_read(&next->context.tlb_gen);
		if (this_cpu_read(cpu_tlbstate.ctxs[prev_asid].tlb_gen) ==
				next_tlb_gen)
			return;

		/*
		 * TLB contents went out of date while we were in lazy
		 * mode. Fall through to the TLB switching code below.
		 */
		new_asid = prev_asid;
		need_flush = true;
	} else {
		/*
		 * Apply process to process speculation vulnerability
		 * mitigations if applicable.
		 */
		cond_mitigation(tsk);

		/*
		 * Stop remote flushes for the previous mm.
		 * Skip kernel threads; we never send init_mm TLB flushing IPIs,
		 * but the bitmap manipulation can cause cache line contention.
		 */
		if (real_prev != &init_mm) {
			VM_WARN_ON_ONCE(!cpumask_test_cpu(cpu,
						mm_cpumask(real_prev)));
			cpumask_clear_cpu(cpu, mm_cpumask(real_prev));
		}

		/*
		 * Start remote flushes and then read tlb_gen.
		 */
		if (next != &init_mm)
			cpumask_set_cpu(cpu, mm_cpumask(next));
		next_tlb_gen = atomic64_read(&next->context.tlb_gen);

		choose_new_asid(next, next_tlb_gen, &new_asid, &need_flush);

		/* Let nmi_uaccess_okay() know that we're changing CR3. */
		this_cpu_write(cpu_tlbstate.loaded_mm, LOADED_MM_SWITCHING);
		barrier();
	}

	if (need_flush) {
		this_cpu_write(cpu_tlbstate.ctxs[new_asid].ctx_id, next->context.ctx_id);
		this_cpu_write(cpu_tlbstate.ctxs[new_asid].tlb_gen, next_tlb_gen);
		load_new_mm_cr3(next->pgd, new_asid, true);

		trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
	} else {
		/* The new ASID is already up to date. */
		load_new_mm_cr3(next->pgd, new_asid, false);

		trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, 0);
	}

	/* Make sure we write CR3 before loaded_mm. */
	barrier();

	this_cpu_write(cpu_tlbstate.loaded_mm, next);
	this_cpu_write(cpu_tlbstate.loaded_mm_asid, new_asid);

	if (next != real_prev) {
		cr4_update_pce_mm(next);
		switch_ldt(real_prev, next);
	}
}
----

▪ CR3 register
CR3 register contains the physical address(物理地址) of the base address of the page directory table. This value is unique for each running process, since every process has it’s own page table.

cr3里面存放当前进程的顶级pgd的物理内存地址，用户进程在运行的过程中访问虚拟内存中的数据，会被cr3里面指向的页表转换为物理地址，之后在物理内存中访问数据，这个过程在用户态运行的，这个地址转换的过程无需进入内核态，当然，如果物理内存中不存在，就会走缺页异常的流程。

load_new_mm_cr3()会加载新(next)的pgd的物理内存地址到cr3寄存器:
load_new_mm_cr3最终会调用:
#define __sme_pa(x)		(__pa(x) | sme_me_mask) : https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/mem_encrypt.h

▪ ASID
ASID(address space ID)。原来TLB查找是通过虚拟地址VA来判断是否TLB hit。有了ASID的支持后，TLB hit的判断标准修改为（虚拟地址＋ASID），ASID是每一个进程分配一个，标识自己的进程地址空间。TLB block如何知道一个tlb entry的ASID呢？一般会来自CPU的系统寄存器（对于ARM64平台，它来自TTBRx_EL1寄存器），这样在TLB block在缓存（VA-PA-Global flag）的同时，也就把当前的ASID缓存在了对应的TLB entry中，这样一个TLB entry中包括了（VA-PA-Global flag-ASID）。

有了ASID的支持后，A进程切换到B进程再也不需要flush tlb了，因为A进程执行时候缓存在TLB中的残留A地址空间相关的entry不会影响到B进程，虽然A和B可能有相同的VA，但是ASID保证了硬件可以区分A和B进程地址空间。
参考: http://www.wowotech.net/process_management/context-switch-tlb.html

===== ARM64
对于arm64架构来说，有两个页表基址寄存器ttbr0_el1和ttbr1_el1, ttbr0_el1用来存放用户地址空间的页表基地址，在每次调度上下文切换的时候从tsk->mm->pgd加载，ttbr1_el1是内核地址空间的页表基地址，内核初始化完成之后存放swapper_pg_dir的地址。
内核线程共享内核地址空间，也只能访问内核地址空间，使用swapper_pg_dir去查询页表就可以，而对于arm64来说swapper_pg_dir在内核初始化的时候被加载到ttbr1_le1中，一旦内核线程访问内核虚拟地址，则mmu就会从ttbr1_le1指向的页表基地址开始查询各级页表，进行正常的虚实地址转换。当然，上面是arm64这种架构的处理，它有两个页表基地址寄存器，其他很多处理器如x86, riscv处理器架构都只有一个页表基址寄存器，如x86的cr3，那么这个时候怎么办呢？答案是：使用内核线程借用的prev->active_mm来做，实际上前一个用户任务（记住：不一定是上一个，有可能上上个任务才是用户任务）的active_mm=mm,当切换到前一个用户任务的时候就会将tsk->mm->pgd放到cr3, 对于x86这样的只有一个页表基址寄存器的处理器架构来说，tsk->mm->pgd存放的是整个虚拟地址空间的页表基地址，在fork的时候会将主内核页表的pgd表项拷贝到tsk->mm->pgd对于表项中（可以看看fork的copy_mm相关代码，对于arm64这样的架构没有做内核页表同步）。

[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/mmu_context.h
----
/* Architectures that care about IRQ state in switch_mm can override this. */
#ifndef switch_mm_irqs_off
# define switch_mm_irqs_off switch_mm
#endif
----

[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/arm64/include/asm/mmu_context.h
----
static inline void __switch_mm(struct mm_struct *next)
{
	/*
	 * init_mm.pgd does not contain any user mappings and it is always
	 * active for kernel addresses in TTBR1. Just set the reserved TTBR0.
	 */
	if (next == &init_mm) {
		cpu_set_reserved_ttbr0();
		return;
	}

	check_and_switch_context(next);
}

static inline void
switch_mm(struct mm_struct *prev, struct mm_struct *next,
	  struct task_struct *tsk)
{
	if (prev != next)
		__switch_mm(next);

	/*
	 * Update the saved TTBR0_EL1 of the scheduled-in task as the previous
	 * value may have not been initialised yet (activate_mm caller) or the
	 * ASID has changed since the last run (following the context switch
	 * of another thread of the same process).
	 */
	update_saved_ttbr0(tsk, next);
}
----

[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/arm64/mm/context.c
----
void check_and_switch_context(struct mm_struct *mm)
{
	unsigned long flags;
	unsigned int cpu;
	u64 asid, old_active_asid;

	if (system_supports_cnp())
		cpu_set_reserved_ttbr0();

	asid = atomic64_read(&mm->context.id);

	/*
	 * The memory ordering here is subtle.
	 * If our active_asids is non-zero and the ASID matches the current
	 * generation, then we update the active_asids entry with a relaxed
	 * cmpxchg. Racing with a concurrent rollover means that either:
	 *
	 * - We get a zero back from the cmpxchg and end up waiting on the
	 *   lock. Taking the lock synchronises with the rollover and so
	 *   we are forced to see the updated generation.
	 *
	 * - We get a valid ASID back from the cmpxchg, which means the
	 *   relaxed xchg in flush_context will treat us as reserved
	 *   because atomic RmWs are totally ordered for a given location.
	 */
	old_active_asid = atomic64_read(this_cpu_ptr(&active_asids));
	if (old_active_asid && asid_gen_match(asid) &&
	    atomic64_cmpxchg_relaxed(this_cpu_ptr(&active_asids),
				     old_active_asid, asid))
		goto switch_mm_fastpath;

	raw_spin_lock_irqsave(&cpu_asid_lock, flags);
	/* Check that our ASID belongs to the current generation. */
	asid = atomic64_read(&mm->context.id);
	if (!asid_gen_match(asid)) {
		asid = new_context(mm);
		atomic64_set(&mm->context.id, asid);
	}

	cpu = smp_processor_id();
	if (cpumask_test_and_clear_cpu(cpu, &tlb_flush_pending))
		local_flush_tlb_all();

	atomic64_set(this_cpu_ptr(&active_asids), asid);
	raw_spin_unlock_irqrestore(&cpu_asid_lock, flags);

switch_mm_fastpath:

	arm64_apply_bp_hardening();

	/*
	 * Defer TTBR0_EL1 setting for user threads to uaccess_enable() when
	 * emulating PAN.
	 */
	if (!system_uses_ttbr0_pan())
		cpu_switch_mm(mm->pgd, mm);
}
----

参考: https://zhuanlan.zhihu.com/p/373959024
参考: https://mp.weixin.qq.com/s/pmWuGS6thCj6GNwwjh0bRw

=== 内核空间

==== x86-64内核空间布局
https://github.com/torvalds/linux/blob/master/Documentation/x86/x86_64/mm.rst

========================================================================================================================
    Start addr    |   Offset   |     End addr     |  Size   | VM area description
========================================================================================================================
                  |            |                  |         |
 0000000000000000 |    0       | 00007fffffffffff |  128 TB | user-space virtual memory, different per mm
__________________|____________|__________________|_________|___________________________________________________________
                  |            |                  |         |
 0000800000000000 | +128    TB | ffff7fffffffffff | ~16M TB | ... huge, almost 64 bits wide hole of non-canonical
                  |            |                  |         |     virtual memory addresses up to the -128 TB
                  |            |                  |         |     starting offset of kernel mappings.
__________________|____________|__________________|_________|___________________________________________________________
                                                            |
                                                            | Kernel-space virtual memory, shared between all processes:
____________________________________________________________|___________________________________________________________
                  |            |                  |         |
 ffff800000000000 | -128    TB | ffff87ffffffffff |    8 TB | ... guard hole, also reserved for hypervisor
 ffff880000000000 | -120    TB | ffff887fffffffff |  0.5 TB | LDT remap for PTI
 ffff888000000000 | -119.5  TB | ffffc87fffffffff |   64 TB | direct mapping of all physical memory (page_offset_base)
 ffffc88000000000 |  -55.5  TB | ffffc8ffffffffff |  0.5 TB | ... unused hole
 ffffc90000000000 |  -55    TB | ffffe8ffffffffff |   32 TB | vmalloc/ioremap space (vmalloc_base)
 ffffe90000000000 |  -23    TB | ffffe9ffffffffff |    1 TB | ... unused hole
 ffffea0000000000 |  -22    TB | ffffeaffffffffff |    1 TB | virtual memory map (vmemmap_base)
 ffffeb0000000000 |  -21    TB | ffffebffffffffff |    1 TB | ... unused hole
 ffffec0000000000 |  -20    TB | fffffbffffffffff |   16 TB | KASAN shadow memory
__________________|____________|__________________|_________|____________________________________________________________
                                                            |
                                                            | Identical layout to the 56-bit one from here on:
____________________________________________________________|____________________________________________________________
                  |            |                  |         |
 fffffc0000000000 |   -4    TB | fffffdffffffffff |    2 TB | ... unused hole
                  |            |                  |         | vaddr_end for KASLR
 fffffe0000000000 |   -2    TB | fffffe7fffffffff |  0.5 TB | cpu_entry_area mapping
 fffffe8000000000 |   -1.5  TB | fffffeffffffffff |  0.5 TB | ... unused hole
 ffffff0000000000 |   -1    TB | ffffff7fffffffff |  0.5 TB | %esp fixup stacks
 ffffff8000000000 | -512    GB | ffffffeeffffffff |  444 GB | ... unused hole
 ffffffef00000000 |  -68    GB | fffffffeffffffff |   64 GB | EFI region mapping space
 ffffffff00000000 |   -4    GB | ffffffff7fffffff |    2 GB | ... unused hole
 ffffffff80000000 |   -2    GB | ffffffff9fffffff |  512 MB | kernel text mapping, mapped to physical address 0
 ffffffff80000000 |-2048    MB |                  |         |
 ffffffffa0000000 |-1536    MB | fffffffffeffffff | 1520 MB | module mapping space
 ffffffffff000000 |  -16    MB |                  |         |
    FIXADDR_START | ~-11    MB | ffffffffff5fffff | ~0.5 MB | kernel-internal fixmap range, variable size and offset
 ffffffffff600000 |  -10    MB | ffffffffff600fff |    4 kB | legacy vsyscall ABI
 ffffffffffe00000 |   -2    MB | ffffffffffffffff |    2 MB | ... unused hole
__________________|____________|__________________|_________|___________________________________________________________

内核空间虚拟地址从低到高依次为:

▪ hypervisor: 8 TB
虚拟机相关?

▪ LDT remap for PTI: 0.5 TB
LDT remap for PTI , PTI(page table isolation), 在kernel mode切换到user space时，user space的page table需要保留一些能够进入kernel space的入口地址，比如syscall, IDT的内存mapping, 内核会复制一份缩减版本的内核page table返回给user space, 这样一定程度保证安全。该区域是user space能看到的映射到LDT的区域。
https://en.wikipedia.org/wiki/Kernel_page-table_isolation
https://docs.kernel.org/x86/pti.html
http://happyseeker.github.io/kernel/2018/04/18/pti-code.html

▪ direct mapping of all physical memory: 直接映射，64T，起始地址page_offset_base
▪ vmalloc/ioremap space: 32T，起始地址vmalloc_base
▪ virtual memory map: 1T，起始地址vmemmap_base

▪ KASAN shadow memory: 16TB
KASAN: The Kernel Address Sanitizer
KASAN用来检测use-after-free与out-of-bounds问题。
KASAN使用影子内存(shadow memory)记录内存的每个字节是否可以安全访问，使用编译时插桩在每次访问内存时检查影子内存。
KASAN使用内核地址空间的1/8作为影子内存，影子内存的每个字节记录内存连续8字节的状态。
https://github.com/torvalds/linux/blob/master/Documentation/dev-tools/kasan.rst
https://en.wikipedia.org/wiki/AddressSanitizer#KernelAddressSanitizer

▪ KASLR:
Kernel address space layout randomization
https://elixir.bootlin.com/linux/latest/source/arch/x86/mm/kaslr.c
https://lwn.net/Articles/569635/
https://en.wikipedia.org/wiki/Address_space_layout_randomization

▪ cpu_entry_area: 0.5 TB, GDT/TSS(task state segment)/syscall等
▪ esp fixup stacks: 0.5 TB, 防止16-bit userspace stack出现error
▪ EFI region mapping space: 64 GB, EFI runtime 使用的单独的PGD
▪ kernel text mapping: 512 MB, kernel代码映射区间，映射到物理地址0
▪ module mapping space: 1520 MB, 动态模块的mapping区间
▪ kernel-internal fixmap range: ~0.5 MB kernel内部固定映射区间
▪ legacy vsyscall ABI: 4 kB, 废弃的vsyscall的映射区间，用于系统调用加速

==== ARM64
https://github.com/torvalds/linux/blob/master/Documentation/arm64/memory.rst

==== 线性映射与非线性映射
按照连续的虚拟空间对应的物理内存是否连续，内核空间可以划分成两类:
一部分其物理内存也是连续的，另一部分则物理内存不连续。
前者为线性映射/直接映射，后者为非线性映射/非直接映射。

Q: 为什么不全部使用线性映射呢?
在32位处理器下，按照经典用户态与内核3:1的划分比例，内核能够使用的虚拟地址只有1G，按照固定offset的映射方式，这意味着内核能够使用的物理地址大小也只有1G。但随着内核越来越复杂，内存技术的发展使得高于4G的内存变得十分常见，受限于32位系统与这种线性映射，内核可用的物理内存大小限制在1G。
这也正是high memory出现的原因。"high memory"要解决的是32位下虚拟地址空间不足带来的问题(显然64位系统这个问题就不存在了)。实际上在很早以前这个问题就在lwn上讨论过了，在当时已经有一些临时的方法去规避这个问题，比如重新划分用户/内核的地址空间比例，变为2.5:1.5等等，但在特定场景下(比如用户态使用的内存非常非常多)会使得用户态运行效率降低，因此也不是一个很好的办法。怎么解决呢? 可以把这1G，划分成两部分，一部分用来线性映射，一部分用来非线性映射。以x86为例，实际中的做法是，0xC0000000-0xF7FFFFFF的896MB用作线性映射，0xF8000000-0xFFFFFFFF的128MB用作非线性映射，前者仍然对应于物理地址的0x00000000-0x37FFFFFF(只不过部分要优先分配给DMA)；后者就是所谓的high memory。当然，high memory也有自己的缺点，就是效率比较低(既然是动态的，就绕不开重映射、pte操作等等)。实际上high memory还被划分为了3个区域，一部分用于vmalloc分配虚拟地址上连续的内存，一部分用于较长期的动态映射(persistent kernel mappings)，还有一部分用于编译时可以直接分配物理地址的高端固定映射(fixmaps)。
参考: https://www.zhihu.com/question/280526042

Q: 64位内核虚拟地址空间足够使用，为什么不全部使用线性映射呢？
像vmalloc这些有可能物理上不连续的需求依然是存在的，使用线性映射是不合适的。

Q: 32位下如果物理内存比896M还小呢？
[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/x86/mm/init_32.c
----
#ifdef CONFIG_HIGHMEM
	highstart_pfn = highend_pfn = max_pfn;
	if (max_pfn > max_low_pfn)
		highstart_pfn = max_low_pfn;
	printk(KERN_NOTICE "%ldMB HIGHMEM available.\n",
		pages_to_mb(highend_pfn - highstart_pfn));
	high_memory = (void *) __va(highstart_pfn * PAGE_SIZE - 1) + 1;
#else
	high_memory = (void *) __va(max_low_pfn * PAGE_SIZE - 1) + 1;
#endif
----

参考: https://linux-kernel-labs.github.io/refs/heads/master/lectures/address-space.html#linear-mappings

=== v0.12
v0.12默认最多支持64M逻辑地址空间

=== 参考
Andrea Arcangeli: 20 YEARS OF LINUX VIRTUAL MEMORY
https://lwn.net/Articles/253361/
https://people.freebsd.org/~lstewart/articles/cpumemory.pdf