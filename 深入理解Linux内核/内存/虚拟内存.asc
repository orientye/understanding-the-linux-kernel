:toc:
:toclevels: 5
:hardbreaks-option:

== 虚拟内存

=== 作用
- Caching

    It uses mainmemory efficiently by treating it as a cache for an address space stored on disk, 
    keeping only the active areas in main memory and transferring data back and forth between disk and memory as needed. 

- Memory Management

    It simplifies memory management by providing each process with a uniform address space.
    简化: 链接, 加载, 共享, 内存分配

- Memory Protection

    It protects the address space of each process from corruption by other processes.
    如果一条指令违反权限，会触发异常，发送一个SIGSEGV signal，linux称之为segmentation fault

=== 空间划分
==== 概念
虚拟空间划分为用户空间与内核空间:
1. 不同的用户进程，虚拟空间不同
2. 内核空间则是一样的，所有进程共享一个内核空间
3. 以TASK_SIZE大小划分用户空间与内核空间

==== 32位虚拟空间划分
32位下的用户空间
32位下: 0xC0000000 即3G
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/page_32_types.h

32位下的内核空间
1. 3G - 3G+896M
直接进行映射的896MB物理内存其实又分为两个区域, 在低于16MB的区域, ISA设备可以做DMA, 所以该区域为DMA区域(内核为了保证ISA驱动在申请DMA缓冲区的时候, 通过GFP_DMA标记可以确保申请到16MB以内的内存, 所以必须把这个区域列为一个单独的区域管理); 16MB~896MB之间的为常规区域。 
2. 3G+896M - 4G(即high memory)
这一部分又可以划分为三部分:
VMALLOC_START ~ VMALLOC_END
KMAP_BASE ~ FIXADDR_START
FIXADDR_START ~ 0xFFFFFFFF

参考: https://www.kernel.org/doc/html/latest/mm/highmem.html

==== 64位虚拟空间划分
64位下用户空间: 1<<47, 即128T(4级页表下)
[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/page_64_types.h
----
#ifdef CONFIG_X86_5LEVEL
#define __VIRTUAL_MASK_SHIFT	(pgtable_l5_enabled() ? 56 : 47)
/* See task_size_max() in <asm/page_64.h> */
#else
#define __VIRTUAL_MASK_SHIFT	47
#define task_size_max()		((_AC(1,UL) << __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)
#endif
----

随机性: CONFIG_DYNAMIC_MEMORY_LAYOUT
Note that if CONFIG_RANDOMIZE_MEMORY is enabled, the direct mapping of all physical memory, vmalloc/ioremap space and virtual memory map are randomized. Their order is preserved but their base will be offset early at boot time.

参考: https://github.com/torvalds/linux/blob/master/Documentation/x86/x86_64/mm.rst

==== 参考
https://linux-kernel-labs.github.io/refs/heads/master/lectures/address-space.html
https://www.arm.linux.org.uk/developer/memory.txt

=== 用户空间

==== 数据结构
每个用户进程拥有一个struct mm_struct
一个用户进程下的多线程，共享进程的虚拟空间

==== 布局
布局:
当前运行代码的二进制代码
程序使用的动态库代码
存储全局变量和动态产生的数据的堆
保存局部变量和实现函数/过程调用的栈
环境变量和命令行参数
将文件内容映射到虚拟地址空间中的内存映射

布局的建立:
load_elf_binary()(execve系统调用使用了此函数): https://elixir.bootlin.com/linux/latest/source/fs/binfmt_elf.c

Q: 环境变量和命令行参数存在哪里？
https://unix.stackexchange.com/questions/75939/where-is-the-environment-string-actual-stored

==== 多线程
对于用户进程，主线程堆栈也称进程栈；非主线程堆栈也叫线程栈。
对于同一个用户进程下的多线程，共享进程的虚拟空间，主线程的堆栈就是进程的堆栈。
非主线程的堆栈又是如何分配的呢？
一般是从heap的顶部附近向下分配8M大小，多个(非主)线程之间会有少量间隔填充。

==== 空间切换[[空间切换]]
void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next, struct task_struct *tsk): 切换地址空间

===== x86
[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/x86/mm/tlb.c
----
void switch_mm(struct mm_struct *prev, struct mm_struct *next,
	       struct task_struct *tsk)
{
	unsigned long flags;

	local_irq_save(flags);
	switch_mm_irqs_off(prev, next, tsk);
	local_irq_restore(flags);
}

void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,
			struct task_struct *tsk)
{
	struct mm_struct *real_prev = this_cpu_read(cpu_tlbstate.loaded_mm);
	u16 prev_asid = this_cpu_read(cpu_tlbstate.loaded_mm_asid);
	bool was_lazy = this_cpu_read(cpu_tlbstate_shared.is_lazy);
	unsigned cpu = smp_processor_id();
	u64 next_tlb_gen;
	bool need_flush;
	u16 new_asid;

	/*
	 * NB: The scheduler will call us with prev == next when switching
	 * from lazy TLB mode to normal mode if active_mm isn't changing.
	 * When this happens, we don't assume that CR3 (and hence
	 * cpu_tlbstate.loaded_mm) matches next.
	 *
	 * NB: leave_mm() calls us with prev == NULL and tsk == NULL.
	 */

	/* We don't want flush_tlb_func() to run concurrently with us. */
	if (IS_ENABLED(CONFIG_PROVE_LOCKING))
		WARN_ON_ONCE(!irqs_disabled());

	/*
	 * Verify that CR3 is what we think it is.  This will catch
	 * hypothetical buggy code that directly switches to swapper_pg_dir
	 * without going through leave_mm() / switch_mm_irqs_off() or that
	 * does something like write_cr3(read_cr3_pa()).
	 *
	 * Only do this check if CONFIG_DEBUG_VM=y because __read_cr3()
	 * isn't free.
	 */
#ifdef CONFIG_DEBUG_VM
	if (WARN_ON_ONCE(__read_cr3() != build_cr3(real_prev->pgd, prev_asid))) {
		/*
		 * If we were to BUG here, we'd be very likely to kill
		 * the system so hard that we don't see the call trace.
		 * Try to recover instead by ignoring the error and doing
		 * a global flush to minimize the chance of corruption.
		 *
		 * (This is far from being a fully correct recovery.
		 *  Architecturally, the CPU could prefetch something
		 *  back into an incorrect ASID slot and leave it there
		 *  to cause trouble down the road.  It's better than
		 *  nothing, though.)
		 */
		__flush_tlb_all();
	}
#endif
	if (was_lazy)
		this_cpu_write(cpu_tlbstate_shared.is_lazy, false);

	/*
	 * The membarrier system call requires a full memory barrier and
	 * core serialization before returning to user-space, after
	 * storing to rq->curr, when changing mm.  This is because
	 * membarrier() sends IPIs to all CPUs that are in the target mm
	 * to make them issue memory barriers.  However, if another CPU
	 * switches to/from the target mm concurrently with
	 * membarrier(), it can cause that CPU not to receive an IPI
	 * when it really should issue a memory barrier.  Writing to CR3
	 * provides that full memory barrier and core serializing
	 * instruction.
	 */
	if (real_prev == next) {
		VM_WARN_ON(this_cpu_read(cpu_tlbstate.ctxs[prev_asid].ctx_id) !=
			   next->context.ctx_id);

		/*
		 * Even in lazy TLB mode, the CPU should stay set in the
		 * mm_cpumask. The TLB shootdown code can figure out from
		 * cpu_tlbstate_shared.is_lazy whether or not to send an IPI.
		 */
		if (WARN_ON_ONCE(real_prev != &init_mm &&
				 !cpumask_test_cpu(cpu, mm_cpumask(next))))
			cpumask_set_cpu(cpu, mm_cpumask(next));

		/*
		 * If the CPU is not in lazy TLB mode, we are just switching
		 * from one thread in a process to another thread in the same
		 * process. No TLB flush required.
		 */
		if (!was_lazy)
			return;

		/*
		 * Read the tlb_gen to check whether a flush is needed.
		 * If the TLB is up to date, just use it.
		 * The barrier synchronizes with the tlb_gen increment in
		 * the TLB shootdown code.
		 */
		smp_mb();
		next_tlb_gen = atomic64_read(&next->context.tlb_gen);
		if (this_cpu_read(cpu_tlbstate.ctxs[prev_asid].tlb_gen) ==
				next_tlb_gen)
			return;

		/*
		 * TLB contents went out of date while we were in lazy
		 * mode. Fall through to the TLB switching code below.
		 */
		new_asid = prev_asid;
		need_flush = true;
	} else {
		/*
		 * Apply process to process speculation vulnerability
		 * mitigations if applicable.
		 */
		cond_mitigation(tsk);

		/*
		 * Stop remote flushes for the previous mm.
		 * Skip kernel threads; we never send init_mm TLB flushing IPIs,
		 * but the bitmap manipulation can cause cache line contention.
		 */
		if (real_prev != &init_mm) {
			VM_WARN_ON_ONCE(!cpumask_test_cpu(cpu,
						mm_cpumask(real_prev)));
			cpumask_clear_cpu(cpu, mm_cpumask(real_prev));
		}

		/*
		 * Start remote flushes and then read tlb_gen.
		 */
		if (next != &init_mm)
			cpumask_set_cpu(cpu, mm_cpumask(next));
		next_tlb_gen = atomic64_read(&next->context.tlb_gen);

		choose_new_asid(next, next_tlb_gen, &new_asid, &need_flush);

		/* Let nmi_uaccess_okay() know that we're changing CR3. */
		this_cpu_write(cpu_tlbstate.loaded_mm, LOADED_MM_SWITCHING);
		barrier();
	}

	if (need_flush) {
		this_cpu_write(cpu_tlbstate.ctxs[new_asid].ctx_id, next->context.ctx_id);
		this_cpu_write(cpu_tlbstate.ctxs[new_asid].tlb_gen, next_tlb_gen);
		load_new_mm_cr3(next->pgd, new_asid, true);

		trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
	} else {
		/* The new ASID is already up to date. */
		load_new_mm_cr3(next->pgd, new_asid, false);

		trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, 0);
	}

	/* Make sure we write CR3 before loaded_mm. */
	barrier();

	this_cpu_write(cpu_tlbstate.loaded_mm, next);
	this_cpu_write(cpu_tlbstate.loaded_mm_asid, new_asid);

	if (next != real_prev) {
		cr4_update_pce_mm(next);
		switch_ldt(real_prev, next);
	}
}
----

▪ CR3 register
CR3 register contains the physical address(物理地址) of the base address of the page directory table. This value is unique for each running process, since every process has it’s own page table.

cr3里面存放当前进程的顶级pgd的物理内存地址，用户进程在运行的过程中访问虚拟内存中的数据，会被cr3里面指向的页表转换为物理地址，之后在物理内存中访问数据，这个过程在用户态运行的，这个地址转换的过程无需进入内核态，当然，如果物理内存中不存在，就会走缺页异常的流程。

load_new_mm_cr3()会加载新(next)的pgd的物理内存地址到cr3寄存器。

===== ARM64
对于arm64架构来说，有两个页表基址寄存器ttbr0_el1和ttbr1_el1, ttbr0_el1用来存放用户地址空间的页表基地址，在每次调度上下文切换的时候从tsk->mm->pgd加载，ttbr1_el1是内核地址空间的页表基地址，内核初始化完成之后存放swapper_pg_dir的地址。
内核线程共享内核地址空间，也只能访问内核地址空间，使用swapper_pg_dir去查询页表就可以，而对于arm64来说swapper_pg_dir在内核初始化的时候被加载到ttbr1_le1中，一旦内核线程访问内核虚拟地址，则mmu就会从ttbr1_le1指向的页表基地址开始查询各级页表，进行正常的虚实地址转换。当然，上面是arm64这种架构的处理，它有两个页表基地址寄存器，其他很多处理器如x86, riscv处理器架构都只有一个页表基址寄存器，如x86的cr3，那么这个时候怎么办呢？答案是：使用内核线程借用的prev->active_mm来做，实际上前一个用户任务（记住：不一定是上一个，有可能上上个任务才是用户任务）的active_mm=mm,当切换到前一个用户任务的时候就会将tsk->mm->pgd放到cr3, 对于x86这样的只有一个页表基址寄存器的处理器架构来说，tsk->mm->pgd存放的是整个虚拟地址空间的页表基地址，在fork的时候会将主内核页表的pgd表项拷贝到tsk->mm->pgd对于表项中（可以看看fork的copy_mm相关代码，对于arm64这样的架构没有做内核页表同步）。

[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/mmu_context.h
----
/* Architectures that care about IRQ state in switch_mm can override this. */
#ifndef switch_mm_irqs_off
# define switch_mm_irqs_off switch_mm
#endif
----

[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/arm64/include/asm/mmu_context.h
----
static inline void __switch_mm(struct mm_struct *next)
{
	/*
	 * init_mm.pgd does not contain any user mappings and it is always
	 * active for kernel addresses in TTBR1. Just set the reserved TTBR0.
	 */
	if (next == &init_mm) {
		cpu_set_reserved_ttbr0();
		return;
	}

	check_and_switch_context(next);
}

static inline void
switch_mm(struct mm_struct *prev, struct mm_struct *next,
	  struct task_struct *tsk)
{
	if (prev != next)
		__switch_mm(next);

	/*
	 * Update the saved TTBR0_EL1 of the scheduled-in task as the previous
	 * value may have not been initialised yet (activate_mm caller) or the
	 * ASID has changed since the last run (following the context switch
	 * of another thread of the same process).
	 */
	update_saved_ttbr0(tsk, next);
}
----

[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/arm64/mm/context.c
----
void check_and_switch_context(struct mm_struct *mm)
{
	unsigned long flags;
	unsigned int cpu;
	u64 asid, old_active_asid;

	if (system_supports_cnp())
		cpu_set_reserved_ttbr0();

	asid = atomic64_read(&mm->context.id);

	/*
	 * The memory ordering here is subtle.
	 * If our active_asids is non-zero and the ASID matches the current
	 * generation, then we update the active_asids entry with a relaxed
	 * cmpxchg. Racing with a concurrent rollover means that either:
	 *
	 * - We get a zero back from the cmpxchg and end up waiting on the
	 *   lock. Taking the lock synchronises with the rollover and so
	 *   we are forced to see the updated generation.
	 *
	 * - We get a valid ASID back from the cmpxchg, which means the
	 *   relaxed xchg in flush_context will treat us as reserved
	 *   because atomic RmWs are totally ordered for a given location.
	 */
	old_active_asid = atomic64_read(this_cpu_ptr(&active_asids));
	if (old_active_asid && asid_gen_match(asid) &&
	    atomic64_cmpxchg_relaxed(this_cpu_ptr(&active_asids),
				     old_active_asid, asid))
		goto switch_mm_fastpath;

	raw_spin_lock_irqsave(&cpu_asid_lock, flags);
	/* Check that our ASID belongs to the current generation. */
	asid = atomic64_read(&mm->context.id);
	if (!asid_gen_match(asid)) {
		asid = new_context(mm);
		atomic64_set(&mm->context.id, asid);
	}

	cpu = smp_processor_id();
	if (cpumask_test_and_clear_cpu(cpu, &tlb_flush_pending))
		local_flush_tlb_all();

	atomic64_set(this_cpu_ptr(&active_asids), asid);
	raw_spin_unlock_irqrestore(&cpu_asid_lock, flags);

switch_mm_fastpath:

	arm64_apply_bp_hardening();

	/*
	 * Defer TTBR0_EL1 setting for user threads to uaccess_enable() when
	 * emulating PAN.
	 */
	if (!system_uses_ttbr0_pan())
		cpu_switch_mm(mm->pgd, mm);
}
----

参考: https://zhuanlan.zhihu.com/p/373959024
参考: https://mp.weixin.qq.com/s/pmWuGS6thCj6GNwwjh0bRw

=== 内核空间

==== x86-64内核空间布局
https://github.com/torvalds/linux/blob/master/Documentation/x86/x86_64/mm.rst

========================================================================================================================
    Start addr    |   Offset   |     End addr     |  Size   | VM area description
========================================================================================================================
                  |            |                  |         |
 0000000000000000 |    0       | 00007fffffffffff |  128 TB | user-space virtual memory, different per mm
__________________|____________|__________________|_________|___________________________________________________________
                  |            |                  |         |
 0000800000000000 | +128    TB | ffff7fffffffffff | ~16M TB | ... huge, almost 64 bits wide hole of non-canonical
                  |            |                  |         |     virtual memory addresses up to the -128 TB
                  |            |                  |         |     starting offset of kernel mappings.
__________________|____________|__________________|_________|___________________________________________________________
                                                            |
                                                            | Kernel-space virtual memory, shared between all processes:
____________________________________________________________|___________________________________________________________
                  |            |                  |         |
 ffff800000000000 | -128    TB | ffff87ffffffffff |    8 TB | ... guard hole, also reserved for hypervisor
 ffff880000000000 | -120    TB | ffff887fffffffff |  0.5 TB | LDT remap for PTI
 ffff888000000000 | -119.5  TB | ffffc87fffffffff |   64 TB | direct mapping of all physical memory (page_offset_base)
 ffffc88000000000 |  -55.5  TB | ffffc8ffffffffff |  0.5 TB | ... unused hole
 ffffc90000000000 |  -55    TB | ffffe8ffffffffff |   32 TB | vmalloc/ioremap space (vmalloc_base)
 ffffe90000000000 |  -23    TB | ffffe9ffffffffff |    1 TB | ... unused hole
 ffffea0000000000 |  -22    TB | ffffeaffffffffff |    1 TB | virtual memory map (vmemmap_base)
 ffffeb0000000000 |  -21    TB | ffffebffffffffff |    1 TB | ... unused hole
 ffffec0000000000 |  -20    TB | fffffbffffffffff |   16 TB | KASAN shadow memory
__________________|____________|__________________|_________|____________________________________________________________
                                                            |
                                                            | Identical layout to the 56-bit one from here on:
____________________________________________________________|____________________________________________________________
                  |            |                  |         |
 fffffc0000000000 |   -4    TB | fffffdffffffffff |    2 TB | ... unused hole
                  |            |                  |         | vaddr_end for KASLR
 fffffe0000000000 |   -2    TB | fffffe7fffffffff |  0.5 TB | cpu_entry_area mapping
 fffffe8000000000 |   -1.5  TB | fffffeffffffffff |  0.5 TB | ... unused hole
 ffffff0000000000 |   -1    TB | ffffff7fffffffff |  0.5 TB | %esp fixup stacks
 ffffff8000000000 | -512    GB | ffffffeeffffffff |  444 GB | ... unused hole
 ffffffef00000000 |  -68    GB | fffffffeffffffff |   64 GB | EFI region mapping space
 ffffffff00000000 |   -4    GB | ffffffff7fffffff |    2 GB | ... unused hole
 ffffffff80000000 |   -2    GB | ffffffff9fffffff |  512 MB | kernel text mapping, mapped to physical address 0
 ffffffff80000000 |-2048    MB |                  |         |
 ffffffffa0000000 |-1536    MB | fffffffffeffffff | 1520 MB | module mapping space
 ffffffffff000000 |  -16    MB |                  |         |
    FIXADDR_START | ~-11    MB | ffffffffff5fffff | ~0.5 MB | kernel-internal fixmap range, variable size and offset
 ffffffffff600000 |  -10    MB | ffffffffff600fff |    4 kB | legacy vsyscall ABI
 ffffffffffe00000 |   -2    MB | ffffffffffffffff |    2 MB | ... unused hole
__________________|____________|__________________|_________|___________________________________________________________

内核空间虚拟地址从低到高依次为:

▪ hypervisor: 8 TB
虚拟机相关?

▪ LDT remap for PTI: 0.5 TB
LDT remap for PTI , PTI(page table isolation), 在kernel mode切换到user space时，user space的page table需要保留一些能够进入kernel space的入口地址，比如syscall, IDT的内存mapping, 内核会复制一份缩减版本的内核page table返回给user space, 这样一定程度保证安全。该区域是user space能看到的映射到LDT的区域。

▪ direct mapping of all physical memory: 直接映射，64T，起始地址page_offset_base
▪ vmalloc/ioremap space: 32T，起始地址vmalloc_base
▪ virtual memory map: 1T，起始地址vmemmap_base

▪ KASAN shadow memory: 16TB
KASAN: The Kernel Address Sanitizer
KASAN用来检测use-after-free与out-of-bounds问题。
KASAN使用影子内存(shadow memory)记录内存的每个字节是否可以安全访问，使用编译时插桩在每次访问内存时检查影子内存。
KASAN使用内核地址空间的1/8作为影子内存，影子内存的每个字节记录内存连续8字节的状态。
https://github.com/torvalds/linux/blob/master/Documentation/dev-tools/kasan.rst
https://en.wikipedia.org/wiki/AddressSanitizer#KernelAddressSanitizer

▪ KASLR:
Kernel address space layout randomization
https://elixir.bootlin.com/linux/latest/source/arch/x86/mm/kaslr.c
https://lwn.net/Articles/569635/
https://en.wikipedia.org/wiki/Address_space_layout_randomization

▪ cpu_entry_area: 0.5 TB, GDT/TSS(task state segment)/syscall等
▪ esp fixup stacks: 0.5 TB, 防止16-bit userspace stack出现error
▪ EFI region mapping space: 64 GB, EFI runtime 使用的单独的PGD
▪ kernel text mapping: 512 MB, kernel代码映射区间，映射到物理地址0
▪ module mapping space: 1520 MB, 动态模块的mapping区间
▪ kernel-internal fixmap range: ~0.5 MB kernel内部固定映射区间
▪ legacy vsyscall ABI: 4 kB, 废弃的vsyscall的映射区间，用于系统调用加速

=== v0.12
v0.12默认最多支持64M逻辑地址空间

=== 参考
Andrea Arcangeli: 20 YEARS OF LINUX VIRTUAL MEMORY
https://lwn.net/Articles/253361/
https://people.freebsd.org/~lstewart/articles/cpumemory.pdf