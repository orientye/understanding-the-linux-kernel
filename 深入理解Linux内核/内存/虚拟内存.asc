:toc:
:toclevels: 5
:hardbreaks-option:

== 虚拟内存

=== 作用
- Caching

    It uses mainmemory efficiently by treating it as a cache for an address space stored on disk, 
    keeping only the active areas in main memory and transferring data back and forth between disk and memory as needed. 

- Memory Management

    It simplifies memory management by providing each process with a uniform address space.
    简化: 链接, 加载, 共享, 内存分配

- Memory Protection

    It protects the address space of each process from corruption by other processes.
	如果一条指令违反权限，会触发异常，发送一个SIGSEGV signal，linux称之为segmentation fault

=== 空间划分
虚拟空间划分为用户空间与内核空间。

用户虚拟空间大小: TASK_SIZE

64位下: 约1<<47,即128T(4级页表下)
[source,c]
.https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/page_64_types.h
----
#ifdef CONFIG_X86_5LEVEL
#define __VIRTUAL_MASK_SHIFT	(pgtable_l5_enabled() ? 56 : 47)
/* See task_size_max() in <asm/page_64.h> */
#else
#define __VIRTUAL_MASK_SHIFT	47
#define task_size_max()		((_AC(1,UL) << __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)
#endif
----

- 32位空间划分

32位下的用户空间
32位下: 0xC0000000 即3G
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/page_32_types.h

32位下的内核空间
1. 3G - 3G+896M
直接进行映射的896MB物理内存其实又分为两个区域, 在低于16MB的区域, ISA设备可以做DMA, 所以该区域为DMA区域(内核为了保证ISA驱动在申请DMA缓冲区的时候, 通过GFP_DMA标记可以确保申请到16MB以内的内存, 所以必须把这个区域列为一个单独的区域管理); 16MB~896MB之间的为常规区域。 
2. 3G+896M - 4G(即high memory)
这一部分又可以划分为三部分: vmalloc区，固定映射区，临时映射区。

https://www.kernel.org/doc/html/latest/mm/highmem.html
https://linux-kernel-labs.github.io/refs/heads/master/lectures/address-space.html
ARM: https://www.arm.linux.org.uk/developer/memory.txt

- 64位空间划分
https://www.kernel.org/doc/Documentation/x86/x86_64/mm.txt

- 多进程多线程

=== Address Translation
- 历史与现状

Early PCs used physical addressing, and systems such as digital signal processors, embedded microcontrollers, and Cray supercomputers continue to do so. However, modern processors use a form of addressing known as virtual addressing.

- 用户空间

    虚拟地址: |PGD|PUD|PMD|PTE|OFFSET|
    48bit: |9bit|9bit|9bit|9bit|12bit|
    https://www.kernel.org/doc/html/latest/x86/x86_64/mm.html
    https://en.wikipedia.org/wiki/Intel_5-level_paging

▪ Q&A
https://www.quora.com/Is-there-a-way-to-see-a-process-physical-memory-address-in-Linux

- 内核空间

可以按照连续的虚拟空间，物理内存是否连续分成两个部分。一部分其物理内存也是连续的，另一部分则物理内存不连续。在这里，我们称前者为固定映射(fix-mapping)，后者称为动态映射(dynamic-mapping)。

显然fix-mapping不需要按照多级页表path walk去查找，地址转换效率高。

Q: fix-mapping这么好，那为什么不全部使用fix-mapping呢?
在32位处理器下，按照经典用户态与内核3:1的划分比例，内核能够使用的虚拟地址只有1G，按照固定offset的映射方式，这意味着内核能够使用的物理地址大小也只有1G。但随着内核越来越复杂，内存技术的发展使得高于4G的内存变得十分常见，受限于32位系统与这种fix-mapping，内核可用的物理内存大小限制在1G。
这也正是high memory出现的原因。"high memory"要解决的是32位下虚拟地址空间不足带来的问题(显然64位系统这个问题就不存在了)。实际上在很早以前这个问题就在lwn上讨论过了 ，在当时已经有一些临时的方法去规避这个问题，比如重新划分用户/内核的地址空间比例，变为2.5:1.5等等，但在特定场景下(比如用户态使用的内存非常非常多)会使得用户态运行效率降低，因此也不是一个很好的办法。怎么解决呢可以把这1G，划分成两部分，一部分用来fix-mapping，一部分用来dynamic-mapping。以x86为例，实际中的做法是，0xC0000000-0xF7FFFFFF的896MB用作fix-mapping，0xF8000000-0xFFFFFFFF的128MB用作dynamic-mapping，前者仍然对应于物理地址的0x00000000-0x37FFFFFF(只不过部分要优先分配给DMA)；后者就是所谓的high memory。当然，high memory也有自己的缺点，就是效率比较低(既然是动态的，就绕不开重映射、pte操作等等)。实际上high memory还被划分为了3个区域，一部分用于vmalloc分配虚拟地址上连续的内存，一部分用于较长期的动态映射(persistent kernel mappings)，还有一部分用于编译时可以直接分配物理地址的高端固定映射(fixmaps)。
参考: https://www.zhihu.com/question/280526042

Q: 64位内核虚拟地址空间足够使用，为什么不全部使用fix-mapping呢？
像vmalloc这些有可能物理上不连续的需求依然是存在的，用fix-mapping显然不合适。

映射: __pa(x)与__va(x)
[source,c]
.https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/page.h
----
#ifndef __pa
#define __pa(x)		__phys_addr((unsigned long)(x))
#endif

#ifndef __va
#define __va(x)			((void *)((unsigned long)(x)+PAGE_OFFSET))
#endif
----

[source,c]
.https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/page_64.h
----
static __always_inline unsigned long __phys_addr_nodebug(unsigned long x)
{
	unsigned long y = x - __START_KERNEL_map;

	/* use the carry flag to determine if x was < __START_KERNEL_map */
	x = y + ((x > y) ? phys_base : (__START_KERNEL_map - PAGE_OFFSET));

	return x;
}

#ifdef CONFIG_DEBUG_VIRTUAL
extern unsigned long __phys_addr(unsigned long);
extern unsigned long __phys_addr_symbol(unsigned long);
#else
#define __phys_addr(x)		__phys_addr_nodebug(x)
#define __phys_addr_symbol(x) \
	((unsigned long)(x) - __START_KERNEL_map + phys_base)
#endif
----

[source,c]
.https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/page_64_types.h
----
#define __START_KERNEL_map	_AC(0xffffffff80000000, UL)
----

https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/segment.h

    #define __KERNEL_CS			(GDT_ENTRY_KERNEL_CS*8)
    #define __KERNEL_DS			(GDT_ENTRY_KERNEL_DS*8)
    #define __USER_DS			(GDT_ENTRY_DEFAULT_USER_DS*8 + 3)
    #define __USER_CS			(GDT_ENTRY_DEFAULT_USER_CS*8 + 3)

=== 用户空间
struct mm_struct: https://elixir.bootlin.com/linux/latest/source/include/linux/mm_types.h
系统中每个进程拥有一个mm_struct

当前运行代码的二进制代码，即text段
程序使用的动态库代码
存储全局变量和动态产生的数据的堆
保存局部变量和实现函数/过程调用的栈
环境变量和命令行参数的段
将文件内容映射到虚拟地址空间中的内存映射

Q: 环境变量和命令行参数存在哪里？
https://unix.stackexchange.com/questions/75939/where-is-the-environment-string-actual-stored

布局的建立:
load_elf_binary()(execve系统调用使用了此函数): https://elixir.bootlin.com/linux/latest/source/fs/binfmt_elf.c

==== 空间切换[[空间切换]]
[source,c]
.https://elixir.bootlin.com/linux/latest/source/arch/x86/mm/tlb.c
----
void switch_mm(struct mm_struct *prev, struct mm_struct *next,
	       struct task_struct *tsk)
{
	unsigned long flags;

	local_irq_save(flags);
	switch_mm_irqs_off(prev, next, tsk);//切换地址空间
	local_irq_restore(flags);
}

void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,
			struct task_struct *tsk)
{
	struct mm_struct *real_prev = this_cpu_read(cpu_tlbstate.loaded_mm);
	u16 prev_asid = this_cpu_read(cpu_tlbstate.loaded_mm_asid);
	bool was_lazy = this_cpu_read(cpu_tlbstate_shared.is_lazy);
	unsigned cpu = smp_processor_id();
	u64 next_tlb_gen;
	bool need_flush;
	u16 new_asid;

	/*
	 * NB: The scheduler will call us with prev == next when switching
	 * from lazy TLB mode to normal mode if active_mm isn't changing.
	 * When this happens, we don't assume that CR3 (and hence
	 * cpu_tlbstate.loaded_mm) matches next.
	 *
	 * NB: leave_mm() calls us with prev == NULL and tsk == NULL.
	 */

	/* We don't want flush_tlb_func() to run concurrently with us. */
	if (IS_ENABLED(CONFIG_PROVE_LOCKING))
		WARN_ON_ONCE(!irqs_disabled());

	/*
	 * Verify that CR3 is what we think it is.  This will catch
	 * hypothetical buggy code that directly switches to swapper_pg_dir
	 * without going through leave_mm() / switch_mm_irqs_off() or that
	 * does something like write_cr3(read_cr3_pa()).
	 *
	 * Only do this check if CONFIG_DEBUG_VM=y because __read_cr3()
	 * isn't free.
	 */
#ifdef CONFIG_DEBUG_VM
	if (WARN_ON_ONCE(__read_cr3() != build_cr3(real_prev->pgd, prev_asid))) {
		/*
		 * If we were to BUG here, we'd be very likely to kill
		 * the system so hard that we don't see the call trace.
		 * Try to recover instead by ignoring the error and doing
		 * a global flush to minimize the chance of corruption.
		 *
		 * (This is far from being a fully correct recovery.
		 *  Architecturally, the CPU could prefetch something
		 *  back into an incorrect ASID slot and leave it there
		 *  to cause trouble down the road.  It's better than
		 *  nothing, though.)
		 */
		__flush_tlb_all();
	}
#endif
	if (was_lazy)
		this_cpu_write(cpu_tlbstate_shared.is_lazy, false);

	/*
	 * The membarrier system call requires a full memory barrier and
	 * core serialization before returning to user-space, after
	 * storing to rq->curr, when changing mm.  This is because
	 * membarrier() sends IPIs to all CPUs that are in the target mm
	 * to make them issue memory barriers.  However, if another CPU
	 * switches to/from the target mm concurrently with
	 * membarrier(), it can cause that CPU not to receive an IPI
	 * when it really should issue a memory barrier.  Writing to CR3
	 * provides that full memory barrier and core serializing
	 * instruction.
	 */
	if (real_prev == next) {
		VM_WARN_ON(this_cpu_read(cpu_tlbstate.ctxs[prev_asid].ctx_id) !=
			   next->context.ctx_id);

		/*
		 * Even in lazy TLB mode, the CPU should stay set in the
		 * mm_cpumask. The TLB shootdown code can figure out from
		 * cpu_tlbstate_shared.is_lazy whether or not to send an IPI.
		 */
		if (WARN_ON_ONCE(real_prev != &init_mm &&
				 !cpumask_test_cpu(cpu, mm_cpumask(next))))
			cpumask_set_cpu(cpu, mm_cpumask(next));

		/*
		 * If the CPU is not in lazy TLB mode, we are just switching
		 * from one thread in a process to another thread in the same
		 * process. No TLB flush required.
		 */
		if (!was_lazy)
			return;

		/*
		 * Read the tlb_gen to check whether a flush is needed.
		 * If the TLB is up to date, just use it.
		 * The barrier synchronizes with the tlb_gen increment in
		 * the TLB shootdown code.
		 */
		smp_mb();
		next_tlb_gen = atomic64_read(&next->context.tlb_gen);
		if (this_cpu_read(cpu_tlbstate.ctxs[prev_asid].tlb_gen) ==
				next_tlb_gen)
			return;

		/*
		 * TLB contents went out of date while we were in lazy
		 * mode. Fall through to the TLB switching code below.
		 */
		new_asid = prev_asid;
		need_flush = true;
	} else {
		/*
		 * Apply process to process speculation vulnerability
		 * mitigations if applicable.
		 */
		cond_mitigation(tsk);

		/*
		 * Stop remote flushes for the previous mm.
		 * Skip kernel threads; we never send init_mm TLB flushing IPIs,
		 * but the bitmap manipulation can cause cache line contention.
		 */
		if (real_prev != &init_mm) {
			VM_WARN_ON_ONCE(!cpumask_test_cpu(cpu,
						mm_cpumask(real_prev)));
			cpumask_clear_cpu(cpu, mm_cpumask(real_prev));
		}

		/*
		 * Start remote flushes and then read tlb_gen.
		 */
		if (next != &init_mm)
			cpumask_set_cpu(cpu, mm_cpumask(next));
		next_tlb_gen = atomic64_read(&next->context.tlb_gen);

		choose_new_asid(next, next_tlb_gen, &new_asid, &need_flush);

		/* Let nmi_uaccess_okay() know that we're changing CR3. */
		this_cpu_write(cpu_tlbstate.loaded_mm, LOADED_MM_SWITCHING);
		barrier();
	}

	if (need_flush) {
		this_cpu_write(cpu_tlbstate.ctxs[new_asid].ctx_id, next->context.ctx_id);
		this_cpu_write(cpu_tlbstate.ctxs[new_asid].tlb_gen, next_tlb_gen);
		load_new_mm_cr3(next->pgd, new_asid, true);

		trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
	} else {
		/* The new ASID is already up to date. */
		load_new_mm_cr3(next->pgd, new_asid, false);

		trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, 0);
	}

	/* Make sure we write CR3 before loaded_mm. */
	barrier();

	this_cpu_write(cpu_tlbstate.loaded_mm, next);
	this_cpu_write(cpu_tlbstate.loaded_mm_asid, new_asid);

	if (next != real_prev) {
		cr4_update_pce_mm(next);
		switch_ldt(real_prev, next);
	}
}
----

CR3 register:
CR3 register contains the physical address of the base address of the page directory table. This value is unique for each running process, since every process has it’s own page table.

http://www.wowotech.net/process_management/context-switch-arch.html

=== 内核空间
==== 概要
内核空间不属于任何一个特定的进程，因此，单独设置了一个内核专用的mm_struct, 即init_mm。

==== vmalloc/vfree[[vmallocvfree]]
vmalloc提供了内核虚拟地址连续但物理地址不一定连续的区域。
vmalloc从内核的虚存空间分配一块虚存以及相应的物理内存，vmalloc分配的空间不会被kswapd换出，kswapd只扫描各个进程的用户空间，通过vmalloc分配的页面表项对其不可见。

https://elixir.bootlin.com/linux/latest/source/include/linux/vmalloc.h

VMALLOC_START与VMALLOC_END:
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/pgtable_32_areas.h
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/pgtable_64_types.h

参考: https://www.cnblogs.com/LoyenWang/p/11965787.html

=== 内存映射
==== 作用与原理
Linux initializes the contents of a virtual memory area by associating it with an object on disk, a process known as memory mapping. 
Areas can be mapped to one of two types of objects:

1. Regular file in the Linux file system: An area can be mapped to a contiguous section of a regular disk file, such as an executable object file. The file section is divided into page-size pieces, with each piece containing the initial contents of a virtual page. Because of demand paging, none of these virtual pages is actually swapped into physical memory until the CPU first touches the page (i.e., issues a virtual address that falls within that page’s region of the address space). If the area is larger than the file section, then the area is padded with zeros. 适合于很大的文件

2. Anonymous file: An area can also be mapped to an anonymous file, created by the kernel, that contains all binary zeros. The first time the CPU touches a virtual page in such an area, the kernel finds an appropriate victim page in physical memory, swaps out the victim page if it is dirty, overwrites the victim page with binary zeros, and updates the page table to mark the page as resident. Notice that no data are actually transferred between disk and memory. For this reason, pages in areas that are mapped to anonymous files are sometimes called demand-zero pages. 适合于创建进程间通信的共享内存。

In either case, once a virtual page is initialized, it is swapped back and forthbetween a special swap file maintained by the kernel. The swap file is also known as the swap space or the swap area. An important point to realize is that at any point in time, the swap space bounds the total amount of virtual pages that can be allocated by the currently running processes.

内存映射是一种重要的抽象，在内核和应用程序中均大量使用。内存映射将数据映射到进程的虚拟地址空间中。作为映射目标的地址空间区域，对改区域的内存修改都会自动同步到数据源。例如，文件的内容映射到内存中，处理只需要读取相应的内存即可访问文件内容，或向内存写入数据来修改文件的内容，内核保证任何修改都会自动同步到文件中。内核在实现设备驱动程序时直接使用了内存映射，外设的输入/输出可以映射到虚拟地址空间的区域中，对相关内存区域的都写会由系统重定向到设备，因而大大简化了驱动程序的实现。

▪ 问题
内存映射文件需要在进程上占用一块很大的连续地址空间。对于Intel的IA-32的4G逻辑地址空间，可用的连续地址空间远远小于2-3G。
相关联的文件的I/O错误(如可拔出驱动器或光驱被弹出，磁盘满时写操作等)的内存映射文件会向应用程序报告SIGSEGV/SIGBUS信号(POSIX环境)或EXECUTE_IN_PAGE_ERROR结构化异常(Windows环境)。通常的内存操作是无需考虑这些异常的。
有内存管理单元(MMU)才支持内存映射文件。

参考: CSAPP 3rd, 9.8
参考: https://linux-kernel-labs.github.io/refs/heads/master/labs/memory_mapping.html

==== 用户空间映射

==== 内核空间映射[[内核空间映射]]
▪ 持久内核映射
kmap(): https://elixir.bootlin.com/linux/latest/source/include/linux/highmem-internal.h
kmap函数不能用于中断处理程序, 因为它可能进入睡眠状态

▪ 固定内存映射/临时内核映射
kmap_atomic(): https://elixir.bootlin.com/linux/latest/source/include/linux/highmem-internal.h
kmap_atomic函数不能用于可能进入睡眠的代码

▪ 没有高端内存的计算机上的映射函数
许多体系结构不支持高端内存, 例如64位体系结构: 内核提供了兼容宏CONFIG_HIGHMEM

init_memory_mapping()

==== 外部设备存储空间的地址映射
一般来说，对外部设备的访问有两种不同的形式，一种叫做内存映射式(memory mapped), 另一种叫做IO映射式(I/O mapped)。

对于内存映射式，外部设备的存储单元例如控制寄存器，状态寄存器，数据寄存器等，是作为内存的一部分出现在系统中的。CPU可以像访问一个内存单元一样访问外部设备的内存单元，因此不需要专门用于外设I/O的指令。

对于IO映射式，外部设备的存储单元与内存属于两个不同的体系，访问内存的指令不能用于访问外部设备的存储单元，例如X86的IN和OUT指令， 但通常用于IO指令的地址空间相对来说是很小的。可以说，IO映射式只适合与早期的计算机技术，彼时外设通常只有几个寄存器，通过这几个寄存器就能完成对外设的所有操作。随着计算机技术的发展，单纯的IO映射方式显然已经不能满足需求了。

尤其随着显卡以及PCI总线的出现，不管是采用IO映射还是内存映射，都需要将外设上的存储器映射到内存空间。在linux内核中，通过函数ioremap()来建立这个映射。

https://elixir.bootlin.com/linux/latest/source/arch/x86/mm/ioremap.c

    ioremap()
    iounmap()

=== v0.12
v0.12默认最多支持64M逻辑地址空间

=== 参考
https://lwn.net/Articles/253361/
https://people.freebsd.org/~lstewart/articles/cpumemory.pdf