:toc:
:toclevels: 5
:hardbreaks-option:

== 数据的接收

=== 过程概述
• 数据包从外部网络进入网卡
• 网卡(通过DMA)将包拷贝到内核内存中的ring buffer
• 产生硬件中断，通知系统收到了一个包
• 驱动调用NAPI，如果轮询(poll)还没有开始，就开始轮询
• ksoftirqd软中断调用NAPI的poll函数从ring buffer收包(poll函数是网卡驱动在初始化阶段注册的；每个cpu上都运行着一个ksoftirqd进程，在系统启动期间就注册了)
• ring buffer里面对应的内存区域解除映射(unmapped)
• 如果packet steering功能打开，或者网卡有多队列，网卡收到的数据包会被分发到多个cpu
• 数据包从队列进入协议层
• 协议层处理数据包
• 数据包从协议层进入相应socket的接收队列: poll函数将收到的包送到协议栈注册的ip_rcv函数中，ip_rcv函数再将包送到tcp_v4_rcv函数或udp_rcv函数中。

=== 硬中断处理
当数据帧从网线到达网卡上的时候，网卡会在分配给自己的ring buffer中寻找可以使用的内存位置，找到后DMA会把数据DMA到⽹卡之前关联的内存⾥，此时CPU是没有感知的。当DMA操作完成以后，网卡会向CPU发起⼀个硬中断，通知CPU有数据到达。

注意，这个过程中，如果ring buffer满了，新来的数据包将会被丢弃(ifconfig命令查看网卡，可以看到overruns信息项，表示因为环形队列满被丢弃的包，此时可以通过ethtool命令来加大队列的长度)。

对于intel igb网卡，当有数据包到达网卡时，DMA把数据映射到内存，通知CPU硬中断，执行注册的硬中断处理函数igb_msix_ring():

igb_msix_ring(): https://elixir.bootlin.com/linux/latest/source/drivers/net/ethernet/intel/igb/igb_main.c

    igb_msix_ring() - drivers/net/ethernet/intel/igb/igb_main.c
        __napi_schedule() - net/core/dev.c
            ____napi_schedule() - net/core/dev.c
                __raise_softirq_irqoff(NET_RX_SOFTIRQ) - net/core/dev.c

__raise_softirq_irqoff(NET_RX_SOFTIRQ)发起NET_RX_SOFTIRQ软中断。

=== 软中断处理
ksoftirqd为软中断处理进程，ksoftirqd收到NET_RX_SOFTIRQ软中断后，执行软中断处理函数net_rx_action()，调用网卡驱动poll()函数(对于igb网卡为igb_poll函数)收包。

    run_ksoftirqd() - kernel/softirqd.c
        __do_softirq() - kernel/softirqd.c
            h->action(h) - kernel/softirqd.c
                net_rx_action() - net/core/dev.c
                    napi_poll() - net/core/dev.c
                        __napi_poll - net/core/dev.c
                            work = n->poll(n, weight) - net/core/dev.c
                                igb_poll() - drivers/net/ethernet/intel/igb/igb_main.c

=== 链路层
==== igb_clean_rx_irq()
[source, c]
.https://elixir.bootlin.com/linux/latest/source/drivers/net/ethernet/intel/igb/igb_main.c
----
static int igb_clean_rx_irq(struct igb_q_vector *q_vector, const int budget)
{
	struct igb_adapter *adapter = q_vector->adapter;
	struct igb_ring *rx_ring = q_vector->rx.ring;
	struct sk_buff *skb = rx_ring->skb;
	unsigned int total_bytes = 0, total_packets = 0;
	u16 cleaned_count = igb_desc_unused(rx_ring);
	unsigned int xdp_xmit = 0;
	struct xdp_buff xdp;
	u32 frame_sz = 0;
	int rx_buf_pgcnt;

	/* Frame size depend on rx_ring setup when PAGE_SIZE=4K */
#if (PAGE_SIZE < 8192)
	frame_sz = igb_rx_frame_truesize(rx_ring, 0);
#endif
	xdp_init_buff(&xdp, frame_sz, &rx_ring->xdp_rxq);

	while (likely(total_packets < budget)) {
		union e1000_adv_rx_desc *rx_desc;
		struct igb_rx_buffer *rx_buffer;
		ktime_t timestamp = 0;
		int pkt_offset = 0;
		unsigned int size;
		void *pktbuf;

		/* return some buffers to hardware, one at a time is too slow */
		if (cleaned_count >= IGB_RX_BUFFER_WRITE) {
			igb_alloc_rx_buffers(rx_ring, cleaned_count);
			cleaned_count = 0;
		}

		rx_desc = IGB_RX_DESC(rx_ring, rx_ring->next_to_clean);
		size = le16_to_cpu(rx_desc->wb.upper.length);
		if (!size)
			break;

		/* This memory barrier is needed to keep us from reading
		 * any other fields out of the rx_desc until we know the
		 * descriptor has been written back
		 */
		dma_rmb();

		rx_buffer = igb_get_rx_buffer(rx_ring, size, &rx_buf_pgcnt);
		pktbuf = page_address(rx_buffer->page) + rx_buffer->page_offset;

		/* pull rx packet timestamp if available and valid */
		if (igb_test_staterr(rx_desc, E1000_RXDADV_STAT_TSIP)) {
			int ts_hdr_len;

			ts_hdr_len = igb_ptp_rx_pktstamp(rx_ring->q_vector,
							 pktbuf, &timestamp);

			pkt_offset += ts_hdr_len;
			size -= ts_hdr_len;
		}

		/* retrieve a buffer from the ring */
		if (!skb) {
			unsigned char *hard_start = pktbuf - igb_rx_offset(rx_ring);
			unsigned int offset = pkt_offset + igb_rx_offset(rx_ring);

			xdp_prepare_buff(&xdp, hard_start, offset, size, true);
			xdp_buff_clear_frags_flag(&xdp);
#if (PAGE_SIZE > 4096)
			/* At larger PAGE_SIZE, frame_sz depend on len size */
			xdp.frame_sz = igb_rx_frame_truesize(rx_ring, size);
#endif
			skb = igb_run_xdp(adapter, rx_ring, &xdp);
		}

		if (IS_ERR(skb)) {
			unsigned int xdp_res = -PTR_ERR(skb);

			if (xdp_res & (IGB_XDP_TX | IGB_XDP_REDIR)) {
				xdp_xmit |= xdp_res;
				igb_rx_buffer_flip(rx_ring, rx_buffer, size);
			} else {
				rx_buffer->pagecnt_bias++;
			}
			total_packets++;
			total_bytes += size;
		} else if (skb)
			igb_add_rx_frag(rx_ring, rx_buffer, skb, size);
		else if (ring_uses_build_skb(rx_ring))
			skb = igb_build_skb(rx_ring, rx_buffer, &xdp,
					    timestamp);
		else
			skb = igb_construct_skb(rx_ring, rx_buffer,
						&xdp, timestamp);

		/* exit if we failed to retrieve a buffer */
		if (!skb) {
			rx_ring->rx_stats.alloc_failed++;
			rx_buffer->pagecnt_bias++;
			break;
		}

		igb_put_rx_buffer(rx_ring, rx_buffer, rx_buf_pgcnt);
		cleaned_count++;

		/* fetch next buffer in frame if non-eop */
		if (igb_is_non_eop(rx_ring, rx_desc))
			continue;

		/* verify the packet layout is correct */
		if (igb_cleanup_headers(rx_ring, rx_desc, skb)) {
			skb = NULL;
			continue;
		}

		/* probably a little skewed due to removing CRC */
		total_bytes += skb->len;

		/* populate checksum, timestamp, VLAN, and protocol */
		igb_process_skb_fields(rx_ring, rx_desc, skb);

		napi_gro_receive(&q_vector->napi, skb);

		/* reset skb pointer */
		skb = NULL;

		/* update budget accounting */
		total_packets++;
	}

	/* place incomplete frames back on ring for completion */
	rx_ring->skb = skb;

	if (xdp_xmit & IGB_XDP_REDIR)
		xdp_do_flush();

	if (xdp_xmit & IGB_XDP_TX) {
		struct igb_ring *tx_ring = igb_xdp_tx_queue_mapping(adapter);

		igb_xdp_ring_update_tail(tx_ring);
	}

	u64_stats_update_begin(&rx_ring->rx_syncp);
	rx_ring->rx_stats.packets += total_packets;
	rx_ring->rx_stats.bytes += total_bytes;
	u64_stats_update_end(&rx_ring->rx_syncp);
	q_vector->rx.total_packets += total_packets;
	q_vector->rx.total_bytes += total_bytes;

	if (cleaned_count)
		igb_alloc_rx_buffers(rx_ring, cleaned_count);

	return total_packets;
}
----
napi_gro_receive(): 

==== GRO(Generic Receive Offloading)
主要思想:
都是通过合并"足够类似"的包来减少传送给网络栈的包数，减少CPU的使用量。例如，大文件传输的场景，包的数量非常多，大部分包都是一段文件数据。相比于每次都将小包送到网络栈，可以将收到的小包合并成一个很大的包再送到网络栈。GRO使协议层只需处理一个header，就可以将包含大量数据的大包送到用户程序。
GRO给协议栈提供了一次将包交给网络协议栈之前，对其检查校验和、修改协议头和发送应答包(ACK packets)的机会。

处理规则:
如果GRO的buffer相比于包太小了，则可能会什么都不做；
如果当前包属于某个更大包的一个分片，调用enqueue_backlog()将这个分片放到某个CPU的包队列；当包重组完成后，会交给协议栈继续处理；
如果当前包不是分片包，会交给协议栈继续处理。

优缺点:
这类优化方式的缺点是信息丢失：包的option或者flag信息在合并时会丢失。只是GRO对于包合并的规则更严苛。

TIPS:
使用tcpdump抓包时，如果看收到了非常大、看似不太正常的包，这很可能是系统开启了GRO(tcpdump的抓包在GRO处理之后)。

命令:
查看GSO是否开启: ethtool -k ens33 | grep generic-receive-offload

==== napi_gro_receive()
[source, c]
.https://elixir.bootlin.com/linux/latest/source/net/core/gro.c
----
gro_result_t napi_gro_receive(struct napi_struct *napi, struct sk_buff *skb)
{
	//...
	ret = napi_skb_finish(napi, skb, dev_gro_receive(napi, skb));
	//...
}
----
napi_gro_receive()首先调用dev_gro_receive(), 然后通过napi_skb_finish()返回结果。

==== dev_gro_receive()
[source, c]
.https://elixir.bootlin.com/linux/latest/source/net/core/gro.c
----
static enum gro_result dev_gro_receive(struct napi_struct *napi, struct sk_buff *skb)
{
	u32 bucket = skb_get_hash_raw(skb) & (GRO_HASH_BUCKETS - 1);
	struct gro_list *gro_list = &napi->gro_hash[bucket];
	struct list_head *head = &offload_base;
	struct packet_offload *ptype;
	__be16 type = skb->protocol;
	struct sk_buff *pp = NULL;
	enum gro_result ret;
	int same_flow;

	if (netif_elide_gro(skb->dev))
		goto normal;

	gro_list_prepare(&gro_list->list, skb);

	rcu_read_lock();
	list_for_each_entry_rcu(ptype, head, list) {
		if (ptype->type == type && ptype->callbacks.gro_receive)
			goto found_ptype;
	}
	rcu_read_unlock();
	goto normal;

found_ptype:
	skb_set_network_header(skb, skb_gro_offset(skb));
	skb_reset_mac_len(skb);
	BUILD_BUG_ON(sizeof_field(struct napi_gro_cb, zeroed) != sizeof(u32));
	BUILD_BUG_ON(!IS_ALIGNED(offsetof(struct napi_gro_cb, zeroed),
					sizeof(u32))); /* Avoid slow unaligned acc */
	*(u32 *)&NAPI_GRO_CB(skb)->zeroed = 0;
	NAPI_GRO_CB(skb)->flush = skb_has_frag_list(skb);
	NAPI_GRO_CB(skb)->is_atomic = 1;
	NAPI_GRO_CB(skb)->count = 1;
	if (unlikely(skb_is_gso(skb))) {
		NAPI_GRO_CB(skb)->count = skb_shinfo(skb)->gso_segs;
		/* Only support TCP and non DODGY users. */
		if (!skb_is_gso_tcp(skb) ||
		    (skb_shinfo(skb)->gso_type & SKB_GSO_DODGY))
			NAPI_GRO_CB(skb)->flush = 1;
	}

	/* Setup for GRO checksum validation */
	switch (skb->ip_summed) {
	case CHECKSUM_COMPLETE:
		NAPI_GRO_CB(skb)->csum = skb->csum;
		NAPI_GRO_CB(skb)->csum_valid = 1;
		break;
	case CHECKSUM_UNNECESSARY:
		NAPI_GRO_CB(skb)->csum_cnt = skb->csum_level + 1;
		break;
	}

	pp = INDIRECT_CALL_INET(ptype->callbacks.gro_receive,
				ipv6_gro_receive, inet_gro_receive,
				&gro_list->list, skb);

	rcu_read_unlock();

	if (PTR_ERR(pp) == -EINPROGRESS) {
		ret = GRO_CONSUMED;
		goto ok;
	}

	same_flow = NAPI_GRO_CB(skb)->same_flow;
	ret = NAPI_GRO_CB(skb)->free ? GRO_MERGED_FREE : GRO_MERGED;

	if (pp) {
		skb_list_del_init(pp);
		napi_gro_complete(napi, pp);
		gro_list->count--;
	}

	if (same_flow)
		goto ok;

	if (NAPI_GRO_CB(skb)->flush)
		goto normal;

	if (unlikely(gro_list->count >= MAX_GRO_SKBS))
		gro_flush_oldest(napi, &gro_list->list);
	else
		gro_list->count++;

	/* Must be called before setting NAPI_GRO_CB(skb)->{age|last} */
	gro_try_pull_from_frag0(skb);
	NAPI_GRO_CB(skb)->age = jiffies;
	NAPI_GRO_CB(skb)->last = skb;
	if (!skb_is_gso(skb))
		skb_shinfo(skb)->gso_size = skb_gro_len(skb);
	list_add(&skb->list, &gro_list->list);
	ret = GRO_HELD;
ok:
	if (gro_list->count) {
		if (!test_bit(bucket, &napi->gro_bitmask))
			__set_bit(bucket, &napi->gro_bitmask);
	} else if (test_bit(bucket, &napi->gro_bitmask)) {
		__clear_bit(bucket, &napi->gro_bitmask);
	}

	return ret;

normal:
	ret = GRO_NORMAL;
	gro_try_pull_from_frag0(skb);
	goto ok;
}
----

==== napi_skb_finish()
[source, c]
.https://elixir.bootlin.com/linux/latest/source/net/core/gro.c
----
static gro_result_t napi_skb_finish(struct napi_struct *napi,
				    struct sk_buff *skb,
				    gro_result_t ret)
{
	switch (ret) {
	case GRO_NORMAL:
		gro_normal_one(napi, skb, 1);
		break;

	case GRO_MERGED_FREE:
		if (NAPI_GRO_CB(skb)->free == NAPI_GRO_FREE_STOLEN_HEAD)
			napi_skb_free_stolen_head(skb);
		else if (skb->fclone != SKB_FCLONE_UNAVAILABLE)
			__kfree_skb(skb);
		else
			__napi_kfree_skb(skb, SKB_CONSUMED);
		break;

	case GRO_HELD:
	case GRO_MERGED:
	case GRO_CONSUMED:
		break;
	}

	return ret;
}
----

==== gro_normal_one()
    
    gro_normal_list() - include/net/gro.h
        netif_receive_skb_list_internal() - net/core/dev.c
            __netif_receive_skb_list() - net/core/dev.c
                __netif_receive_skb_list_core() - net/core/dev.c
                    __netif_receive_skb_core() - net/core/dev.c

=== 网络层

=== 传输层

=== 系统调用
无论是socket fd的read系统调用，还是recvfrom/recv/recvmsg系统调用，最终都会调用sock_recvmsg()。

=== 参考
https://blog.packagecloud.io/monitoring-tuning-linux-networking-stack-receiving-data/
https://arthurchiao.art/blog/linux-net-stack-implementation-rx-zh/
https://zhuanlan.zhihu.com/p/643195830
https://blog.51cto.com/u_15109148/5469108