:toc:
:toclevels: 5
:hardbreaks-option:

== 链路层

=== 网络设备

==== 注册与注销
注册/注销时机:
(1) 加载/卸载网络设备驱动程序
(2) 插入/移除可热插拔网络设备

分配:
alloc_netdev()
alloc_etherdev()

注册:
int register_netdevice(struct net_device *dev);
int register_netdev(struct net_device *dev);

释放:
void free_netdev(struct net_device *dev);

注销:
int unregister_netdevice(struct net_device *dev);
int unregister_netdev(struct net_device *dev);

注册状态通知:
int register_netdevice_notifier(struct notifier_block *nb);
int unregister_netdevice_notifier(struct notifier_block *nb);
等，即netdev_chain通知链

==== 启用与禁用
int dev_open(struct net_device *dev, struct netlink_ext_ack *extack);
void dev_close(struct net_device *dev);

==== 网卡启动
[source, c]
.https://elixir.bootlin.com/linux/latest/source/drivers/net/ethernet/intel/igb/igb_main.c
----
/**
 *  __igb_open - Called when a network interface is made active
 *  @netdev: network interface device structure
 *  @resuming: indicates whether we are in a resume call
 *
 *  Returns 0 on success, negative value on failure
 *
 *  The open entry point is called when a network interface is made
 *  active by the system (IFF_UP).  At this point all resources needed
 *  for transmit and receive operations are allocated, the interrupt
 *  handler is registered with the OS, the watchdog timer is started,
 *  and the stack is notified that the interface is ready.
 **/
static int __igb_open(struct net_device *netdev, bool resuming)
{
	struct igb_adapter *adapter = netdev_priv(netdev);
	struct e1000_hw *hw = &adapter->hw;
	struct pci_dev *pdev = adapter->pdev;
	int err;
	int i;

	/* disallow open during test */
	if (test_bit(__IGB_TESTING, &adapter->state)) {
		WARN_ON(resuming);
		return -EBUSY;
	}

	if (!resuming)
		pm_runtime_get_sync(&pdev->dev);

	netif_carrier_off(netdev);

	/* allocate transmit descriptors */
	err = igb_setup_all_tx_resources(adapter);
	if (err)
		goto err_setup_tx;

	/* allocate receive descriptors */
	err = igb_setup_all_rx_resources(adapter);
	if (err)
		goto err_setup_rx;

	igb_power_up_link(adapter);

	/* before we allocate an interrupt, we must be ready to handle it.
	 * Setting DEBUG_SHIRQ in the kernel makes it fire an interrupt
	 * as soon as we call pci_request_irq, so we have to setup our
	 * clean_rx handler before we do so.
	 */
	igb_configure(adapter);

	err = igb_request_irq(adapter);
	if (err)
		goto err_req_irq;

	/* Notify the stack of the actual queue counts. */
	err = netif_set_real_num_tx_queues(adapter->netdev,
					   adapter->num_tx_queues);
	if (err)
		goto err_set_queues;

	err = netif_set_real_num_rx_queues(adapter->netdev,
					   adapter->num_rx_queues);
	if (err)
		goto err_set_queues;

	/* From here on the code is the same as igb_up() */
	clear_bit(__IGB_DOWN, &adapter->state);

	for (i = 0; i < adapter->num_q_vectors; i++)
		napi_enable(&(adapter->q_vector[i]->napi));

	/* Clear any pending interrupts. */
	rd32(E1000_TSICR);
	rd32(E1000_ICR);

	igb_irq_enable(adapter);

	/* notify VFs that reset has been completed */
	if (adapter->vfs_allocated_count) {
		u32 reg_data = rd32(E1000_CTRL_EXT);

		reg_data |= E1000_CTRL_EXT_PFRSTD;
		wr32(E1000_CTRL_EXT, reg_data);
	}

	netif_tx_start_all_queues(netdev);

	if (!resuming)
		pm_runtime_put(&pdev->dev);

	/* start the watchdog. */
	hw->mac.get_link_status = 1;
	schedule_work(&adapter->watchdog_task);

	return 0;
	//...
}

int igb_open(struct net_device *netdev)
{
	return __igb_open(netdev, false);
}
----

==== 电源管理
以e1000网卡驱动为例:
[source, c]
.https://elixir.bootlin.com/linux/latest/source/drivers/net/ethernet/intel/e1000/e1000_main.c
----
static SIMPLE_DEV_PM_OPS(e1000_pm_ops, e1000_suspend, e1000_resume);
----
suspend表示挂起，resume表示唤醒。

==== 侦测连接状态改变
void netif_carrier_on(struct net_device *dev);
void netif_carrier_off(struct net_device *dev);
void netif_carrier_event(struct net_device *dev);
例如从网络设备插拔网线或者网络另一端的设备(例如路由器)的关闭/禁止，均会导致连接状态的改变。

static void linkwatch_event(struct work_struct *dummy);
static DECLARE_DELAYED_WORK(linkwatch_work, linkwatch_event);

==== 虚拟网络设备
https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking

=== 中断与网络驱动程序
内核准备处理进出L2层的帧之前，必须先处理中断系统。

负责接受帧的代码分成两部分: 首先，驱动程序把该帧拷贝到内核可访问的输入队列，然后内核再予以处理，通常是把那个帧传给一个相关协议(如IP)专用的处理函数。第一部分会在中断环境中执行, 而且可以抢占第二部分的执行。

在高流量负载下，中断代码会持续抢占正在处理的代码。这种结果很明显: 到某一个时间点，输入队列会满，但是由于应该让帧退出队列并予以处理的代码的优先级过低而没有机会执行，结果系统就崩溃了。新的帧无法排入队列，因为已经没有空间了，而旧的帧又无法被处理，因为没有CPU可供其使用了，这种情况被称为接受-活锁(receive-livelock)。

在低负载下，纯中断模型可以保证低时延，但是在高负载下其运行就很糟糕。
另一方面，定时器驱动的中断事件在低负载下可能会引入过多的延时，而且浪费更多的CPU时间，但是在高负载下可以大量减少CPU用量并解决接受-活锁问题。
好的组合就是在低负载下使用中断技术，在高负载下切换至定时器驱动的中断事件。

=== 网络接口层的输入

==== softnet_data结构
softnet_data是每CPU变量，描述了与网络软中断处理相关的报文输入和输出队列，是网络接口层与网络层之间的桥梁。

[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/netdevice.h
----
/*
 * Incoming packets are placed on per-CPU queues
 */
struct softnet_data {
	struct list_head	poll_list;
	struct sk_buff_head	process_queue;

	/* stats */
	unsigned int		processed;
	unsigned int		time_squeeze;
#ifdef CONFIG_RPS
	struct softnet_data	*rps_ipi_list;
#endif
#ifdef CONFIG_NET_FLOW_LIMIT
	struct sd_flow_limit __rcu *flow_limit;
#endif
	struct Qdisc		*output_queue;
	struct Qdisc		**output_queue_tailp;
	struct sk_buff		*completion_queue;
#ifdef CONFIG_XFRM_OFFLOAD
	struct sk_buff_head	xfrm_backlog;
#endif
	/* written and read only by owning cpu: */
	struct {
		u16 recursion;
		u8  more;
#ifdef CONFIG_NET_EGRESS
		u8  skip_txqueue;
#endif
	} xmit;
#ifdef CONFIG_RPS
	/* input_queue_head should be written by cpu owning this struct,
	 * and only read by other cpus. Worth using a cache line.
	 */
	unsigned int		input_queue_head ____cacheline_aligned_in_smp;

	/* Elements below can be accessed between CPUs for RPS/RFS */
	call_single_data_t	csd ____cacheline_aligned_in_smp;
	struct softnet_data	*rps_ipi_next;
	unsigned int		cpu;
	unsigned int		input_queue_tail;
#endif
	unsigned int		received_rps;
	unsigned int		dropped;
	struct sk_buff_head	input_pkt_queue;
	struct napi_struct	backlog;

	/* Another possibly contended cache line */
	spinlock_t		defer_lock ____cacheline_aligned_in_smp;
	int			defer_count;
	int			defer_ipi_scheduled;
	struct sk_buff		*defer_list;
	call_single_data_t	defer_csd;
};
----

==== NAPI
NAPI(New API):
在高负载的网络数据传输时，网络驱动收到硬件中断后，通过poll(轮询)方式将传过来的数据包统一处理，在poll时通过禁止网络设备中断来减少硬件中断数量，以实现更高的数据传输速率。

参考:
https://docs.kernel.org/networking/napi.html
https://wiki.linuxfoundation.org/networking/napi
https://lwn.net/Kernel/Index/#NAPI
https://lwn.net/Kernel/Index/#Networking-NAPI

==== 非NAPI
int netif_rx(struct sk_buff *skb):
https://elixir.bootlin.com/linux/latest/source/net/core/dev.c

static int process_backlog(struct napi_struct *napi, int quota):
https://elixir.bootlin.com/linux/latest/source/net/core/dev.c

==== 报文处理
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/netdevice.h
----
struct packet_type {
	__be16			type;	/* This is really htons(ether_type). */
	bool			ignore_outgoing;
	struct net_device	*dev;	/* NULL is wildcarded here	     */
	netdevice_tracker	dev_tracker;
	int			(*func) (struct sk_buff *,
					 struct net_device *,
					 struct packet_type *,
					 struct net_device *);
	void			(*list_func) (struct list_head *,
					      struct packet_type *,
					      struct net_device *);
	bool			(*id_match)(struct packet_type *ptype,
					    struct sock *sk);
	struct net		*af_packet_net;
	void			*af_packet_priv;
	struct list_head	list;
};
----

int netif_receive_skb(struct sk_buff *skb):
https://elixir.bootlin.com/linux/latest/source/net/core/dev.c

void dev_queue_xmit_nit(struct sk_buff *skb, struct net_device *dev):
https://elixir.bootlin.com/linux/latest/source/net/core/dev.c

=== 网络接口层的输出

==== 输出接口

===== dev_queue_xmit()
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/netdevice.h
----
static inline int dev_queue_xmit(struct sk_buff *skb)
{
	return __dev_queue_xmit(skb, NULL);
}
----

[source, c]
.https://elixir.bootlin.com/linux/latest/source/net/core/dev.c
----
/**
 * __dev_queue_xmit() - transmit a buffer
 * @skb:	buffer to transmit
 * @sb_dev:	suboordinate device used for L2 forwarding offload
 *
 * Queue a buffer for transmission to a network device. The caller must
 * have set the device and priority and built the buffer before calling
 * this function. The function can be called from an interrupt.
 *
 * When calling this method, interrupts MUST be enabled. This is because
 * the BH enable code must have IRQs enabled so that it will not deadlock.
 *
 * Regardless of the return value, the skb is consumed, so it is currently
 * difficult to retry a send to this method. (You can bump the ref count
 * before sending to hold a reference for retry if you are careful.)
 *
 * Return:
 * * 0				- buffer successfully transmitted
 * * positive qdisc return code	- NET_XMIT_DROP etc.
 * * negative errno		- other errors
 */
int __dev_queue_xmit(struct sk_buff *skb, struct net_device *sb_dev)
{
	struct net_device *dev = skb->dev;
	struct netdev_queue *txq = NULL;
	struct Qdisc *q;
	int rc = -ENOMEM;
	bool again = false;

	skb_reset_mac_header(skb);
	skb_assert_len(skb);

	if (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_SCHED_TSTAMP))
		__skb_tstamp_tx(skb, NULL, NULL, skb->sk, SCM_TSTAMP_SCHED);

	/* Disable soft irqs for various locks below. Also
	 * stops preemption for RCU.
	 */
	rcu_read_lock_bh();

	skb_update_prio(skb);

	qdisc_pkt_len_init(skb);
#ifdef CONFIG_NET_CLS_ACT
	skb->tc_at_ingress = 0;
#endif
#ifdef CONFIG_NET_EGRESS
	if (static_branch_unlikely(&egress_needed_key)) {
		if (nf_hook_egress_active()) {
			skb = nf_hook_egress(skb, &rc, dev);
			if (!skb)
				goto out;
		}

		netdev_xmit_skip_txqueue(false);

		nf_skip_egress(skb, true);
		skb = sch_handle_egress(skb, &rc, dev);
		if (!skb)
			goto out;
		nf_skip_egress(skb, false);

		if (netdev_xmit_txqueue_skipped())
			txq = netdev_tx_queue_mapping(dev, skb);
	}
#endif
	/* If device/qdisc don't need skb->dst, release it right now while
	 * its hot in this cpu cache.
	 */
	if (dev->priv_flags & IFF_XMIT_DST_RELEASE)
		skb_dst_drop(skb);
	else
		skb_dst_force(skb);

	if (!txq)
		txq = netdev_core_pick_tx(dev, skb, sb_dev);

	q = rcu_dereference_bh(txq->qdisc);

	trace_net_dev_queue(skb);
	if (q->enqueue) {
		rc = __dev_xmit_skb(skb, q, dev, txq);
		goto out;
	}

	/* The device has no queue. Common case for software devices:
	 * loopback, all the sorts of tunnels...

	 * Really, it is unlikely that netif_tx_lock protection is necessary
	 * here.  (f.e. loopback and IP tunnels are clean ignoring statistics
	 * counters.)
	 * However, it is possible, that they rely on protection
	 * made by us here.

	 * Check this and shot the lock. It is not prone from deadlocks.
	 *Either shot noqueue qdisc, it is even simpler 8)
	 */
	if (dev->flags & IFF_UP) {
		int cpu = smp_processor_id(); /* ok because BHs are off */

		/* Other cpus might concurrently change txq->xmit_lock_owner
		 * to -1 or to their cpu id, but not to our id.
		 */
		if (READ_ONCE(txq->xmit_lock_owner) != cpu) {
			if (dev_xmit_recursion())
				goto recursion_alert;

			skb = validate_xmit_skb(skb, dev, &again);
			if (!skb)
				goto out;

			HARD_TX_LOCK(dev, txq, cpu);

			if (!netif_xmit_stopped(txq)) {
				dev_xmit_recursion_inc();
				skb = dev_hard_start_xmit(skb, dev, txq, &rc);
				dev_xmit_recursion_dec();
				if (dev_xmit_complete(rc)) {
					HARD_TX_UNLOCK(dev, txq);
					goto out;
				}
			}
			HARD_TX_UNLOCK(dev, txq);
			net_crit_ratelimited("Virtual device %s asks to queue packet!\n",
					     dev->name);
		} else {
			/* Recursion is detected! It is possible,
			 * unfortunately
			 */
recursion_alert:
			net_crit_ratelimited("Dead loop on virtual device %s, fix it urgently!\n",
					     dev->name);
		}
	}

	rc = -ENETDOWN;
	rcu_read_unlock_bh();

	dev_core_stats_tx_dropped_inc(dev);
	kfree_skb_list(skb);
	return rc;
out:
	rcu_read_unlock_bh();
	return rc;
}
----
现代网卡是有多个发送队列的，首先要选择一个队列即struct netdev_queue *txq，之后调用__dev_xmit_skb():
(其实还有dev_hard_start_xmit()这条路径，后面会分析dev_hard_start_xmit()，此处先忽略)
[source, c]
.https://elixir.bootlin.com/linux/latest/source/net/core/dev.c
----
static inline int __dev_xmit_skb(struct sk_buff *skb, struct Qdisc *q,
				 struct net_device *dev,
				 struct netdev_queue *txq)
{
	spinlock_t *root_lock = qdisc_lock(q);
	struct sk_buff *to_free = NULL;
	bool contended;
	int rc;

	qdisc_calculate_pkt_len(skb, q);

	if (q->flags & TCQ_F_NOLOCK) {
		if (q->flags & TCQ_F_CAN_BYPASS && nolock_qdisc_is_empty(q) &&
		    qdisc_run_begin(q)) {
			/* Retest nolock_qdisc_is_empty() within the protection
			 * of q->seqlock to protect from racing with requeuing.
			 */
			if (unlikely(!nolock_qdisc_is_empty(q))) {
				rc = dev_qdisc_enqueue(skb, q, &to_free, txq);
				__qdisc_run(q);
				qdisc_run_end(q);

				goto no_lock_out;
			}

			qdisc_bstats_cpu_update(q, skb);
			if (sch_direct_xmit(skb, q, dev, txq, NULL, true) &&
			    !nolock_qdisc_is_empty(q))
				__qdisc_run(q);

			qdisc_run_end(q);
			return NET_XMIT_SUCCESS;
		}

		rc = dev_qdisc_enqueue(skb, q, &to_free, txq);
		qdisc_run(q);

no_lock_out:
		if (unlikely(to_free))
			kfree_skb_list_reason(to_free,
					      SKB_DROP_REASON_QDISC_DROP);
		return rc;
	}

	/*
	 * Heuristic to force contended enqueues to serialize on a
	 * separate lock before trying to get qdisc main lock.
	 * This permits qdisc->running owner to get the lock more
	 * often and dequeue packets faster.
	 * On PREEMPT_RT it is possible to preempt the qdisc owner during xmit
	 * and then other tasks will only enqueue packets. The packets will be
	 * sent after the qdisc owner is scheduled again. To prevent this
	 * scenario the task always serialize on the lock.
	 */
	contended = qdisc_is_running(q) || IS_ENABLED(CONFIG_PREEMPT_RT);
	if (unlikely(contended))
		spin_lock(&q->busylock);

	spin_lock(root_lock);
	if (unlikely(test_bit(__QDISC_STATE_DEACTIVATED, &q->state))) {
		__qdisc_drop(skb, &to_free);
		rc = NET_XMIT_DROP;
	} else if ((q->flags & TCQ_F_CAN_BYPASS) && !qdisc_qlen(q) &&
		   qdisc_run_begin(q)) {
		/*
		 * This is a work-conserving queue; there are no old skbs
		 * waiting to be sent out; and the qdisc is not running -
		 * xmit the skb directly.
		 */

		qdisc_bstats_update(q, skb);

		if (sch_direct_xmit(skb, q, dev, txq, root_lock, true)) {
			if (unlikely(contended)) {
				spin_unlock(&q->busylock);
				contended = false;
			}
			__qdisc_run(q);
		}

		qdisc_run_end(q);
		rc = NET_XMIT_SUCCESS;
	} else {
		rc = dev_qdisc_enqueue(skb, q, &to_free, txq);
		if (qdisc_run_begin(q)) {
			if (unlikely(contended)) {
				spin_unlock(&q->busylock);
				contended = false;
			}
			__qdisc_run(q);
			qdisc_run_end(q);
		}
	}
	spin_unlock(root_lock);
	if (unlikely(to_free))
		kfree_skb_list_reason(to_free, SKB_DROP_REASON_QDISC_DROP);
	if (unlikely(contended))
		spin_unlock(&q->busylock);
	return rc;
}
----
关键函数是__qdisc_run()与sch_direct_xmit()。
而__qdisc_run()最终也会调用sch_direct_xmit(): https://elixir.bootlin.com/linux/latest/source/net/sched/sch_generic.c

    __qdisc_run()
        qdisc_restart()
            sch_direct_xmit()
                dev_hard_start_xmit()


struct sk_buff *dev_hard_start_xmit(struct sk_buff *first, struct net_device *dev, struct netdev_queue *txq, int *ret):
https://elixir.bootlin.com/linux/latest/source/net/core/dev.c

e1000:
static netdev_tx_t e1000_xmit_frame(struct sk_buff *skb, struct net_device *netdev):
https://elixir.bootlin.com/linux/latest/source/drivers/net/ethernet/intel/e1000/e1000_main.c

==== 网络输出软中断
void __netif_schedule(struct Qdisc *q):
https://elixir.bootlin.com/linux/latest/source/net/core/dev.c

static __latent_entropy void net_tx_action(struct softirq_action *h):
https://elixir.bootlin.com/linux/latest/source/net/core/dev.c

==== 设备不支持GSO的处理
struct sk_buff *__skb_gso_segment(struct sk_buff *skb, netdev_features_t features, bool tx_path):
https://elixir.bootlin.com/linux/latest/source/net/core/dev.c

=== 帧的接收
==== 帧接收的中断处理
中断处理函数:
一些立即性的任务,
1) 把帧拷贝到sk_buf结构(DMA就只需初始化一个指针不需要拷贝)
2) 对一些sk_buff参数进行初始化以便稍后由网络层使用
3) 更新其它一些该设备私用的参数
为NET_RX_SOFTIRQ调度以准备执行。

==== 设备的开启与关闭
设备的开启与关闭是由net_device->state成员进行标识的
dev_open(), dev_close(), _LINK_STATE_START: https://elixir.bootlin.com/linux/latest/source/net/core/dev.c

==== 队列
帧接收时有入口队列，帧传输时有出口队列。
每个队列都有一个指针指向其相关的设备，以及一个指针指向存储输入/输出缓冲区的sk_buff数据接口。
只有少数专用设备不需要队列，例如回环设备。
回环设备: https://elixir.bootlin.com/linux/latest/source/drivers/net/loopback.c

==== 通知内核帧已接收: NAPI和netif_rx
https://elixir.bootlin.com/linux/latest/source/include/linux/netdevice.h
https://elixir.bootlin.com/linux/latest/source/net/core/dev.c

==== 拥塞管理
高流量网络负载下降低CPU负载的常见方式:
尽可能减少中断事件的数目: 实现方法是驱动程序在一次中断事件中处理许多帧, 或者使用NAPI.
在入口尽早丢弃帧：例如如果设备驱动程序知道入口队列已经满了，就可以立即丢弃帧，而不用转发给内核使其稍后再予以丢弃。

=== 接受分组
分组到达内核的时间是不可预测的。所有现代的设备驱动程序都使用中断来通知内核有分组到达。网络驱动程序对特定于设备的中断设置了一个处理例程，因此每当该中断被引发(即分组到达)，内核都调用该处理程序，将数据从网卡传输到物理内存，或通知内核在一定时间后处理。
几乎所有的网卡都支持DMA模式，能够自行将数据传输到物理内存，但这些数据仍然需要解释和处理。

- 传统方法

- 高速接口

=== 发送分组
int dev_queue_xmit(struct sk_buff *skb)
https://elixir.bootlin.com/linux/latest/source/include/linux/netdevice.h
https://elixir.bootlin.com/linux/latest/source/net/core/dev.c
