:toc:
:toclevels: 5
:hardbreaks-option:

== 链路层

=== 说明
这里的链路层指TCP/IP分层模型(四层模型)中的链路层，也称网络接口层，等同于OSI七层模型中的数据链路层加上物理层。

在使用L层术语的时候, 则表示OSI七层模型的第几层。

参考:
https://en.wikipedia.org/wiki/Internet_protocol_suite#Layering_evolution_and_representations_in_the_literature

=== 网络设备

==== 注册与注销
注册/注销时机:
(1) 加载/卸载网络设备驱动程序
(2) 插入/移除可热插拔网络设备

分配:
alloc_netdev()
alloc_etherdev()

注册:
int register_netdevice(struct net_device *dev);
int register_netdev(struct net_device *dev);

释放:
void free_netdev(struct net_device *dev);

注销:
int unregister_netdevice(struct net_device *dev);
int unregister_netdev(struct net_device *dev);

注册状态通知:
int register_netdevice_notifier(struct notifier_block *nb);
int unregister_netdevice_notifier(struct notifier_block *nb);
等，即netdev_chain通知链

==== 启用与禁用
int dev_open(struct net_device *dev, struct netlink_ext_ack *extack);
void dev_close(struct net_device *dev);

==== 电源管理
以e1000网卡驱动为例:
[source, c]
.https://elixir.bootlin.com/linux/latest/source/drivers/net/ethernet/intel/e1000/e1000_main.c
----
static SIMPLE_DEV_PM_OPS(e1000_pm_ops, e1000_suspend, e1000_resume);
----
suspend表示挂起，resume表示唤醒。

==== 侦测连接状态改变
void netif_carrier_on(struct net_device *dev);
void netif_carrier_off(struct net_device *dev);
void netif_carrier_event(struct net_device *dev);
例如从网络设备插拔网线或者网络另一端的设备(例如路由器)的关闭/禁止，均会导致连接状态的改变。

static void linkwatch_event(struct work_struct *dummy);
static DECLARE_DELAYED_WORK(linkwatch_work, linkwatch_event);

==== 虚拟网络设备
https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking

=== 中断与网络驱动程序
内核准备处理进出L2层的帧之前，必须先处理中断系统。

负责接受帧的代码分成两部分: 首先，驱动程序把该帧拷贝到内核可访问的输入队列，然后内核再予以处理，通常是把那个帧传给一个相关协议(如IP)专用的处理函数。第一部分会在中断环境中执行, 而且可以抢占第二部分的执行。

在高流量负载下，中断代码会持续抢占正在处理的代码。这种结果很明显: 到某一个时间点，输入队列会满，但是由于应该让帧退出队列并予以处理的代码的优先级过低而没有机会执行，结果系统就崩溃了。新的帧无法排入队列，因为已经没有空间了，而旧的帧又无法被处理，因为没有CPU可供其使用了，这种情况被称为接受-活锁(receive-livelock)。

在低负载下，纯中断模型可以保证低时延，但是在高负载下其运行就很糟糕。
另一方面，定时器驱动的中断事件在低负载下可能会引入过多的延时，而且浪费更多的CPU时间，但是在高负载下可以大量减少CPU用量并解决接受-活锁问题。
好的组合就是在低负载下使用中断技术，在高负载下切换至定时器驱动的中断事件。

=== 网络接口层的输入

==== softnet_data结构
softnet_data是每CPU变量，描述了与网络软中断处理相关的报文输入和输出队列，是网络接口层与网络层之间的桥梁。

[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/netdevice.h
----
/*
 * Incoming packets are placed on per-CPU queues
 */
struct softnet_data {
	struct list_head	poll_list;
	struct sk_buff_head	process_queue;

	/* stats */
	unsigned int		processed;
	unsigned int		time_squeeze;
#ifdef CONFIG_RPS
	struct softnet_data	*rps_ipi_list;
#endif
#ifdef CONFIG_NET_FLOW_LIMIT
	struct sd_flow_limit __rcu *flow_limit;
#endif
	struct Qdisc		*output_queue;
	struct Qdisc		**output_queue_tailp;
	struct sk_buff		*completion_queue;
#ifdef CONFIG_XFRM_OFFLOAD
	struct sk_buff_head	xfrm_backlog;
#endif
	/* written and read only by owning cpu: */
	struct {
		u16 recursion;
		u8  more;
#ifdef CONFIG_NET_EGRESS
		u8  skip_txqueue;
#endif
	} xmit;
#ifdef CONFIG_RPS
	/* input_queue_head should be written by cpu owning this struct,
	 * and only read by other cpus. Worth using a cache line.
	 */
	unsigned int		input_queue_head ____cacheline_aligned_in_smp;

	/* Elements below can be accessed between CPUs for RPS/RFS */
	call_single_data_t	csd ____cacheline_aligned_in_smp;
	struct softnet_data	*rps_ipi_next;
	unsigned int		cpu;
	unsigned int		input_queue_tail;
#endif
	unsigned int		received_rps;
	unsigned int		dropped;
	struct sk_buff_head	input_pkt_queue;
	struct napi_struct	backlog;

	/* Another possibly contended cache line */
	spinlock_t		defer_lock ____cacheline_aligned_in_smp;
	int			defer_count;
	int			defer_ipi_scheduled;
	struct sk_buff		*defer_list;
	call_single_data_t	defer_csd;
};
----

==== NAPI
NAPI(New API):
在高负载的网络数据传输时，网络驱动收到硬件中断后，通过poll(轮询)方式将传过来的数据包统一处理，在poll时通过禁止网络设备中断来减少硬件中断数量，以实现更高的数据传输速率。

参考:
https://docs.kernel.org/networking/napi.html
https://wiki.linuxfoundation.org/networking/napi
https://lwn.net/Kernel/Index/#NAPI
https://lwn.net/Kernel/Index/#Networking-NAPI

==== 非NAPI
int netif_rx(struct sk_buff *skb):
https://elixir.bootlin.com/linux/latest/source/net/core/dev.c

static int process_backlog(struct napi_struct *napi, int quota):
https://elixir.bootlin.com/linux/latest/source/net/core/dev.c

==== 报文处理
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/netdevice.h
----
struct packet_type {
	__be16			type;	/* This is really htons(ether_type). */
	bool			ignore_outgoing;
	struct net_device	*dev;	/* NULL is wildcarded here	     */
	netdevice_tracker	dev_tracker;
	int			(*func) (struct sk_buff *,
					 struct net_device *,
					 struct packet_type *,
					 struct net_device *);
	void			(*list_func) (struct list_head *,
					      struct packet_type *,
					      struct net_device *);
	bool			(*id_match)(struct packet_type *ptype,
					    struct sock *sk);
	struct net		*af_packet_net;
	void			*af_packet_priv;
	struct list_head	list;
};
----

int netif_receive_skb(struct sk_buff *skb):
https://elixir.bootlin.com/linux/latest/source/net/core/dev.c

void dev_queue_xmit_nit(struct sk_buff *skb, struct net_device *dev):
https://elixir.bootlin.com/linux/latest/source/net/core/dev.c

=== 网络接口层的输出

==== 输出接口
int __dev_queue_xmit(struct sk_buff *skb, struct net_device *sb_dev):
https://elixir.bootlin.com/linux/latest/source/net/core/dev.c

struct sk_buff *dev_hard_start_xmit(struct sk_buff *first, struct net_device *dev, struct netdev_queue *txq, int *ret):
https://elixir.bootlin.com/linux/latest/source/net/core/dev.c

e1000:
static netdev_tx_t e1000_xmit_frame(struct sk_buff *skb, struct net_device *netdev):
https://elixir.bootlin.com/linux/latest/source/drivers/net/ethernet/intel/e1000/e1000_main.c

=== 帧的接收
==== 帧接收的中断处理
中断处理函数:
一些立即性的任务,
1) 把帧拷贝到sk_buf结构(DMA就只需初始化一个指针不需要拷贝)
2) 对一些sk_buff参数进行初始化以便稍后由网络层使用
3) 更新其它一些该设备私用的参数
为NET_RX_SOFTIRQ调度以准备执行。

==== 设备的开启与关闭
设备的开启与关闭是由net_device->state成员进行标识的
dev_open(), dev_close(), _LINK_STATE_START: https://elixir.bootlin.com/linux/latest/source/net/core/dev.c

==== 队列
帧接收时有入口队列，帧传输时有出口队列。
每个队列都有一个指针指向其相关的设备，以及一个指针指向存储输入/输出缓冲区的sk_buff数据接口。
只有少数专用设备不需要队列，例如回环设备。
回环设备: https://elixir.bootlin.com/linux/latest/source/drivers/net/loopback.c

==== 通知内核帧已接收: NAPI和netif_rx
https://elixir.bootlin.com/linux/latest/source/include/linux/netdevice.h
https://elixir.bootlin.com/linux/latest/source/net/core/dev.c

==== 拥塞管理
高流量网络负载下降低CPU负载的常见方式:
尽可能减少中断事件的数目: 实现方法是驱动程序在一次中断事件中处理许多帧, 或者使用NAPI.
在入口尽早丢弃帧：例如如果设备驱动程序知道入口队列已经满了，就可以立即丢弃帧，而不用转发给内核使其稍后再予以丢弃。

=== 接受分组
分组到达内核的时间是不可预测的。所有现代的设备驱动程序都使用中断来通知内核有分组到达。网络驱动程序对特定于设备的中断设置了一个处理例程，因此每当该中断被引发(即分组到达)，内核都调用该处理程序，将数据从网卡传输到物理内存，或通知内核在一定时间后处理。
几乎所有的网卡都支持DMA模式，能够自行将数据传输到物理内存，但这些数据仍然需要解释和处理。

- 传统方法

- 高速接口

=== 发送分组
int dev_queue_xmit(struct sk_buff *skb)
https://elixir.bootlin.com/linux/latest/source/include/linux/netdevice.h
https://elixir.bootlin.com/linux/latest/source/net/core/dev.c
