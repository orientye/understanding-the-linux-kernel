:toc:
:toclevels: 5
:hardbreaks-option:

== 数据的发送
=== 概要
[source, c]
.https://elixir.bootlin.com/linux/latest/source/net/socket.c
----
SYSCALL_DEFINE3(sendmsg, int, fd, struct user_msghdr __user *, msg, unsigned int, flags)
{
	return __sys_sendmsg(fd, msg, flags, true);
}
----

__sys_sendmsg:

    ____sys_sendmsg
            sock_sendmsg_nosec(sock, msg_sys);
            sock_sendmsg(sock, msg_sys);

sock_sendmsg最终也是调用sock_sendmsg_nosec
sock_sendmsg_nosec最终调用inet6_sendmsg或inet_sendmsg
最终调用tcp_sendmsg, udp_sendmsg/udpv6_sendmsg:

=== 发送数据包过程概述
• 使用系统调用(如sendto，sendmsg等)写数据
• 数据穿过socket子系统，进入socket协议族(protocol family)系统
• 协议族处理：数据穿过协议层，这一过程(在许多情况下)会将数据(data)转换成数据包(packet)
• 数据穿过路由层，这会涉及路由缓存和ARP缓存的更新；如果目的MAC不在ARP缓存表中，将触发一次ARP广播来查找MAC地址
• 穿过协议层，packet到达设备无关层(device agnostic layer)
• 使用XPS(如果启用)或散列函数选择发送队列
• 调用网卡驱动的发送函数
• 数据传送到网卡的qdisc(queue discipline，排队规则)
• qdisc会直接发送数据(如果可以)，或者将其放到队列，下次触发NET_TX类型软中断(softirq)的时候再发送
• 数据从qdisc传送给驱动程序
• 驱动程序创建所需的DMA映射，以便网卡从RAM读取数据
• 驱动向网卡发送信号，通知数据可以发送了
• 网卡从RAM中获取数据并发送
• 发送完成后，设备触发一个硬中断(IRQ)，表示发送完成
• 硬中断处理函数被唤醒执行。对许多设备来说，这会触发NET_RX类型的软中断，然后NAPI poll循环开始收包
• poll函数会调用驱动程序的相应函数，解除DMA映射，释放数据

=== tcp_sendmsg()
https://elixir.bootlin.com/linux/latest/source/net/ipv4/tcp.c

    tcp_sendmsg_locked
        tcp_push
            __tcp_push_pending_frames: https://elixir.bootlin.com/linux/latest/source/net/ipv4/tcp_output.c
                tcp_write_xmit
                    tcp_transmit_skb
                         __tcp_transmit_skb
                            icsk->icsk_af_ops->queue_xmit
                            ip_queue_xmit: https://elixir.bootlin.com/linux/latest/source/net/ipv4/ip_output.c
                                __ip_queue_xmit
                                    ip_local_out

=== udp_sendmsg()
https://elixir.bootlin.com/linux/latest/source/net/ipv4/udp.c

=== ip_local_out()
[source, c]
.https://elixir.bootlin.com/linux/latest/source/net/ipv4/ip_output.c
----
int __ip_local_out(struct net *net, struct sock *sk, struct sk_buff *skb)
{
	struct iphdr *iph = ip_hdr(skb);

	iph_set_totlen(iph, skb->len);
	ip_send_check(iph);

	/* if egress device is enslaved to an L3 master device pass the
	 * skb to its handler for processing
	 */
	skb = l3mdev_ip_out(sk, skb);
	if (unlikely(!skb))
		return 0;

	skb->protocol = htons(ETH_P_IP);

	return nf_hook(NFPROTO_IPV4, NF_INET_LOCAL_OUT,
		       net, sk, skb, NULL, skb_dst(skb)->dev,
		       dst_output);
}

int ip_local_out(struct net *net, struct sock *sk, struct sk_buff *skb)
{
	int err;

	err = __ip_local_out(net, sk, skb);
	if (likely(err == 1))
		err = dst_output(net, sk, skb);

	return err;
}
----
ip_local_out调用__ip_local_out，如果返回值为1，则调用路由层dst_output发送数据包。
__ip_local_out(net, sk, skb):
设置IP数据包的长度；
然后调用ip_send_check来计算要写入IP头的校验和；
之后调用nf_hook进入netfilter，其返回值将传递回ip_local_out。如果nf_hook返回1，则表示允许数据包通过，并且调用者应该自己发送数据包。即上文中的ip_local_out检查返回值1，调用dst_output发送数据包。

nf_hook与netfilter:
nf_hook是一个wrapper，它会检查是否有为这个协议族和hook类型(这里分别为 NFPROTO_IPV4和NF_INET_LOCAL_OUT)安装的过滤器，然后将返回到IP协议层，避免深入到netfilter或更下面，比如iptables和conntrack。
如果有非常多或者非常复杂的netfilter或iptables规则，那些规则将在触发sendmsg系统调的用户进程的上下文中执行。如果对这个用户进程设置了CPU亲和性，相应的CPU将花费系统时间(system time)处理出站(outbound)iptables 规则。如果做性能回归测试，那可能要考虑根据系统的负载，将相应的用户进程绑到到特定的CPU，或者是减少netfilter/iptables规则的复杂度，以减少对性能测试的影响。
假设nf_hook返回1，表示调用者(在这种情况下是IP协议层)应该自己发送数据包。

dst_output:
dst代码在内核中实现协议无关的目标缓存。
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/net/dst.h
----
/* Output packet to network from transport.  */
static inline int dst_output(struct net *net, struct sock *sk, struct sk_buff *skb)
{
	return INDIRECT_CALL_INET(skb_dst(skb)->output,
				  ip6_output, ip_output,
				  net, sk, skb);
}
----
dst_output()查找关联到skb的dst条目，然后调用output方法，大多数情况下是ip_output。

=== ip_output()
ip_output调用ip_finish_output。

=== ip_finish_output()
ip_finish_out调用ip_finish_output2。

[source, c]
.https://elixir.bootlin.com/linux/latest/source/net/ipv4/ip_output.c
----
static int ip_finish_output2(struct net *net, struct sock *sk, struct sk_buff *skb)
{
	struct dst_entry *dst = skb_dst(skb);
	struct rtable *rt = (struct rtable *)dst;
	struct net_device *dev = dst->dev;
	unsigned int hh_len = LL_RESERVED_SPACE(dev);
	struct neighbour *neigh;
	bool is_v6gw = false;

	if (rt->rt_type == RTN_MULTICAST) {
		IP_UPD_PO_STATS(net, IPSTATS_MIB_OUTMCAST, skb->len);
	} else if (rt->rt_type == RTN_BROADCAST)
		IP_UPD_PO_STATS(net, IPSTATS_MIB_OUTBCAST, skb->len);

	if (unlikely(skb_headroom(skb) < hh_len && dev->header_ops)) {
		skb = skb_expand_head(skb, hh_len);
		if (!skb)
			return -ENOMEM;
	}

	if (lwtunnel_xmit_redirect(dst->lwtstate)) {
		int res = lwtunnel_xmit(skb);

		if (res != LWTUNNEL_XMIT_CONTINUE)
			return res;
	}

	rcu_read_lock();
	neigh = ip_neigh_for_gw(rt, skb, &is_v6gw);
	if (!IS_ERR(neigh)) {
		int res;

		sock_confirm_neigh(skb, neigh);
		/* if crossing protocols, can not use the cached header */
		res = neigh_output(neigh, skb, is_v6gw);
		rcu_read_unlock();
		return res;
	}
	rcu_read_unlock();

	net_dbg_ratelimited("%s: No header cache and no neighbour!\n",
			    __func__);
	kfree_skb_reason(skb, SKB_DROP_REASON_NEIGH_CREATEFAIL);
	return PTR_ERR(neigh);
}
----

=== neigh_output()
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/net/neighbour.h
----
static inline int neigh_output(struct neighbour *n, struct sk_buff *skb,
			       bool skip_cache)
{
	const struct hh_cache *hh = &n->hh;

	/* n->nud_state and hh->hh_len could be changed under us.
	 * () is taking care of the race later.
	 */
	if (!skip_cache &&
	    (READ_ONCE(n->nud_state) & NUD_CONNECTED) &&
	    READ_ONCE(hh->hh_len))
		return neigh_hh_output(hh, skb);

	return n->output(n, skb);
}
----

如果hardware header(hh)被缓存(之前发送过数据并进行了缓存)，将调用neigh_hh_output函数;
否则，调用n->output函数。
这两种情况，最后都会调用dev_queue_xmit()，将skb发送给网络设备子系统，在它进入设备驱动程序层之前将对其进行更多处理。

n->output():

struct neighbour包含几个重要字段：nud_state字段，output函数和ops结构。
到如果在缓存中找不到现有条目，会从ip_finish_output2调用__neigh_create创建一个。当调用__neigh_create时，将分配邻居，其output函数最初设置为neigh_blackhole。随着__neigh_create代码的进行，它将根据邻居的状态修改output值以指向适当的发送方法。

例如，当代码确定是"已连接的"邻居时，neigh_connect会将output设置为neigh->ops->connected_output。当代码怀疑邻居可能已关闭时，neigh_suspect会将output设置为neigh->ops->output（例如，如果已超过/proc/sys/net/ipv4/neigh/default/delay_first_probe_time自发送探测以来的delay_first_probe_time 秒）。

也就是说neigh->output会被设置为neigh->ops_connected_output或neigh->ops->output，具体取决于邻居的状态。neigh->ops来自哪里？

分配邻居后，调用arp_constructor来设置struct neighbor的某些字段。特别是，此函数会检查与此邻居关联的设备是否导出来一个struct header_ops实例，该结构体有一个cache方法。
[source, c]
.https://elixir.bootlin.com/linux/latest/source/net/ipv4/arp.c
----
static const struct neigh_ops arp_hh_ops = {
	.family =		AF_INET,
	.solicit =		arp_solicit,
	.error_report =		arp_error_report,
	.output =		neigh_resolve_output,
	.connected_output =	neigh_resolve_output,
};
----
因此，不管neighbor是不是"已连接的"，或者邻居缓存代码是否怀疑连接"已关闭"，neigh_resolve_output最终都会被赋给neigh->output。当执行到n->output时就会调用它。

=== neigh_resolve_output()
int neigh_resolve_output(struct neighbour *neigh, struct sk_buff *skb):
https://elixir.bootlin.com/linux/latest/source/net/core/neighbour.c
最终也会调用dev_queue_xmit()。

=== 参考 
http://kerneltravel.net/blog/2020/network_ljr11/
http://kerneltravel.net/blog/2020/network_ljr12/
http://kerneltravel.net/blog/2020/network_ljr13/
http://kerneltravel.net/blog/2020/network_ljr14/
http://kerneltravel.net/blog/2020/network_ljr15/