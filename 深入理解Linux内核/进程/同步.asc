:toc:
:toclevels: 5
:hardbreaks-option:

== 同步

=== 概念
内核中可能造成并发执行的原因:

    中断
    内核抢占
    睡眠及调度
    多处理器

并发安全:

    线程与线程，线程与中断，中断与中断

锁类型:

    Sleeping locks
    CPU local locks
    Spinning locks
    https://github.com/torvalds/linux/blob/master/Documentation/locking/locktypes.rst

嵌套规则:
https://github.com/torvalds/linux/blob/master/Documentation/locking/locktypes.rst#lock-type-nesting-rules

=== 原子操作

==== 整数
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/types.h
----
typedef struct {
	int counter;
} atomic_t;

#define ATOMIC_INIT(i) { (i) }

#ifdef CONFIG_64BIT
typedef struct {
	s64 counter;
} atomic64_t;
#endif
----

操作: https://elixir.bootlin.com/linux/latest/source/include/asm-generic/atomic.h

x86:
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/atomic.h

ARM64:
https://elixir.bootlin.com/linux/latest/source/arch/arm64/include/asm/atomic.h
https://elixir.bootlin.com/linux/latest/source/arch/arm64/include/asm/atomic_ll_sc.h
https://elixir.bootlin.com/linux/latest/source/arch/arm64/include/asm/atomic_lse.h

参考:
https://stackoverflow.com/questions/36624881/why-is-integer-assignment-on-a-naturally-aligned-variable-atomic-on-x86
https://developer.arm.com/documentation/ddi0406/cb/Application-Level-Architecture/Application-Level-Memory-Model/Memory-types-and-attributes-and-the-memory-order-model/Atomicity-in-the-ARM-architecture

==== 位操作
x86:
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/bitops.h

ARM:
https://elixir.bootlin.com/linux/latest/source/arch/arm/include/asm/bitops.h

==== 参考
https://lwn.net/Kernel/Index/#Atomic_operations
https://lwn.net/Kernel/Index/#atomic_t
https://lwn.net/Kernel/Index/#C11_atomic_operations
https://lwn.net/Kernel/Index/#Atomic_spinlocks
https://lwn.net/Kernel/Index/#Atomic_IO_operations
https://lwn.net/Kernel/Index/#ACCESS_ONCE

=== barrier
==== 概念
考虑如下情形：
（1）编译器在编译程序的过程中，对代码会进行调整；
（2）CPU在执行指令的过程中，对指令会进行重排。

显然，有时候，这些优化并不符合我们的预期，那么如何防止这两种情况的发生呢？这就需要compiler barrier和memory barrier。

==== compiler barrier
===== barrier()与barrier_data(ptr)
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/compiler.h
----
/* Optimization barrier */
#ifndef barrier
/* The "volatile" is due to gcc bugs */
# define barrier() __asm__ __volatile__("": : :"memory")
#endif

#ifndef barrier_data
/*
 * This version is i.e. to prevent dead stores elimination on @ptr
 * where gcc and llvm may behave differently when otherwise using
 * normal barrier(): while gcc behavior gets along with a normal
 * barrier(), llvm needs an explicit input variable to be assumed
 * clobbered. The issue is as follows: while the inline asm might
 * access any memory it wants, the compiler could have fit all of
 * @ptr into memory registers instead, and since @ptr never escaped
 * from that, it proved that the inline asm wasn't touching any of
 * it. This version works well with both compilers, i.e. we're telling
 * the compiler that the inline asm absolutely may see the contents
 * of @ptr. See also: https://llvm.org/bugs/show_bug.cgi?id=15495
 */
# define barrier_data(ptr) __asm__ __volatile__("": :"r"(ptr) :"memory")
#endif
----

===== READ_WRITE_ONCE
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/asm-generic/rwonce.h
----
/*
 * Yes, this permits 64-bit accesses on 32-bit architectures. These will
 * actually be atomic in some cases (namely Armv7 + LPAE), but for others we
 * rely on the access being split into 2x32-bit accesses for a 32-bit quantity
 * (e.g. a virtual address) and a strong prevailing wind.
 */
#define compiletime_assert_rwonce_type(t)					\
	compiletime_assert(__native_word(t) || sizeof(t) == sizeof(long long),	\
		"Unsupported access size for {READ,WRITE}_ONCE().")

/*
 * Use __READ_ONCE() instead of READ_ONCE() if you do not require any
 * atomicity. Note that this may result in tears!
 */
#ifndef __READ_ONCE
#define __READ_ONCE(x)	(*(const volatile __unqual_scalar_typeof(x) *)&(x))
#endif

#define READ_ONCE(x)							\
({									\
	compiletime_assert_rwonce_type(x);				\
	__READ_ONCE(x);							\
})

#define __WRITE_ONCE(x, val)						\
do {									\
	*(volatile typeof(x) *)&(x) = (val);				\
} while (0)

#define WRITE_ONCE(x, val)						\
do {									\
	compiletime_assert_rwonce_type(x);				\
	__WRITE_ONCE(x, val);						\
} while (0)
----

Prevent the compiler from merging or refetching reads or writes. The compiler is also forbidden from reordering successive instances of READ_ONCE and WRITE_ONCE, but only when the compiler is aware of some particular ordering. One way to make the compiler aware of ordering is to put the two invocations of READ_ONCE or WRITE_ONCE in different C statements.

These two macros will also work on aggregate data types like structs or unions.

Their two major use cases are: (1) Mediating communication between process-level code and irq/NMI handlers, all running on the same CPU, and (2) Ensuring that the compiler does not fold, spindle, or otherwise mutilate accesses that either do not require ordering or that interact with an explicit memory barrier or atomic instruction that provides the required ordering.

参考: 
https://github.com/google/kernel-sanitizers/blob/master/other/READ_WRITE_ONCE.md
https://lwn.net/Kernel/Index/#ACCESS_ONCE
https://stackoverflow.com/questions/50589499/write-once-and-read-once-in-linux-kernel
https://www.usenix.org/legacy/event/hotpar11/tech/final_files/Boehm.pdf

==== memory order
CPU在什么情况下会reorder呢？

对于有前后有依赖的指令，CPU一般不会reorder(Alpha架构除外)。
例如: a = 5; b = a + 1; 这两条指令存在依赖关系，不会被cpu重排顺序。

对于没有前后依赖关系的指令，CPU就有可能对这些指令进行重排(除非使用memory barrier进行一些显示控制)，具体的力度则与CPU体系结构相关:

    x86是一种strong order(也叫TSO，total store order):
        同一CPU执行的load指令后接load指令(L-L)，store指令后接store指令(S-S)，load指令后接store指令(L-S):
            均不能交换指令的执行顺序
        仅store指令后接load指令(S-L)才可以

    ARM则是一种weak order:
        只要没有依赖关系，load指令和store指令就可任意交换。

==== memory barrier
memory barrier就像一个栅栏一样，隔开了在其前面和后面的指令。
"barrier"前面的指令不能与后面的指令进行调换；
如果指令均处在其前面，或者均处在其后面，只要没有违反当前CPU的memory order的规则，则是可以调换的。

memory barrier约束了CPU的行为，同时也约束了编译器的行为，即memory barrier也隐含了compiler barrier语义。

实现:
https://elixir.bootlin.com/linux/latest/source/include/asm-generic/barrier.h

▪ X86
[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/barrier.h
----
/*
 * Force strict CPU ordering.
 * And yes, this might be required on UP too when we're talking
 * to devices.
 */

#ifdef CONFIG_X86_32
#define mb() asm volatile(ALTERNATIVE("lock; addl $0,-4(%%esp)", "mfence", \
				      X86_FEATURE_XMM2) ::: "memory", "cc")
#define rmb() asm volatile(ALTERNATIVE("lock; addl $0,-4(%%esp)", "lfence", \
				       X86_FEATURE_XMM2) ::: "memory", "cc")
#define wmb() asm volatile(ALTERNATIVE("lock; addl $0,-4(%%esp)", "sfence", \
				       X86_FEATURE_XMM2) ::: "memory", "cc")
#else
#define __mb()	asm volatile("mfence":::"memory")
#define __rmb()	asm volatile("lfence":::"memory")
#define __wmb()	asm volatile("sfence" ::: "memory")
#endif

/**
 * array_index_mask_nospec() - generate a mask that is ~0UL when the
 * 	bounds check succeeds and 0 otherwise
 * @index: array element index
 * @size: number of elements in array
 *
 * Returns:
 *     0 - (index < size)
 */
static inline unsigned long array_index_mask_nospec(unsigned long index,
		unsigned long size)
{
	unsigned long mask;

	asm volatile ("cmp %1,%2; sbb %0,%0;"
			:"=r" (mask)
			:"g"(size),"r" (index)
			:"cc");
	return mask;
}

/* Override the default implementation from linux/nospec.h. */
#define array_index_mask_nospec array_index_mask_nospec

/* Prevent speculative execution past this barrier. */
#define barrier_nospec() alternative("", "lfence", X86_FEATURE_LFENCE_RDTSC)

#define __dma_rmb()	barrier()
#define __dma_wmb()	barrier()

#define __smp_mb()	asm volatile("lock; addl $0,-4(%%" _ASM_SP ")" ::: "memory", "cc")

#define __smp_rmb()	dma_rmb()
#define __smp_wmb()	barrier()
#define __smp_store_mb(var, value) do { (void)xchg(&var, value); } while (0)

#define __smp_store_release(p, v)					\
do {									\
	compiletime_assert_atomic_type(*p);				\
	barrier();							\
	WRITE_ONCE(*p, v);						\
} while (0)

#define __smp_load_acquire(p)						\
({									\
	typeof(*p) ___p1 = READ_ONCE(*p);				\
	compiletime_assert_atomic_type(*p);				\
	barrier();							\
	___p1;								\
})

/* Atomic operations are already serializing on x86 */
#define __smp_mb__before_atomic()	do { } while (0)
#define __smp_mb__after_atomic()	do { } while (0)

#include <asm-generic/barrier.h>

/*
 * Make previous memory operations globally visible before
 * a WRMSR.
 *
 * MFENCE makes writes visible, but only affects load/store
 * instructions.  WRMSR is unfortunately not a load/store
 * instruction and is unaffected by MFENCE.  The LFENCE ensures
 * that the WRMSR is not reordered.
 *
 * Most WRMSRs are full serializing instructions themselves and
 * do not require this barrier.  This is only required for the
 * IA32_TSC_DEADLINE and X2APIC MSRs.
 */
static inline void weak_wrmsr_fence(void)
{
	asm volatile("mfence; lfence" : : : "memory");
}
----

▪ ARM
[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/arm/include/asm/barrier.h
----
#if __LINUX_ARM_ARCH__ >= 7 ||		\
	(__LINUX_ARM_ARCH__ == 6 && defined(CONFIG_CPU_32v6K))
#define sev()	__asm__ __volatile__ ("sev" : : : "memory")
#define wfe()	__asm__ __volatile__ ("wfe" : : : "memory")
#define wfi()	__asm__ __volatile__ ("wfi" : : : "memory")
#else
#define wfe()	do { } while (0)
#endif

#if __LINUX_ARM_ARCH__ >= 7
#define isb(option) __asm__ __volatile__ ("isb " #option : : : "memory")
#define dsb(option) __asm__ __volatile__ ("dsb " #option : : : "memory")
#define dmb(option) __asm__ __volatile__ ("dmb " #option : : : "memory")
#ifdef CONFIG_THUMB2_KERNEL
#define CSDB	".inst.w 0xf3af8014"
#else
#define CSDB	".inst	0xe320f014"
#endif
#define csdb() __asm__ __volatile__(CSDB : : : "memory")
#elif defined(CONFIG_CPU_XSC3) || __LINUX_ARM_ARCH__ == 6
#define isb(x) __asm__ __volatile__ ("mcr p15, 0, %0, c7, c5, 4" \
				    : : "r" (0) : "memory")
#define dsb(x) __asm__ __volatile__ ("mcr p15, 0, %0, c7, c10, 4" \
				    : : "r" (0) : "memory")
#define dmb(x) __asm__ __volatile__ ("mcr p15, 0, %0, c7, c10, 5" \
				    : : "r" (0) : "memory")
#elif defined(CONFIG_CPU_FA526)
#define isb(x) __asm__ __volatile__ ("mcr p15, 0, %0, c7, c5, 4" \
				    : : "r" (0) : "memory")
#define dsb(x) __asm__ __volatile__ ("mcr p15, 0, %0, c7, c10, 4" \
				    : : "r" (0) : "memory")
#define dmb(x) __asm__ __volatile__ ("" : : : "memory")
#else
#define isb(x) __asm__ __volatile__ ("" : : : "memory")
#define dsb(x) __asm__ __volatile__ ("mcr p15, 0, %0, c7, c10, 4" \
				    : : "r" (0) : "memory")
#define dmb(x) __asm__ __volatile__ ("" : : : "memory")
#endif

#ifndef CSDB
#define CSDB
#endif
#ifndef csdb
#define csdb()
#endif

#ifdef CONFIG_ARM_HEAVY_MB
extern void (*soc_mb)(void);
extern void arm_heavy_mb(void);
#define __arm_heavy_mb(x...) do { dsb(x); arm_heavy_mb(); } while (0)
#else
#define __arm_heavy_mb(x...) dsb(x)
#endif

#if defined(CONFIG_ARM_DMA_MEM_BUFFERABLE) || defined(CONFIG_SMP)
#define mb()		__arm_heavy_mb()
#define rmb()		dsb()
#define wmb()		__arm_heavy_mb(st)
#define dma_rmb()	dmb(osh)
#define dma_wmb()	dmb(oshst)
#else
#define mb()		barrier()
#define rmb()		barrier()
#define wmb()		barrier()
#define dma_rmb()	barrier()
#define dma_wmb()	barrier()
#endif

#define __smp_mb()	dmb(ish)
#define __smp_rmb()	__smp_mb()
#define __smp_wmb()	dmb(ishst)

#ifdef CONFIG_CPU_SPECTRE
static inline unsigned long array_index_mask_nospec(unsigned long idx,
						    unsigned long sz)
{
	unsigned long mask;

	asm volatile(
		"cmp	%1, %2\n"
	"	sbc	%0, %1, %1\n"
	CSDB
	: "=r" (mask)
	: "r" (idx), "Ir" (sz)
	: "cc");

	return mask;
}
#define array_index_mask_nospec array_index_mask_nospec
#endif

#include <asm-generic/barrier.h>
----

==== 规范
Our memory model is highly constrained because it must match the kernel's behavior (or intended behavior). However, there are numerous choices to be made, so we formulated the following principles to guide those choices:

    1. Strength preferred to weakness.
    2. Simplicity preferred to complexity.
    3. Support existing non-buggy Linux-kernel code.
    4. Be compatible with hardware supported by the Linux kernel.
    5. Support future hardware, within reason.
    6. Be compatible with the C11 memory model, where prudent and reasonable.
    7. Expose questions and areas of uncertainty.

参考: https://lwn.net/Articles/718628/

==== 参考
https://github.com/torvalds/linux/blob/master/Documentation/memory-barriers.txt
https://github.com/torvalds/linux/tree/master/tools/memory-model
https://lwn.net/Kernel/Index/#Memory_barriers
https://lwn.net/Articles/718628/
https://lwn.net/Articles/720550/
Linux-Kernel Memory Model: https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4374.html
A Strong Formal Model of Linux-Kernel Memory Ordering: https://mirrors.edge.kernel.org/pub/linux/kernel/people/paulmck/LWNLinuxMM/StrongModel.html
A Weak Formal Model of Linux-Kernel Memory Ordering: https://mirrors.edge.kernel.org/pub/linux/kernel/people/paulmck/LWNLinuxMM/WeakModel.html
<<Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors>>: https://www.csl.cornell.edu/courses/ece5750/gharachorloo.isca90.pdf
https://developer.arm.com/documentation/100941/0101/Barriers
https://zhuanlan.zhihu.com/p/96001570
https://mp.weixin.qq.com/s?__biz=MzAxMDM0NjExNA==&mid=2247487950&idx=1&sn=c6cb416efc2831c5666a5ae9a205dcf3&chksm=9b509a23ac2713352a6998b9ff34f29b02cc57a0cfe62bcc13a8aa3583c5e78d2d038aacf17c&mpshare=1&scene=1&srcid=0827lWWERv8vkNak5sANCSAc&sharer_sharetime=1667631485956&sharer_shareid=3d769dc0d221507247ffa710c89a2772&exportkey=n_ChQIAhIQ8I4w6FfIvqlzaen82GNKVxKZAgIE97dBBAEAAAAAAFWOLeiWGmcAAAAOpnltbLcz9gKNyK89dVj0hC%2FkJy43MLPqIOkmygOzvgdnTEDr65XdDo6dZ7SwCefoAImebvkCfI61eSTYahvaCP06FEy0n7UJvX9u%2FYIz1JS9%2BCCA7d5%2FKyMAjXkuHHt86AbasiIxztU5VUIa9lMcGL9zwaT6JlgIkO%2FIk6YccnxLN2UKLxcaBqJJeySCIuPYpjDSxg56oh3IgIsMMvIhwN96Lxu3LjEzNZIhOyr%2B88hqFYIZTFtAul%2BgftiyMUHcrW3Jhlg8obgZBYXDR1BC%2BgtBQQhATMXarg893oVKs7fJNkN69Ckjn23rqVgibQjnuMaNadco8Fw2bkVzDaCUkU2Z&acctmode=0&pass_ticket=QA7RhEsMegR3%2FRIcRqL6%2FVpdMlpaht0VwPJpMgvvdoPO%2FeAeWFcoxw0fS41vu6Qx&wx_header=0#rd


=== 自旋锁
==== 概念
- 自旋锁用于保护短的代码段
- 试图获得一个被持有的自旋锁的线程，会一直进行忙循环等待
- 自旋锁不应被长时间地持有，持有自旋锁的时间建议小于两次上下文切换的时间
- 与引起睡眠的锁相比，自旋锁不会引起上下文切换，通常比睡眠锁效率高

- Q: 为什么要关闭抢占？
假设在单CPU上，task a获取了spinlock之后发生了中断，中断唤醒了优先级更高的task b，在中断返回现场的时候切换到task b，如果task b也需要获取该spinlock，此时就会发生死锁。因此，内核在获取spinlock的时候，需要禁止本CPU上的抢占。
如果a和b运行在不同的CPU上，因为a马上就会释放spin lock，所以b的spin状态很快就会得到解除，这种情况下就不会发生死锁了。

- Q: 为什么自旋锁保护的代码不能进入睡眠状态？
自旋锁睡眠时，由于当前CPU的睡眠以及当前CPU的禁止处理器抢占，系统运行时不正常的。
但:
如果存在多个CPU，那么其它CPU可以继续运行，并有可能唤醒睡眠了的自旋锁，此时自旋锁睡眠的CPU才会得以运行正常。
如果允许中断处理，那么中断的代码是可以正常运行的，但是中断通常不会唤醒睡眠的自旋锁，因此系统仍然运行不正常。

- Q: 什么情况下需要关闭中断？
当中断上下文中与task存在共享资源时，可能会发生死锁。例如某个任务获得了spinlock，此时发生中断，而中断里面也在获取该spinlock，死锁就发生了，此时尽管可以产生调度，但由于spinlock关闭了抢占，这个CPU就浪费了。

- 主要API
    * spin_lock()与spin_unlock()
		** 禁止内核抢占
        ** spin_lock()并不会禁止中断，当不会与中断处理程序共享资源时可以使用
        ** 系统的很多事件/输入来源于中断(例如最重要的时钟中断)，禁止中断会造成系统响应的延迟
    * spin_lock_irq()与spin_unlock_irq()
		** 禁止内核抢占并屏蔽中断
        ** 共享资源有可能被中断处理程序修改，此时就应该关闭中断
    * spin_lock_irqsave()与spin_unlock_irqrestore()
		** 禁止内核抢占并屏蔽中断，事先保存中断屏蔽位并事后恢复原状
    * spin_lock_bh()与spin_unlock_bh()
		** 禁止内核抢占并屏蔽软中断(SoftIRQ)

==== raw spinlock
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/spinlock_types.h
----
#ifndef CONFIG_PREEMPT_RT

/* Non PREEMPT_RT kernels map spinlock to raw_spinlock */
typedef struct spinlock {
	union {
		struct raw_spinlock rlock;

#ifdef CONFIG_DEBUG_LOCK_ALLOC
# define LOCK_PADSIZE (offsetof(struct raw_spinlock, dep_map))
		struct {
			u8 __padding[LOCK_PADSIZE];
			struct lockdep_map dep_map;
		};
#endif
	};
} spinlock_t;
//...
#else /* !CONFIG_PREEMPT_RT */

/* PREEMPT_RT kernels map spinlock to rt_mutex */
#include <linux/rtmutex.h>

typedef struct spinlock {
	struct rt_mutex_base	lock;
#ifdef CONFIG_DEBUG_LOCK_ALLOC
	struct lockdep_map	dep_map;
#endif
} spinlock_t;
#endif /* CONFIG_PREEMPT_RT */
----

[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/spinlock_types_raw.h
----
typedef struct raw_spinlock {
	arch_spinlock_t raw_lock;
#ifdef CONFIG_DEBUG_SPINLOCK
	unsigned int magic, owner_cpu;
	void *owner;
#endif
#ifdef CONFIG_DEBUG_LOCK_ALLOC
	struct lockdep_map dep_map;
#endif
} raw_spinlock_t;
----

spinlock的实现会区分单核、多核的区分，且多核是架构相关的，此外还涉及CONFIG_PREEMPT_RT等众多宏。
在没有定义CONFIG_PREEMPT_RT的情况下，由arch_spinlock_t实现。
在单核上，arch_spinlock_t基本等同于空结构体: https://elixir.bootlin.com/linux/latest/source/include/linux/spinlock_types_up.h

▪ x86与ARM64:
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/spinlock.h
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/spinlock_types.h

可见，x86通过:
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/qspinlock.h
最终通过:
https://elixir.bootlin.com/linux/latest/source/include/asm-generic/qspinlock.h

ARM64也是:
https://elixir.bootlin.com/linux/latest/source/arch/arm64/include/asm/spinlock.h
https://elixir.bootlin.com/linux/latest/source/arch/arm64/include/asm/spinlock_types.h
即通用排队自旋锁来实现自旋锁。

历史:
x86: since 4.2, https://lkml.iu.edu/hypermail/linux/kernel/1506.2/04205.html
ARM64: since 4.19

▪ 32位ARM:
https://elixir.bootlin.com/linux/latest/source/arch/arm/include/asm/spinlock_types.h
可见，其自旋锁是通过下文的tick spinlock来实现的。

参考:
https://github.com/0xAX/linux-insides/blob/master/SyncPrim/linux-sync-1.md#spinlocks-in-the-linux-kernel

==== ticket spinlock
▪ 历史与作用
since 2.6.25
主要解决公平性
之前的自旋锁有一个缺点: it is unfair.
lock unfairness can also create latency issues; it is hard to give latency guarantees when the wait time for a spinlock can be arbitrarily long.

▪ 主要思想:
使用u16 next和u16 owner两个变量
类似于叫号，当柜台叫到的号码(next变量)与手中的号码(owner变量)一致时，柜台就可以为之服务了。
这样，也就可以让CPU按照获取的顺序(即FIFO)获得所有权，由此形成了一种有序的竞争。

▪ 但也带来了新的问题:
只要spinlock的值被更改，所有试图获取spinlock的CPU对应的cache-line都会被invalidate，此时会一直重新从内存读取新的spinlock的值到自己的cache-line中。
然而，实时上只有队列中最先达到的CPU才可以获得所有权，只有它的cache-line才需要被invalidate，其它的CPU则都是在做无用功。
也就是说，在cache-line的问题上，tick spinlock存在效率问题。

▪ 32位ARM
其实对于ARM64, v3.13-v4.18也是采用ticket spinlock。
数据结构:
[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/arm/include/asm/spinlock_types.h
----
typedef struct {
	union {
		u32 slock;
		struct __raw_tickets {
#ifdef __ARMEB__
			u16 next;
			u16 owner;
#else
			u16 owner;
			u16 next;
#endif
		} tickets;
	};
} arch_spinlock_t;
----
实现: https://elixir.bootlin.com/linux/latest/source/arch/arm/include/asm/spinlock.h

参考:
https://lwn.net/Articles/267968/
https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=314cdbefd1fd0a7acf3780e9628465b77ea6a836
https://en.wikipedia.org/wiki/Ticket_lock
https://zhuanlan.zhihu.com/p/80727111

==== mcs spinlock
===== 概览
▪ 历史与作用
since 3.15, mcs是两位作者的简称
per-CPU structure, 在cache-line上效率更高:
The MCS lock (proposed by Mellor-Crummey and Scott) is a simple spin-lock with the desirable properties of being fair, and with each cpu trying to acquire the lock spinning on a local variable.It avoids expensive cache bounces that common test-and-set spin-lock implementations incur.
注意:
mcs spinlock目前的唯一使用场景是qspinlock。

▪ 主要思想:
真正拥有所有权的spinlock如果unlock了，只需要把所有权传递给next spinlock，从而解决了ticket spinlock在cache-line上颠簸的效率问题。

▪ 但也带来了新的问题:
mcs spinlock比4个字节的ticket spinlock多了一个指针(即多了4/8个字节)，也就是说空间变为原来的2倍或3倍。而spinlock是内核的基础结构，有时会被嵌入到一些对结构体大小十分敏感的结构体中(例如struct page)，毫无疑问spinlock空间上的增大，对这些结构影响是很大的，这也正是spinlock不(直接)使用mcs spinlock的原因。

===== 结构
[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/locking/mcs_spinlock.h
----
struct mcs_spinlock {
	struct mcs_spinlock *next;
	int locked; /* 1 if lock acquired */
	int count;  /* nesting count, see qspinlock.c */
};
----
struct mcs_spinlock *next: 指向下一个锁的申请者，构成串行的等待队列的链表
int locked: 1表示持有锁
int count: 嵌套的计数

===== 操作
[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/locking/mcs_spinlock.h
----
#ifndef arch_mcs_spin_lock_contended
/*
 * Using smp_cond_load_acquire() provides the acquire semantics
 * required so that subsequent operations happen after the
 * lock is acquired. Additionally, some architectures such as
 * ARM64 would like to do spin-waiting instead of purely
 * spinning, and smp_cond_load_acquire() provides that behavior.
 */
#define arch_mcs_spin_lock_contended(l)					\
do {									\
	smp_cond_load_acquire(l, VAL);					\
} while (0)
#endif

#ifndef arch_mcs_spin_unlock_contended
/*
 * smp_store_release() provides a memory barrier to ensure all
 * operations in the critical section has been completed before
 * unlocking.
 */
#define arch_mcs_spin_unlock_contended(l)				\
	smp_store_release((l), 1)
#endif

/*
 * Note: the smp_load_acquire/smp_store_release pair is not
 * sufficient to form a full memory barrier across
 * cpus for many architectures (except x86) for mcs_unlock and mcs_lock.
 * For applications that need a full barrier across multiple cpus
 * with mcs_unlock and mcs_lock pair, smp_mb__after_unlock_lock() should be
 * used after mcs_lock.
 */

/*
 * In order to acquire the lock, the caller should declare a local node and
 * pass a reference of the node to this function in addition to the lock.
 * If the lock has already been acquired, then this will proceed to spin
 * on this node->locked until the previous lock holder sets the node->locked
 * in mcs_spin_unlock().
 */
static inline
void mcs_spin_lock(struct mcs_spinlock **lock, struct mcs_spinlock *node)
{
	struct mcs_spinlock *prev;

	/* Init node */
	node->locked = 0;
	node->next   = NULL;

	/*
	 * We rely on the full barrier with global transitivity implied by the
	 * below xchg() to order the initialization stores above against any
	 * observation of @node. And to provide the ACQUIRE ordering associated
	 * with a LOCK primitive.
	 */
	prev = xchg(lock, node);
	if (likely(prev == NULL)) {
		/*
		 * Lock acquired, don't need to set node->locked to 1. Threads
		 * only spin on its own node->locked value for lock acquisition.
		 * However, since this thread can immediately acquire the lock
		 * and does not proceed to spin on its own node->locked, this
		 * value won't be used. If a debug mode is needed to
		 * audit lock status, then set node->locked value here.
		 */
		return;
	}
	WRITE_ONCE(prev->next, node);

	/* Wait until the lock holder passes the lock down. */
	arch_mcs_spin_lock_contended(&node->locked);
}

/*
 * Releases the lock. The caller should pass in the corresponding node that
 * was used to acquire the lock.
 */
static inline
void mcs_spin_unlock(struct mcs_spinlock **lock, struct mcs_spinlock *node)
{
	struct mcs_spinlock *next = READ_ONCE(node->next);

	if (likely(!next)) {
		/*
		 * Release the lock by setting it to NULL
		 */
		if (likely(cmpxchg_release(lock, node, NULL) == node))
			return;
		/* Wait until the next pointer is set */
		while (!(next = READ_ONCE(node->next)))
			cpu_relax();
	}

	/* Pass lock to next waiter. */
	arch_mcs_spin_unlock_contended(&next->locked);
}
----

每个CPU使用原子指令将自己的锁结构的地址与MCS Lock的next指针交换来获取锁，因此MCS Lock的next指针始终指向等待锁队列的最后一个。
每个CPU在自己的locked值上自旋，从而避免绝大部分的cache颠簸。

===== 参考
https://lwn.net/Articles/590243/
https://blog.csdn.net/bemind1/article/details/118224344
https://bugzilla.kernel.org/show_bug.cgi?id=206115

==== queue spinlock
===== 概览
▪ 历史与作用
since 4.2
基于mcs spinlock的思想但解决了mcs spinlock占用空间大的问题。

▪ 主要思想:
与传统的mcs相比，qspinlock通过存储队尾mcs节点的CPU号而不直接存储节点的地址将spinlock的结构大小压缩到了32bit。

▪ 参考:
https://lwn.net/Articles/590243/
https://lkml.org/lkml/2015/4/24/631

===== 结构
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/asm-generic/qspinlock_types.h
----
typedef struct qspinlock {
	union {
		atomic_t val;

		/*
		 * By using the whole 2nd least significant byte for the
		 * pending bit, we can allow better optimization of the lock
		 * acquisition for the pending bit holder.
		 */
#ifdef __LITTLE_ENDIAN
		struct {
			u8	locked;
			u8	pending;
		};
		struct {
			u16	locked_pending;
			u16	tail;
		};
#else
		struct {
			u16	tail;
			u16	locked_pending;
		};
		struct {
			u8	reserved[2];
			u8	pending;
			u8	locked;
		};
#endif
	};
} arch_spinlock_t;
//...
/*
 * Bitfields in the atomic value:
 *
 * When NR_CPUS < 16K
 *  0- 7: locked byte
 *     8: pending
 *  9-15: not used
 * 16-17: tail index
 * 18-31: tail cpu (+1)
 *
 * When NR_CPUS >= 16K
 *  0- 7: locked byte
 *     8: pending
 *  9-10: tail index
 * 11-31: tail cpu (+1)
 */
----
struct qspinlock为4个字节的联合体:

    atomic_t val: 整个32bit值
    u8	locked: 表示是否加锁，0表示未加锁，其余值表示已加锁
    u8	pending: 第一个等待锁的CPU需要先设置pending位，后续等待锁的CPU则全部进入MCS spinlock队列自旋等待。
    u16	locked_pending: 8个bit的locked + 1个bit的pending
    u16	tail:
        2bit: qnodes[MAX_NODES]的idx(即tail index)
        14bit(NR_CPUS < 16K)/21bit(NR_CPUS >= 16K): CPU idx+1(即tail cpu)
        Q: 为什么这里是tail呢?
		A: 这是由mcs_spinlock的性质决定的?
        Q: 为什么这里cpu idx需要加1?
		A: 如果不加1，便不能区分0号CPU与等待队列中没有成员的情况。
    __LITTLE_ENDIAN宏: 表示小端

[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/locking/qspinlock.c
----
#include "mcs_spinlock.h"
#define MAX_NODES	4

/*
 * On 64-bit architectures, the mcs_spinlock structure will be 16 bytes in
 * size and four of them will fit nicely in one 64-byte cacheline. For
 * pvqspinlock, however, we need more space for extra data. To accommodate
 * that, we insert two more long words to pad it up to 32 bytes. IOW, only
 * two of them can fit in a cacheline in this case. That is OK as it is rare
 * to have more than 2 levels of slowpath nesting in actual use. We don't
 * want to penalize pvqspinlocks to optimize for a rare case in native
 * qspinlocks.
 */
struct qnode {
	struct mcs_spinlock mcs;
#ifdef CONFIG_PARAVIRT_SPINLOCKS
	long reserved[2];
#endif
};
//...
/*
 * Per-CPU queue node structures; we can never have more than 4 nested
 * contexts: task, softirq, hardirq, nmi.
 *
 * Exactly fits one 64-byte cacheline on a 64-bit architecture.
 *
 * PV doubles the storage and uses the second cacheline for PV state.
 */
static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);
----
每个qnode就是一个mcs_spinlock。
每个CPU上有一个qnodes数组，该数组拥有4个qnode元素，分别用在task, softirq, hardirq, nmi上下文。

===== 操作
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/asm-generic/qspinlock.h
----
static __always_inline void queued_spin_lock(struct qspinlock *lock)
{
	int val = 0;

	if (likely(atomic_try_cmpxchg_acquire(&lock->val, &val, _Q_LOCKED_VAL)))
		return;

	queued_spin_lock_slowpath(lock, val);
}
//...
static __always_inline void queued_spin_unlock(struct qspinlock *lock)
{
	/*
	 * unlock() needs release semantics:
	 */
	smp_store_release(&lock->locked, 0);
}
----

[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/locking/qspinlock.c
----
/**
 * queued_spin_lock_slowpath - acquire the queued spinlock
 * @lock: Pointer to queued spinlock structure
 * @val: Current value of the queued spinlock 32-bit word
 *
 * (queue tail, pending bit, lock value)
 *
 *              fast     :    slow                                  :    unlock
 *                       :                                          :
 * uncontended  (0,0,0) -:--> (0,0,1) ------------------------------:--> (*,*,0)
 *                       :       | ^--------.------.             /  :
 *                       :       v           \      \            |  :
 * pending               :    (0,1,1) +--> (0,1,0)   \           |  :
 *                       :       | ^--'              |           |  :
 *                       :       v                   |           |  :
 * uncontended           :    (n,x,y) +--> (n,0,0) --'           |  :
 *   queue               :       | ^--'                          |  :
 *                       :       v                               |  :
 * contended             :    (*,x,y) +--> (*,0,0) ---> (*,0,1) -'  :
 *   queue               :         ^--'                             :
 */
void __lockfunc queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
{
	struct mcs_spinlock *prev, *next, *node;
	u32 old, tail;
	int idx;

	BUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));

	if (pv_enabled())
		goto pv_queue;

	if (virt_spin_lock(lock))
		return;

	/*
	 * Wait for in-progress pending->locked hand-overs with a bounded
	 * number of spins so that we guarantee forward progress.
	 *
	 * 0,1,0 -> 0,0,1
	 */
	if (val == _Q_PENDING_VAL) {
		int cnt = _Q_PENDING_LOOPS;
		val = atomic_cond_read_relaxed(&lock->val,
					       (VAL != _Q_PENDING_VAL) || !cnt--);
	}

	/*
	 * If we observe any contention; queue.
	 */
	if (val & ~_Q_LOCKED_MASK)
		goto queue;

	/*
	 * trylock || pending
	 *
	 * 0,0,* -> 0,1,* -> 0,0,1 pending, trylock
	 */
	val = queued_fetch_set_pending_acquire(lock);

	/*
	 * If we observe contention, there is a concurrent locker.
	 *
	 * Undo and queue; our setting of PENDING might have made the
	 * n,0,0 -> 0,0,0 transition fail and it will now be waiting
	 * on @next to become !NULL.
	 */
	if (unlikely(val & ~_Q_LOCKED_MASK)) {

		/* Undo PENDING if we set it. */
		if (!(val & _Q_PENDING_MASK))
			clear_pending(lock);

		goto queue;
	}

	/*
	 * We're pending, wait for the owner to go away.
	 *
	 * 0,1,1 -> *,1,0
	 *
	 * this wait loop must be a load-acquire such that we match the
	 * store-release that clears the locked bit and create lock
	 * sequentiality; this is because not all
	 * clear_pending_set_locked() implementations imply full
	 * barriers.
	 */
	if (val & _Q_LOCKED_MASK)
		smp_cond_load_acquire(&lock->locked, !VAL);

	/*
	 * take ownership and clear the pending bit.
	 *
	 * 0,1,0 -> 0,0,1
	 */
	clear_pending_set_locked(lock);
	lockevent_inc(lock_pending);
	return;

	/*
	 * End of pending bit optimistic spinning and beginning of MCS
	 * queuing.
	 */
queue:
	lockevent_inc(lock_slowpath);
pv_queue:
	node = this_cpu_ptr(&qnodes[0].mcs);
	idx = node->count++;
	tail = encode_tail(smp_processor_id(), idx);

	trace_contention_begin(lock, LCB_F_SPIN);

	/*
	 * 4 nodes are allocated based on the assumption that there will
	 * not be nested NMIs taking spinlocks. That may not be true in
	 * some architectures even though the chance of needing more than
	 * 4 nodes will still be extremely unlikely. When that happens,
	 * we fall back to spinning on the lock directly without using
	 * any MCS node. This is not the most elegant solution, but is
	 * simple enough.
	 */
	if (unlikely(idx >= MAX_NODES)) {
		lockevent_inc(lock_no_node);
		while (!queued_spin_trylock(lock))
			cpu_relax();
		goto release;
	}

	node = grab_mcs_node(node, idx);

	/*
	 * Keep counts of non-zero index values:
	 */
	lockevent_cond_inc(lock_use_node2 + idx - 1, idx);

	/*
	 * Ensure that we increment the head node->count before initialising
	 * the actual node. If the compiler is kind enough to reorder these
	 * stores, then an IRQ could overwrite our assignments.
	 */
	barrier();

	node->locked = 0;
	node->next = NULL;
	pv_init_node(node);

	/*
	 * We touched a (possibly) cold cacheline in the per-cpu queue node;
	 * attempt the trylock once more in the hope someone let go while we
	 * weren't watching.
	 */
	if (queued_spin_trylock(lock))
		goto release;

	/*
	 * Ensure that the initialisation of @node is complete before we
	 * publish the updated tail via xchg_tail() and potentially link
	 * @node into the waitqueue via WRITE_ONCE(prev->next, node) below.
	 */
	smp_wmb();

	/*
	 * Publish the updated tail.
	 * We have already touched the queueing cacheline; don't bother with
	 * pending stuff.
	 *
	 * p,*,* -> n,*,*
	 */
	old = xchg_tail(lock, tail);
	next = NULL;

	/*
	 * if there was a previous node; link it and wait until reaching the
	 * head of the waitqueue.
	 */
	if (old & _Q_TAIL_MASK) {
		prev = decode_tail(old);

		/* Link @node into the waitqueue. */
		WRITE_ONCE(prev->next, node);

		pv_wait_node(node, prev);
		arch_mcs_spin_lock_contended(&node->locked);

		/*
		 * While waiting for the MCS lock, the next pointer may have
		 * been set by another lock waiter. We optimistically load
		 * the next pointer & prefetch the cacheline for writing
		 * to reduce latency in the upcoming MCS unlock operation.
		 */
		next = READ_ONCE(node->next);
		if (next)
			prefetchw(next);
	}

	/*
	 * we're at the head of the waitqueue, wait for the owner & pending to
	 * go away.
	 *
	 * *,x,y -> *,0,0
	 *
	 * this wait loop must use a load-acquire such that we match the
	 * store-release that clears the locked bit and create lock
	 * sequentiality; this is because the set_locked() function below
	 * does not imply a full barrier.
	 *
	 * The PV pv_wait_head_or_lock function, if active, will acquire
	 * the lock and return a non-zero value. So we have to skip the
	 * atomic_cond_read_acquire() call. As the next PV queue head hasn't
	 * been designated yet, there is no way for the locked value to become
	 * _Q_SLOW_VAL. So both the set_locked() and the
	 * atomic_cmpxchg_relaxed() calls will be safe.
	 *
	 * If PV isn't active, 0 will be returned instead.
	 *
	 */
	if ((val = pv_wait_head_or_lock(lock, node)))
		goto locked;

	val = atomic_cond_read_acquire(&lock->val, !(VAL & _Q_LOCKED_PENDING_MASK));

locked:
	/*
	 * claim the lock:
	 *
	 * n,0,0 -> 0,0,1 : lock, uncontended
	 * *,*,0 -> *,*,1 : lock, contended
	 *
	 * If the queue head is the only one in the queue (lock value == tail)
	 * and nobody is pending, clear the tail code and grab the lock.
	 * Otherwise, we only need to grab the lock.
	 */

	/*
	 * In the PV case we might already have _Q_LOCKED_VAL set, because
	 * of lock stealing; therefore we must also allow:
	 *
	 * n,0,1 -> 0,0,1
	 *
	 * Note: at this point: (val & _Q_PENDING_MASK) == 0, because of the
	 *       above wait condition, therefore any concurrent setting of
	 *       PENDING will make the uncontended transition fail.
	 */
	if ((val & _Q_TAIL_MASK) == tail) {
		if (atomic_try_cmpxchg_relaxed(&lock->val, &val, _Q_LOCKED_VAL))
			goto release; /* No contention */
	}

	/*
	 * Either somebody is queued behind us or _Q_PENDING_VAL got set
	 * which will then detect the remaining tail and queue behind us
	 * ensuring we'll see a @next.
	 */
	set_locked(lock);

	/*
	 * contended path; wait for next if not observed yet, release.
	 */
	if (!next)
		next = smp_cond_load_relaxed(&node->next, (VAL));

	arch_mcs_spin_unlock_contended(&next->locked);
	pv_kick_node(lock, next);

release:
	trace_contention_end(lock, 0);

	/*
	 * release the node
	 */
	__this_cpu_dec(qnodes[0].mcs.count);
}
----

===== PV support
para-virtualization support to the qspinlock
https://elixir.bootlin.com/linux/latest/source/kernel/locking/qspinlock_paravirt.h

CONFIG_PARAVIRT_SPINLOCKS:
https://cateee.net/lkddb/web-lkddb/PARAVIRT_SPINLOCKS.html

参考:
https://lwn.net/Articles/641792/

===== 参考
https://lwn.net/Articles/561775/
https://bugzilla.kernel.org/show_bug.cgi?id=206115
https://blog.csdn.net/bemind1/article/details/118224344
http://www.wowotech.net/kernel_synchronization/queued_spinlock.html

==== 参考
https://github.com/torvalds/linux/blob/master/Documentation/locking/spinlocks.rst
https://lwn.net/Kernel/Index/#Spinlocks

=== RCU
==== 概念
since 2.5.44

主要适用于下面的场景:

    ▪ RCU只能保护动态分配的数据结构, 并且必须是通过指针访问该数据结构
    ▪ 受RCU保护的临界区内不能sleep(SRCU?)
    ▪ writer对性能没有特别要求, reader对性能较高
    ▪ reader对新旧数据不敏感

==== 参考
https://github.com/torvalds/linux/tree/master/Documentation/RCU
https://www.kernel.org/doc/html/latest/RCU/index.html
https://lwn.net/Kernel/Index/#Read-copy-update
https://lwn.net/Archives/GuestIndex/#McKenney_Paul_E.
https://docs.google.com/document/d/1X0lThx8OK0ZgLMqVoXiR4ZrGURHrXK6NyLRbeXe3Xac/
https://hackmd.io/@sysprog/linux-rcu?type=view

=== 读写锁

==== rwlock
也叫读写自旋锁
写饥饿问题
现在的内核开发已经不建议再使用rwlock了, 之前使用到的rwlock也在逐渐被移除或者替换为普通的spinlock或者RCU

https://elixir.bootlin.com/linux/latest/source/include/linux/rwlock.h

==== 排队读写锁
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/asm-generic/qrwlock_types.h
----
typedef struct qrwlock {
	union {
		atomic_t cnts;
		struct {
#ifdef __LITTLE_ENDIAN
			u8 wlocked;	/* Locked for write? */
			u8 __lstate[3];
#else
			u8 __lstate[3];
			u8 wlocked;	/* Locked for write? */
#endif
		};
	};
	arch_spinlock_t		wait_lock;
} arch_rwlock_t;

#define	__ARCH_RW_LOCK_UNLOCKED {		\
	{ .cnts = ATOMIC_INIT(0), },		\
	.wait_lock = __ARCH_SPIN_LOCK_UNLOCKED,	\
}
----

==== Seqlock顺序锁
▪ 历史与作用:
v2.6引入, 对写友好, 写总能成功。

▪ 主要思想:
sequence number的初始值是一个偶数。
writer持有spinlock时，sequence number的值将是一个奇数(sequence number+1), 释放后则又变成偶数(sequence number+1)。
reader在读取一个共享变量之前, 需要先读取一下sequence number的值，如果为奇数，说明有writer正在修改这个变量，需要等待，直到sequence number变为偶数，才可以开始读取变量。
reader可以随时读, 但可能需要多读几次。
writer只会被其他writer造成饥饿，不再被reader造成饥饿。

▪ 实现:
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/seqlock.h
----
/*
 * Sequential locks (seqlock_t)
 *
 * Sequence counters with an embedded spinlock for writer serialization
 * and non-preemptibility.
 *
 * For more info, see:
 *    - Comments on top of seqcount_t
 *    - Documentation/locking/seqlock.rst
 */
typedef struct {
	/*
	 * Make sure that readers don't starve writers on PREEMPT_RT: use
	 * seqcount_spinlock_t instead of seqcount_t. Check __SEQ_LOCK().
	 */
	seqcount_spinlock_t seqcount;
	spinlock_t lock;
} seqlock_t;
----

▪ 参考:
https://github.com/torvalds/linux/blob/master/Documentation/locking/seqlock.rst

=== 信号量
==== 概念
信号量是一种睡眠锁，比自旋锁开销要大，适合需要长期持锁的场景。

注意：
信号量越来越少用了: https://lwn.net/Articles/928026/[The shrinking role of semaphores]

vs. mutex
https://lwn.net/ml/linux-kernel/20230331034209.GA12892@google.com/

==== 实现
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/semaphore.h
----
/* Please don't access any members of this structure directly */
struct semaphore {
	raw_spinlock_t		lock;
	unsigned int		count;
	struct list_head	wait_list;
};
//...
extern void down(struct semaphore *sem);
extern int __must_check down_interruptible(struct semaphore *sem);
extern int __must_check down_killable(struct semaphore *sem);
extern int __must_check down_trylock(struct semaphore *sem);
extern int __must_check down_timeout(struct semaphore *sem, long jiffies);
extern void up(struct semaphore *sem);
----

void down(struct semaphore *sem): acquire the semaphore
void up(struct semaphore *sem): release the semaphore

[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/locking/semaphore.c
----
/**
 * down - acquire the semaphore
 * @sem: the semaphore to be acquired
 *
 * Acquires the semaphore.  If no more tasks are allowed to acquire the
 * semaphore, calling this function will put the task to sleep until the
 * semaphore is released.
 *
 * Use of this function is deprecated, please use down_interruptible() or
 * down_killable() instead.
 */
void __sched down(struct semaphore *sem)
{
	unsigned long flags;

	might_sleep();
	raw_spin_lock_irqsave(&sem->lock, flags);
	if (likely(sem->count > 0))
		sem->count--;
	else
		__down(sem);
	raw_spin_unlock_irqrestore(&sem->lock, flags);
}
//...
/**
 * up - release the semaphore
 * @sem: the semaphore to release
 *
 * Release the semaphore.  Unlike mutexes, up() may be called from any
 * context and even by tasks which have never called down().
 */
void __sched up(struct semaphore *sem)
{
	unsigned long flags;

	raw_spin_lock_irqsave(&sem->lock, flags);
	if (likely(list_empty(&sem->wait_list)))
		sem->count++;
	else
		__up(sem);
	raw_spin_unlock_irqrestore(&sem->lock, flags);
}
----

==== 参考
https://lwn.net/Kernel/Index/#Semaphores

=== 互斥锁
==== 概念
当无法获得锁的时候，spinlock原地自旋，mutex挂起当前线程，进入阻塞状态。因此，mutex无法在中断上下文中使用。

互斥锁可以看作是0-1信号量。

==== 经典互斥锁
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/mutex.h
----
#ifndef CONFIG_PREEMPT_RT

/*
 * Simple, straightforward mutexes with strict semantics:
 *
 * - only one task can hold the mutex at a time
 * - only the owner can unlock the mutex
 * - multiple unlocks are not permitted
 * - recursive locking is not permitted
 * - a mutex object must be initialized via the API
 * - a mutex object must not be initialized via memset or copying
 * - task may not exit with mutex held
 * - memory areas where held locks reside must not be freed
 * - held mutexes must not be reinitialized
 * - mutexes may not be used in hardware or software interrupt
 *   contexts such as tasklets and timers
 *
 * These semantics are fully enforced when DEBUG_MUTEXES is
 * enabled. Furthermore, besides enforcing the above rules, the mutex
 * debugging code also implements a number of additional features
 * that make lock debugging easier and faster:
 *
 * - uses symbolic names of mutexes, whenever they are printed in debug output
 * - point-of-acquire tracking, symbolic lookup of function names
 * - list of all locks held in the system, printout of them
 * - owner tracking
 * - detects self-recursing locks and prints out all relevant info
 * - detects multi-task circular deadlocks and prints out all affected
 *   locks and tasks (and only those tasks)
 */
struct mutex {
	atomic_long_t		owner;
	raw_spinlock_t		wait_lock;
#ifdef CONFIG_MUTEX_SPIN_ON_OWNER
	struct optimistic_spin_queue osq; /* Spinner MCS lock */
#endif
	struct list_head	wait_list;
#ifdef CONFIG_DEBUG_MUTEXES
	void			*magic;
#endif
#ifdef CONFIG_DEBUG_LOCK_ALLOC
	struct lockdep_map	dep_map;
#endif
};
//...
#else /* !CONFIG_PREEMPT_RT */
/*
 * Preempt-RT variant based on rtmutexes.
 */
#include <linux/rtmutex.h>

struct mutex {
	struct rt_mutex_base	rtmutex;
#ifdef CONFIG_DEBUG_LOCK_ALLOC
	struct lockdep_map	dep_map;
#endif
};
//...
#endif /* CONFIG_PREEMPT_RT */
----

==== 实时互斥锁
===== 概念与原理
- 概念
since 5.15
配置: CONFIG_RT_MUTEXES=y
实时互斥锁实现了优先级继承(priority inheritance)，解决了优先级反转(priority inversion)的问题。

- 什么是优先级反转?
低优先级的task获取到了CPU，称为优先级反转：
例如A，B，C三个task，优先级从高到低，当前运行C任务，A由于要获取C的某项资源(mutex)，由于资源被占用，此时A会进入睡眠等待资源释放，比A优先级低比C优先级高的B不需要获取资源，抢占了C，也抢占了优先级更高的A，获得了CPU。

- 如何解决优先级反转的问题？
如果高优先级的任务阻塞在互斥量上，同时互斥量被低优先级的任务拥有，那么该低优先级的任务的动态优先级会提升到阻塞队列中的优先级最高之任务的优先级。
在上面的例子中，此时，C的动态优先级会提升到A，B就不再能够抢占C了。

===== 实现
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/rtmutex.h
----
struct rt_mutex_base {
	raw_spinlock_t		wait_lock;
	struct rb_root_cached   waiters;
	struct task_struct	*owner;
};
----
与经典互斥锁mutex不同的是，实时互斥锁的睡眠等待进程列表(waiters)是按优先级排序的struct rb_root_cached, 经典互斥锁则是struct list_head。

==== 参考
https://github.com/torvalds/linux/blob/master/Documentation/locking/mutex-design.rst
https://github.com/torvalds/linux/blob/master/Documentation/locking/rt-mutex.rst
https://github.com/torvalds/linux/blob/master/Documentation/locking/rt-mutex-design.rst

=== 大内核锁
BKL(Big Kernel Lock)从v2.6.39起被淘汰

实现:
__lock_kernel(), __unlock_kernel():
https://elixir.bootlin.com/linux/v2.6.38/source/lib/kernel_lock.c

参考:
https://lwn.net/Kernel/Index/#Big_kernel_lock
https://en.wikipedia.org/wiki/Giant_lock

=== per-cpu变量

==== 概念
per-cpu变量可以静态分配(分为内核中的静态percpu变量和模块中的静态percpu变量)，也可以动态分配。

==== 数据结构
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/percpu.h
----
struct pcpu_group_info {
	int			nr_units;	/* aligned # of units */
	unsigned long		base_offset;	/* base address offset */
	unsigned int		*cpu_map;	/* unit->cpu map, empty
						 * entries contain NR_CPUS */
};

struct pcpu_alloc_info {
	size_t			static_size;
	size_t			reserved_size;
	size_t			dyn_size;
	size_t			unit_size;
	size_t			atom_size;
	size_t			alloc_size;
	size_t			__ai_size;	/* internal, don't use */
	int			nr_groups;	/* 0 if grouping unnecessary */
	struct pcpu_group_info	groups[];
};
----

[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/percpu-internal.h
----
/*
 * pcpu_block_md is the metadata block struct.
 * Each chunk's bitmap is split into a number of full blocks.
 * All units are in terms of bits.
 *
 * The scan hint is the largest known contiguous area before the contig hint.
 * It is not necessarily the actual largest contig hint though.  There is an
 * invariant that the scan_hint_start > contig_hint_start iff
 * scan_hint == contig_hint.  This is necessary because when scanning forward,
 * we don't know if a new contig hint would be better than the current one.
 */
struct pcpu_block_md {
	int			scan_hint;	/* scan hint for block */
	int			scan_hint_start; /* block relative starting
						    position of the scan hint */
	int                     contig_hint;    /* contig hint for block */
	int                     contig_hint_start; /* block relative starting
						      position of the contig hint */
	int                     left_free;      /* size of free space along
						   the left side of the block */
	int                     right_free;     /* size of free space along
						   the right side of the block */
	int                     first_free;     /* block position of first free */
	int			nr_bits;	/* total bits responsible for */
};

struct pcpu_chunk {
#ifdef CONFIG_PERCPU_STATS
	int			nr_alloc;	/* # of allocations */
	size_t			max_alloc_size; /* largest allocation size */
#endif

	struct list_head	list;		/* linked to pcpu_slot lists */
	int			free_bytes;	/* free bytes in the chunk */
	struct pcpu_block_md	chunk_md;
	void			*base_addr;	/* base address of this chunk */

	unsigned long		*alloc_map;	/* allocation map */
	unsigned long		*bound_map;	/* boundary map */
	struct pcpu_block_md	*md_blocks;	/* metadata blocks */

	void			*data;		/* chunk data */
	bool			immutable;	/* no [de]population allowed */
	bool			isolated;	/* isolated from active chunk
						   slots */
	int			start_offset;	/* the overlap with the previous
						   region to have a page aligned
						   base_addr */
	int			end_offset;	/* additional area required to
						   have the region end page
						   aligned */
#ifdef CONFIG_MEMCG_KMEM
	struct obj_cgroup	**obj_cgroups;	/* vector of object cgroups */
#endif

	int			nr_pages;	/* # of pages served by this chunk */
	int			nr_populated;	/* # of populated pages */
	int                     nr_empty_pop_pages; /* # of empty populated pages */
	unsigned long		populated[];	/* populated bitmap */
};
----

==== 内核静态per-cpu变量

==== 模块静态per-cpu变量

==== 动态per-cpu变量

==== 注意事项
Q: per-cpu变量存储在哪里？
A: https://stackoverflow.com/questions/53968755/where-are-declare-per-cpu-variables-stored-in-kernel

https://stackoverflow.com/questions/16978959/how-are-percpu-pointers-implemented-in-the-linux-kernel

https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/percpu.h

==== 参考
https://lwn.net/Kernel/Index/#Per-CPU_variables
https://zhuanlan.zhihu.com/p/260986194

=== 锁与抢占
https://github.com/torvalds/linux/blob/master/Documentation/locking/preempt-locking.rst

=== futex
==== 概念

==== 实现
https://elixir.bootlin.com/linux/latest/source/kernel/futex/futex.h

==== 参考
https://github.com/torvalds/linux/blob/master/Documentation/locking/futex-requeue-pi.rst
https://github.com/torvalds/linux/blob/master/Documentation/locking/pi-futex.rst
https://github.com/torvalds/linux/blob/master/Documentation/locking/robust-futex-ABI.rst
https://github.com/torvalds/linux/blob/master/Documentation/locking/robust-futexes.rst

=== volatile
https://github.com/torvalds/linux/blob/master/Documentation/process/volatile-considered-harmful.rst

Quick Quiz 1: Can't the compiler also reorder these accesses?
Answer: Given the current Linux-kernel definitions of READ_ONCE() and WRITE_ONCE(), no. These two macros map to volatile accesses, which the compiler is not allowed to reorder with respect to each other.
However, if these macros instead mapped to non-volatile C11 memory_order_relaxed loads and stores, then the compiler would be permitted to reorder them. And, as a general rule, compilers are much more aggressive about reordering accesses than even the most weakly ordered hardware. In both cases, those who don't like such code rearrangement call it “weak ordering” while those who do call it “optimization”.
参考: https://lwn.net/Articles/720550/

参考: https://lwn.net/Kernel/Index/#volatile

=== 参考
https://github.com/torvalds/linux/tree/master/Documentation/locking
https://docs.kernel.org/locking/index.html
https://lwn.net/Kernel/Index/#Locking_mechanisms
https://lwn.net/Kernel/Index/#lock_kernel
https://lwn.net/Kernel/Index/#Lockless_algorithms
https://lwn.net/Kernel/Index/#Race_conditions
https://github.com/freelancer-leon/notes#%E9%94%81
https://github.com/0xAX/linux-insides/tree/master/SyncPrim