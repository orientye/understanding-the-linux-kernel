:toc:
:toclevels: 5
:hardbreaks-option:

== 同步
=== 概念
- 内核中可能造成并发执行的原因

    中断
    内核抢占
    睡眠及调度
    多处理器

- 并发安全

    线程与线程，线程与中断，中断与中断

=== 锁类型及其规则
==== 介绍
Sleeping locks(睡眠锁)
CPU local locks(CPU本地锁)
Spinning locks(自旋锁)
参考: https://github.com/torvalds/linux/blob/master/Documentation/locking/locktypes.rst#introduction

==== Sleeping locks(睡眠锁)
睡眠锁只能在可抢占任务上下文中获取(can only be acquired in preemptible task context)。

尽管实现允许从其它上下文调用 try_lock()，但仍有必要仔细评估 unlock() 和 try_lock() 的安全性。此外，还需要评估这些原语的调试版本。简而言之，除非别无选择，否则不要从其他上下文获取睡眠锁。

- Sleeping lock types:

    mutex
    rt_mutex
    semaphore
    rw_semaphore
    ww_mutex
    percpu_rw_semaphore

- On PREEMPT_RT kernels, these lock types are converted to sleeping locks:

    local_lock
    spinlock_t
    rwlock_t

参考: https://github.com/torvalds/linux/blob/master/Documentation/locking/locktypes.rst#sleeping-locks

==== CPU local locks(CPU本地锁)

    local_lock

On non-PREEMPT_RT kernels, local_lock functions are wrappers around preemption and interrupt disabling primitives. Contrary to other locking mechanisms, disabling preemption or interrupts are pure CPU local concurrency control mechanisms and not suited for inter-CPU concurrency control.
在非 PREEMPT_RT 内核中，local_lock 禁用抢占并禁用中断。

参考: https://github.com/torvalds/linux/blob/master/Documentation/locking/locktypes.rst#cpu-local-locks

==== Spinning locks(自旋锁)

    raw_spinlock_t
    bit spinlocks

在非 PREEMPT_RT 内核上，这些锁类型也是自旋锁:

    spinlock_t
    rwlock_t

Spinning locks implicitly disable preemption and the lock / unlock functions can have suffixes which apply further protections:

    _bh()	Disable / enable bottom halves (soft interrupts)
    _irq()	Disable / enable interrupts
    _irqsave/restore()	Save and disable / restore interrupt disabled state

参考: https://github.com/torvalds/linux/blob/master/Documentation/locking/locktypes.rst#spinning-locks

==== 所有者语义(owner semantics)
上述除信号量之外的锁类型都有严格的所有者语义:
获取锁的上下文（任务）必须释放它(The context (task) that acquired the lock must release it)。
rw_semaphores 有一个特殊的接口，允许非所有者释放读者。

参考: https://github.com/torvalds/linux/blob/master/Documentation/locking/locktypes.rst#owner-semantics

==== 锁类型嵌套规则(lock type nesting rules)
最基本的规则是:

    同一锁类别（睡眠、CPU 本地、旋转）的锁类型可以任意嵌套，只要它们遵守一般锁排序规则以防止死锁。
    睡眠锁类型不能嵌套在 CPU 本地锁和旋转锁类型内。
    CPU 本地锁和旋转锁类型可以嵌套在睡眠锁类型中。
    自旋锁类型可以嵌套在所有锁类型中。
    这些约束适用于 PREEMPT_RT 和其他情况。

PREEMPT_RT 将 spinlock_t 和 rwlock_t 的锁类型从自旋改为休眠，并将 local_lock 替换为每个 CPU 的 spinlock_t，这意味着在持有 raw spinlock 时无法获取它们。这导致以下嵌套顺序:

    1. Sleeping locks
    2. spinlock_t, rwlock_t, local_lock
    3. raw_spinlock_t and bit spinlocks

Lockdep will complain if these constraints are violated, both in PREEMPT_RT and otherwise.

参考: https://github.com/torvalds/linux/blob/master/Documentation/locking/locktypes.rst#lock-type-nesting-rules

=== volatile
https://github.com/torvalds/linux/blob/master/Documentation/process/volatile-considered-harmful.rst

Quick Quiz 1: Can't the compiler also reorder these accesses?
Answer: Given the current Linux-kernel definitions of READ_ONCE() and WRITE_ONCE(), no. These two macros map to volatile accesses, which the compiler is not allowed to reorder with respect to each other.
However, if these macros instead mapped to non-volatile C11 memory_order_relaxed loads and stores, then the compiler would be permitted to reorder them. And, as a general rule, compilers are much more aggressive about reordering accesses than even the most weakly ordered hardware. In both cases, those who don't like such code rearrangement call it “weak ordering” while those who do call it “optimization”.
参考: https://lwn.net/Articles/720550/

参考:
https://lwn.net/Kernel/Index/#volatile
perfbook: 4.3.4.2 A Volatile Solution

=== 原子操作
==== 整数
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/types.h
----
typedef struct {
	int counter;
} atomic_t;

#define ATOMIC_INIT(i) { (i) }

#ifdef CONFIG_64BIT
typedef struct {
	s64 counter;
} atomic64_t;
#endif
----

操作: https://elixir.bootlin.com/linux/latest/source/include/asm-generic/atomic.h

x86:
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/atomic.h

ARM64:
https://elixir.bootlin.com/linux/latest/source/arch/arm64/include/asm/atomic.h
https://elixir.bootlin.com/linux/latest/source/arch/arm64/include/asm/atomic_ll_sc.h
https://elixir.bootlin.com/linux/latest/source/arch/arm64/include/asm/atomic_lse.h

参考:
https://stackoverflow.com/questions/36624881/why-is-integer-assignment-on-a-naturally-aligned-variable-atomic-on-x86
https://developer.arm.com/documentation/ddi0406/cb/Application-Level-Architecture/Application-Level-Memory-Model/Memory-types-and-attributes-and-the-memory-order-model/Atomicity-in-the-ARM-architecture

==== 位操作
x86:
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/bitops.h

ARM:
https://elixir.bootlin.com/linux/latest/source/arch/arm/include/asm/bitops.h

==== refcount_t
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/refcount_types.h
----
typedef struct refcount_struct {
	atomic_t refs;
} refcount_t;
----

vs-atomic:
refcount_*() 和 atomic_*() 函数在内存顺序保证方面有很多不同
https://docs.kernel.org/core-api/refcount-vs-atomic.html

==== rcuref_t
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/types.h
----
typedef struct {
	atomic_t refcnt;
} rcuref_t;
----

==== 参考
https://lwn.net/Kernel/Index/#Atomic_operations
https://lwn.net/Kernel/Index/#atomic_t
https://lwn.net/Kernel/Index/#C11_atomic_operations
https://lwn.net/Kernel/Index/#Atomic_spinlocks
https://lwn.net/Kernel/Index/#Atomic_IO_operations
https://lwn.net/Kernel/Index/#ACCESS_ONCE
https://lwn.net/Kernel/Index/#Reference_counting
https://docs.kernel.org/core-api/index.html#concurrency-primitives

=== barrier
==== 概念
https://github.com/orientye/understand/blob/main/concurrency/concurrency/thread/memory-order.asc

==== compiler barrier
===== barrier()与barrier_data(ptr)
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/compiler.h
----
/* Optimization barrier */
#ifndef barrier
/* The "volatile" is due to gcc bugs */
# define barrier() __asm__ __volatile__("": : :"memory")
#endif

#ifndef barrier_data
/*
 * This version is i.e. to prevent dead stores elimination on @ptr
 * where gcc and llvm may behave differently when otherwise using
 * normal barrier(): while gcc behavior gets along with a normal
 * barrier(), llvm needs an explicit input variable to be assumed
 * clobbered. The issue is as follows: while the inline asm might
 * access any memory it wants, the compiler could have fit all of
 * @ptr into memory registers instead, and since @ptr never escaped
 * from that, it proved that the inline asm wasn't touching any of
 * it. This version works well with both compilers, i.e. we're telling
 * the compiler that the inline asm absolutely may see the contents
 * of @ptr. See also: https://llvm.org/bugs/show_bug.cgi?id=15495
 */
# define barrier_data(ptr) __asm__ __volatile__("": :"r"(ptr) :"memory")
#endif
----
看 https://llvm.org/bugs/show_bug.cgi?id=15495 这个BUG
参考: https://en.wikipedia.org/wiki/Memory_ordering#Compile-time_memory_barrier_implementation

===== READ_ONCE()与WRITE_ONCE()
[source, c]
.https://elixir.bootlin.com/linux/latest/source/tools/include/linux/compiler.h
----
static __always_inline void __read_once_size(const volatile void *p, void *res, int size)
{
	switch (size) {
	case 1: *(__u8_alias_t  *) res = *(volatile __u8_alias_t  *) p; break;
	case 2: *(__u16_alias_t *) res = *(volatile __u16_alias_t *) p; break;
	case 4: *(__u32_alias_t *) res = *(volatile __u32_alias_t *) p; break;
	case 8: *(__u64_alias_t *) res = *(volatile __u64_alias_t *) p; break;
	default:
		barrier();
		__builtin_memcpy((void *)res, (const void *)p, size);
		barrier();
	}
}

static __always_inline void __write_once_size(volatile void *p, void *res, int size)
{
	switch (size) {
	case 1: *(volatile  __u8_alias_t *) p = *(__u8_alias_t  *) res; break;
	case 2: *(volatile __u16_alias_t *) p = *(__u16_alias_t *) res; break;
	case 4: *(volatile __u32_alias_t *) p = *(__u32_alias_t *) res; break;
	case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
	default:
		barrier();
		__builtin_memcpy((void *)p, (const void *)res, size);
		barrier();
	}
}

/*
 * Prevent the compiler from merging or refetching reads or writes. The
 * compiler is also forbidden from reordering successive instances of
 * READ_ONCE and WRITE_ONCE, but only when the compiler is aware of some
 * particular ordering. One way to make the compiler aware of ordering is to
 * put the two invocations of READ_ONCE or WRITE_ONCE in different C
 * statements.
 *
 * These two macros will also work on aggregate data types like structs or
 * unions. If the size of the accessed data type exceeds the word size of
 * the machine (e.g., 32 bits or 64 bits) READ_ONCE() and WRITE_ONCE() will
 * fall back to memcpy and print a compile-time warning.
 *
 * Their two major use cases are: (1) Mediating communication between
 * process-level code and irq/NMI handlers, all running on the same CPU,
 * and (2) Ensuring that the compiler does not fold, spindle, or otherwise
 * mutilate accesses that either do not require ordering or that interact
 * with an explicit memory barrier or atomic instruction that provides the
 * required ordering.
 */

#define READ_ONCE(x)					\
({							\
	union { typeof(x) __val; char __c[1]; } __u =	\
		{ .__c = { 0 } };			\
	__read_once_size(&(x), __u.__c, sizeof(x));	\
	__u.__val;					\
})

#define WRITE_ONCE(x, val)				\
({							\
	union { typeof(x) __val; char __c[1]; } __u =	\
		{ .__val = (val) }; 			\
	__write_once_size(&(x), __u.__c, sizeof(x));	\
	__u.__val;					\
})
----

[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/asm-generic/rwonce.h
----
/*
 * Yes, this permits 64-bit accesses on 32-bit architectures. These will
 * actually be atomic in some cases (namely Armv7 + LPAE), but for others we
 * rely on the access being split into 2x32-bit accesses for a 32-bit quantity
 * (e.g. a virtual address) and a strong prevailing wind.
 */
#define compiletime_assert_rwonce_type(t)					\
	compiletime_assert(__native_word(t) || sizeof(t) == sizeof(long long),	\
		"Unsupported access size for {READ,WRITE}_ONCE().")

/*
 * Use __READ_ONCE() instead of READ_ONCE() if you do not require any
 * atomicity. Note that this may result in tears!
 */
#ifndef __READ_ONCE
#define __READ_ONCE(x)	(*(const volatile __unqual_scalar_typeof(x) *)&(x))
#endif

#define READ_ONCE(x)							\
({									\
	compiletime_assert_rwonce_type(x);				\
	__READ_ONCE(x);							\
})

#define __WRITE_ONCE(x, val)						\
do {									\
	*(volatile typeof(x) *)&(x) = (val);				\
} while (0)

#define WRITE_ONCE(x, val)						\
do {									\
	compiletime_assert_rwonce_type(x);				\
	__WRITE_ONCE(x, val);						\
} while (0)
----

Prevent the compiler from merging or refetching reads or writes. The compiler is also forbidden from reordering successive instances of READ_ONCE and WRITE_ONCE, but only when the compiler is aware of some particular ordering. One way to make the compiler aware of ordering is to put the two invocations of READ_ONCE or WRITE_ONCE in different C statements.

These two macros will also work on aggregate data types like structs or unions.

Their two major use cases are: (1) Mediating communication between process-level code and irq/NMI handlers, all running on the same CPU, and (2) Ensuring that the compiler does not fold, spindle, or otherwise mutilate accesses that either do not require ordering or that interact with an explicit memory barrier or atomic instruction that provides the required ordering.

READ_ONCE(x) has much in common with the GCC intrinsic __atomic_load_n(&x, __ATOMIC_
RELAXED) and WRITE_ONCE() has much in common with the GCC intrinsic __atomic_store_n(&x, v,
__ATOMIC_RELAXED). - perfbook 4.2.5 Atomic Operations (GCC Classic)

在2018年的v4.15版本中，Linux内核的ACCESS_ONCE()被分别替换为READ_ONCE()和WRITE_ONCE()，分别用于读取和写入操作。ACCESS_ONCE()最初是作为RCU代码中的一个辅助函数引入的，但不久后就被提升为核心API。Linux内核的READ_ONCE()和WRITE_ONCE()已经演变成复杂的形式，看起来与原始的ACCESS_ONCE()实现有很大不同，这是因为需要支持大型结构体的单次访问语义，同时又要处理在结构体无法通过单条机器指令完成加载/存储时可能出现的读写撕裂问题。- perfbook 4.2.5 Atomic Operations (GCC Classic) Quick Quiz 4.25

参考:
https://github.com/google/kernel-sanitizers/blob/master/other/READ_WRITE_ONCE.md
https://lwn.net/Kernel/Index/#ACCESS_ONCE
https://stackoverflow.com/questions/50589499/write-once-and-read-once-in-linux-kernel
https://www.usenix.org/legacy/event/hotpar11/tech/final_files/Boehm.pdf

==== memory barrier
===== 种类

    通用屏障(General Barriers)
        最强大，也最昂贵，适用于所有情况
    SMP屏障(SMP Barriers)
        在SMP系统上有效，UP系统编译为空操作，性能更好
    DMA屏障(DMA Barriers)
        专门用于CPU和设备之间的内存一致性

    是否需要内存排序？
        ├── 是：涉及设备访问？ → 是 → 使用DMA屏障
        │                        │
        │                        └── 否：系统是SMP？ → 是 → 使用SMP屏障
        │                                                    │
        │                                                    └── 否 → 使用通用屏障
        │
        └── 否：仅需编译器屏障？ → 是 → 使用barrier()

===== 实现
====== generic
https://elixir.bootlin.com/linux/latest/source/include/asm-generic/barrier.h
https://elixir.bootlin.com/linux/latest/source/include/linux/atomic/atomic-instrumented.h
https://elixir.bootlin.com/linux/latest/source/include/linux/atomic.h

====== X86
[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/barrier.h
----
/*
 * Force strict CPU ordering.
 * And yes, this might be required on UP too when we're talking
 * to devices.
 */

#ifdef CONFIG_X86_32
#define mb() asm volatile(ALTERNATIVE("lock addl $0,-4(%%esp)", "mfence", \
				      X86_FEATURE_XMM2) ::: "memory", "cc")
#define rmb() asm volatile(ALTERNATIVE("lock addl $0,-4(%%esp)", "lfence", \
				       X86_FEATURE_XMM2) ::: "memory", "cc")
#define wmb() asm volatile(ALTERNATIVE("lock addl $0,-4(%%esp)", "sfence", \
				       X86_FEATURE_XMM2) ::: "memory", "cc")
#else
#define __mb()	asm volatile("mfence":::"memory")
#define __rmb()	asm volatile("lfence":::"memory")
#define __wmb()	asm volatile("sfence" ::: "memory")
#endif

/**
 * array_index_mask_nospec() - generate a mask that is ~0UL when the
 * 	bounds check succeeds and 0 otherwise
 * @index: array element index
 * @size: number of elements in array
 *
 * Returns:
 *     0 - (index < size)
 */
#define array_index_mask_nospec(idx,sz) ({	\
	typeof((idx)+(sz)) __idx = (idx);	\
	typeof(__idx) __sz = (sz);		\
	unsigned long __mask;			\
	asm volatile ("cmp %1,%2; sbb %0,%0"	\
			:"=r" (__mask)		\
			:ASM_INPUT_G (__sz),	\
			 "r" (__idx)		\
			:"cc");			\
	__mask; })

/* Prevent speculative execution past this barrier. */
#define barrier_nospec() alternative("", "lfence", X86_FEATURE_LFENCE_RDTSC)

#define __dma_rmb()	barrier()
#define __dma_wmb()	barrier()

#define __smp_mb()	asm volatile("lock addl $0,-4(%%" _ASM_SP ")" ::: "memory", "cc")

#define __smp_rmb()	dma_rmb()
#define __smp_wmb()	barrier()
#define __smp_store_mb(var, value) do { (void)xchg(&var, value); } while (0)

#define __smp_store_release(p, v)					\
do {									\
	compiletime_assert_atomic_type(*p);				\
	barrier();							\
	WRITE_ONCE(*p, v);						\
} while (0)

#define __smp_load_acquire(p)						\
({									\
	typeof(*p) ___p1 = READ_ONCE(*p);				\
	compiletime_assert_atomic_type(*p);				\
	barrier();							\
	___p1;								\
})

/* Atomic operations are already serializing on x86 */
#define __smp_mb__before_atomic()	do { } while (0)
#define __smp_mb__after_atomic()	do { } while (0)

/* Writing to CR3 provides a full memory barrier in switch_mm(). */
#define smp_mb__after_switch_mm()	do { } while (0)
----

====== ARM
- DMB: Data Memory Barrier
数据内存屏障
确保在屏障之前的所有内存访问（读/写）都完成后，才执行屏障之后的内存访问。
可以指定访问类型（如只屏障写操作、或全部操作）。
常用场景: 确保对共享变量的写操作在所有核上可见后，再执行后续操作。

- DSB: Data Synchronization Barrier
数据同步屏障
比 DMB 更严格: 确保在屏障之前的所有内存访问和相关操作（如缓存维护、分支预测等）都完成后，才执行屏障之后的任何指令（不仅仅是内存访问）。
常用场景: 在修改页表、切换地址空间、修改系统寄存器后，需要确保所有操作完成。

- ISB: Instruction Synchronization Barrier
指令同步屏障
确保所有在 ISB 之前的指令（包括屏障）执行完成后，才从指令缓存中重新取指。
常用场景: 修改处理器状态（如系统寄存器、内存映射）后，确保后续指令看到新状态。

- ARMv8 上述内存屏障指令的一些变化
ARMv8 除了保持上述指令外，还引入了更精细的控制：
可以通过参数指定屏障的作用域和访问类型：
类型: 读（Load）、写（Store）、全部（Full）
作用域: 全系统（System）、单个核（Inner Shareable）、非共享（Non-shareable）等

- LDAR(Load-Acquire)/STLR(Store-Release)

    ARMv8 引入
    示例:
        旧方式（显式DMB）
            STR x0, [data]
            DMB ISH
            STR w1, [flag]
            需要两条指令，屏障作用范围可能略大
        新方式（STLR）
            STR x0, [data]
            STLR w1, [flag]
            一条指令搞定存储和释放屏障，更高效、更直观

- 实现
https://elixir.bootlin.com/linux/latest/source/arch/arm64/include/asm/barrier.h
https://elixir.bootlin.com/linux/latest/source/arch/arm/include/asm/barrier.h

==== 规范
Our memory model is highly constrained because it must match the kernel's behavior (or intended behavior). However, there are numerous choices to be made, so we formulated the following principles to guide those choices:

    1. Strength preferred to weakness.
    2. Simplicity preferred to complexity.
    3. Support existing non-buggy Linux-kernel code.
    4. Be compatible with hardware supported by the Linux kernel.
    5. Support future hardware, within reason.
    6. Be compatible with the C11 memory model, where prudent and reasonable.
    7. Expose questions and areas of uncertainty.

参考: https://lwn.net/Articles/718628/

==== 性能
https://ipads.se.sjtu.edu.cn/_media/publications/liuppopp20.pdf

==== 测试
Herding cats: Modelling, Simulation, Testing, and Data-mining for Weak Memory:
https://dl.acm.org/doi/pdf/10.1145/2627752

==== tool
https://lwn.net/Kernel/Index/#Development_tools-Linux_kernel_memory_model
https://github.com/torvalds/linux/tree/master/tools/memory-model
https://docs.kernel.org/dev-tools/kcsan.html

==== 参考
https://github.com/torvalds/linux/blob/master/Documentation/memory-barriers.txt
https://github.com/torvalds/linux/tree/master/tools/memory-model
https://github.com/torvalds/linux/tree/master/tools/memory-model/Documentation
https://lwn.net/Kernel/Index/#Memory_barriers
https://lwn.net/Articles/718628/
https://lwn.net/Articles/720550/
Linux-Kernel Memory Model: https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4374.html
A Strong Formal Model of Linux-Kernel Memory Ordering: https://mirrors.edge.kernel.org/pub/linux/kernel/people/paulmck/LWNLinuxMM/StrongModel.html
A Weak Formal Model of Linux-Kernel Memory Ordering: https://mirrors.edge.kernel.org/pub/linux/kernel/people/paulmck/LWNLinuxMM/WeakModel.html
<<Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors>>: https://www.csl.cornell.edu/courses/ece5750/gharachorloo.isca90.pdf
https://developer.arm.com/documentation/100941/0101/Barriers
https://zhuanlan.zhihu.com/p/96001570
https://mp.weixin.qq.com/s?__biz=MzAxMDM0NjExNA==&mid=2247487950&idx=1&sn=c6cb416efc2831c5666a5ae9a205dcf3&chksm=9b509a23ac2713352a6998b9ff34f29b02cc57a0cfe62bcc13a8aa3583c5e78d2d038aacf17c&mpshare=1&scene=1&srcid=0827lWWERv8vkNak5sANCSAc&sharer_sharetime=1667631485956&sharer_shareid=3d769dc0d221507247ffa710c89a2772&exportkey=n_ChQIAhIQ8I4w6FfIvqlzaen82GNKVxKZAgIE97dBBAEAAAAAAFWOLeiWGmcAAAAOpnltbLcz9gKNyK89dVj0hC%2FkJy43MLPqIOkmygOzvgdnTEDr65XdDo6dZ7SwCefoAImebvkCfI61eSTYahvaCP06FEy0n7UJvX9u%2FYIz1JS9%2BCCA7d5%2FKyMAjXkuHHt86AbasiIxztU5VUIa9lMcGL9zwaT6JlgIkO%2FIk6YccnxLN2UKLxcaBqJJeySCIuPYpjDSxg56oh3IgIsMMvIhwN96Lxu3LjEzNZIhOyr%2B88hqFYIZTFtAul%2BgftiyMUHcrW3Jhlg8obgZBYXDR1BC%2BgtBQQhATMXarg893oVKs7fJNkN69Ckjn23rqVgibQjnuMaNadco8Fw2bkVzDaCUkU2Z&acctmode=0&pass_ticket=QA7RhEsMegR3%2FRIcRqL6%2FVpdMlpaht0VwPJpMgvvdoPO%2FeAeWFcoxw0fS41vu6Qx&wx_header=0#rd[原理和实战解析Linux中如何正确地使用内存屏障]
perfbook: Why Memory Barriers

=== 自旋锁
==== 概念
- 自旋锁用于保护短的代码段
- 试图获得一个被持有的自旋锁的线程，会一直进行忙循环等待
- 自旋锁不应被长时间地持有，持有自旋锁的时间建议小于两次上下文切换的时间
- 与引起睡眠的锁相比，自旋锁不会引起上下文切换，通常比睡眠锁效率高

- Q: 为什么要关闭抢占？
假设在单CPU上，task a获取了spinlock之后发生了抢占，抢占唤醒了优先级更高的task b，如果task b也需要获取该spinlock，此时就会发生死锁。因此，内核在获取spinlock的时候，需要禁止本CPU上的抢占。
如果a和b运行在不同的CPU上，因为a马上就会释放spin lock，所以b的spin状态很快就会得到解除，这种情况下就不会发生死锁了。

- Q: 为什么自旋锁保护的代码不能进入睡眠状态？
自旋锁睡眠时，由于当前CPU的睡眠以及当前CPU的禁止处理器抢占，系统运行时不正常的。
但:
如果存在多个CPU，那么其它CPU可以继续运行，并有可能唤醒睡眠了的自旋锁，此时自旋锁睡眠的CPU才会得以运行正常。
如果允许中断处理，那么中断的代码是可以正常运行的，但是中断通常不会唤醒睡眠的自旋锁，因此系统仍然运行不正常。

- Q: 什么情况下需要关闭中断？
当中断上下文中与task存在共享资源时，可能会发生死锁。例如某个任务获得了spinlock，此时发生中断，而中断里面也在获取该spinlock，死锁就发生了，此时尽管可以产生调度，但由于spinlock关闭了抢占，这个CPU就浪费了。

- 主要API
    * spin_lock()与spin_unlock()
        ** 禁止内核抢占
        ** spin_lock()并不会禁止中断，当不会与中断处理程序共享资源时可以使用
        ** 系统的很多事件/输入来源于中断(例如最重要的时钟中断)，禁止中断会造成系统响应的延迟
        ** 使用场景：确定不会有中断上下文（包括软中断和硬中断）来竞争同一把锁时使用
    * spin_lock_irq()与spin_unlock_irq()
        ** 禁止内核抢占并屏蔽中断
        ** spin_unlock_irq() 在释放锁的同时，会无条件地开启中断
        ** 风险：如果加锁前中断已经是关闭的，解锁后中断会被错误地打开。这破坏了中断状态的上下文，可能导致难以调试的问题
        ** 使用场景：仅当能确定加锁前中断一定是开启的（例如，在进程上下文中）
        ** 由于风险存在，实际代码中使用较少，更多使用更安全的 spin_lock_irqsave
    * spin_lock_irqsave()与spin_unlock_irqrestore()
        ** 禁止内核抢占并屏蔽中断，事先保存中断屏蔽位并事后恢复原状
        ** 最安全、最常用的中断控制版本
        ** spin_lock_irqsave() 会在关闭中断前，将当前的中断标志位保存到一个局部变量中
        ** 解锁时，spin_unlock_irqrestore() 会用这个变量来精确恢复之前的中断状态，而不是简单地打开中断
        ** 使用场景：任何可能在中断上下文（硬中断）中竞争同一把锁的情况。是处理“进程上下文 vs 中断上下文”竞争的常用方法
    * spin_lock_bh()与spin_unlock_bh()
        ** 禁止内核抢占并屏蔽软中断(SoftIRQ)
        ** bh: Bottom Half, 主要针对的是软中断和 Tasklet（运行在软中断上下文中）
        ** 不关闭硬件中断，因此硬件中断仍然可以响应，只是其下半部（软中断）被延迟执行了
        ** 使用场景：保护在进程上下文和软中断/Tasklet上下文之间共享的资源。是处理“进程上下文 vs 软中断上下文”竞争的常用方法

==== raw spinlock
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/spinlock_types.h
----
#ifndef CONFIG_PREEMPT_RT

/* Non PREEMPT_RT kernels map spinlock to raw_spinlock */
typedef struct spinlock {
	union {
		struct raw_spinlock rlock;

#ifdef CONFIG_DEBUG_LOCK_ALLOC
# define LOCK_PADSIZE (offsetof(struct raw_spinlock, dep_map))
		struct {
			u8 __padding[LOCK_PADSIZE];
			struct lockdep_map dep_map;
		};
#endif
	};
} spinlock_t;
//...
#else /* !CONFIG_PREEMPT_RT */

/* PREEMPT_RT kernels map spinlock to rt_mutex */
#include <linux/rtmutex.h>

typedef struct spinlock {
	struct rt_mutex_base	lock;
#ifdef CONFIG_DEBUG_LOCK_ALLOC
	struct lockdep_map	dep_map;
#endif
} spinlock_t;
#endif /* CONFIG_PREEMPT_RT */
----

[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/spinlock_types_raw.h
----
typedef struct raw_spinlock {
	arch_spinlock_t raw_lock;
#ifdef CONFIG_DEBUG_SPINLOCK
	unsigned int magic, owner_cpu;
	void *owner;
#endif
#ifdef CONFIG_DEBUG_LOCK_ALLOC
	struct lockdep_map dep_map;
#endif
} raw_spinlock_t;
----

spinlock的实现会区分单核、多核，且多核是架构相关的，此外还涉及CONFIG_PREEMPT_RT等众多宏。
在没有定义CONFIG_PREEMPT_RT的情况下，由arch_spinlock_t实现。
在单核上，arch_spinlock_t基本等同于空结构体: https://elixir.bootlin.com/linux/latest/source/include/linux/spinlock_types_up.h

▪ x86与ARM64:
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/spinlock.h
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/spinlock_types.h

可见，x86通过:
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/qspinlock.h
最终通过:
https://elixir.bootlin.com/linux/latest/source/include/asm-generic/qspinlock.h

ARM64也是:
https://elixir.bootlin.com/linux/latest/source/arch/arm64/include/asm/spinlock.h
https://elixir.bootlin.com/linux/latest/source/arch/arm64/include/asm/spinlock_types.h
即通用排队自旋锁来实现自旋锁。

历史:
x86: since 4.2, https://lkml.iu.edu/hypermail/linux/kernel/1506.2/04205.html
ARM64: since 4.19

▪ 32位ARM:
https://elixir.bootlin.com/linux/latest/source/arch/arm/include/asm/spinlock_types.h
可见，其自旋锁是通过下文的tick spinlock来实现的。

参考:
https://github.com/0xAX/linux-insides/blob/master/SyncPrim/linux-sync-1.md#spinlocks-in-the-linux-kernel

==== ticket spinlock
▪ 历史与作用
since 2.6.25
主要解决公平性
之前的自旋锁有一个缺点: it is unfair.
lock unfairness can also create latency issues; it is hard to give latency guarantees when the wait time for a spinlock can be arbitrarily long.

▪ 主要思想:
使用u16 next和u16 owner两个变量
类似于叫号，当柜台叫到的号码(next变量)与手中的号码(owner变量)一致时，柜台就可以为之服务了。
这样，也就可以让CPU按照获取的顺序(即FIFO)获得所有权，由此形成了一种有序的竞争。

▪ 但也带来了新的问题:
只要spinlock的值被更改，所有试图获取spinlock的CPU对应的cache-line都会被invalidate，此时会一直重新从内存读取新的spinlock的值到自己的cache-line中。
然而，实时上只有队列中最先达到的CPU才可以获得所有权，只有它的cache-line才需要被invalidate，其它的CPU则都是在做无用功。
也就是说，在cache-line的问题上，tick spinlock存在效率问题。

▪ 32位ARM
其实对于ARM64, v3.13-v4.18也是采用ticket spinlock。
数据结构:
[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/arm/include/asm/spinlock_types.h
----
typedef struct {
	union {
		u32 slock;
		struct __raw_tickets {
#ifdef __ARMEB__
			u16 next;
			u16 owner;
#else
			u16 owner;
			u16 next;
#endif
		} tickets;
	};
} arch_spinlock_t;
----
实现: https://elixir.bootlin.com/linux/latest/source/arch/arm/include/asm/spinlock.h

参考:
https://lwn.net/Articles/267968/
https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=314cdbefd1fd0a7acf3780e9628465b77ea6a836
https://en.wikipedia.org/wiki/Ticket_lock
https://zhuanlan.zhihu.com/p/80727111

==== mcs spinlock
===== 概览
▪ 历史与作用
since 3.15, mcs是两位作者的简称
per-CPU structure, 在cache-line上效率更高:
The MCS lock (proposed by Mellor-Crummey and Scott) is a simple spin-lock with the desirable properties of being fair, and with each cpu trying to acquire the lock spinning on a local variable. It avoids expensive cache bounces that common test-and-set spin-lock implementations incur.
注意:
mcs spinlock目前的唯一使用场景是qspinlock。

▪ 主要思想:
真正拥有所有权的spinlock如果unlock了，只需要把所有权传递给next spinlock，从而解决了ticket spinlock在cache-line上颠簸的效率问题。

▪ 但也带来了新的问题:
mcs spinlock比4个字节的ticket spinlock多了一个指针(即多了4/8个字节)，也就是说空间变为原来的2倍或3倍。而spinlock是内核的基础结构，有时会被嵌入到一些对结构体大小十分敏感的结构体中(例如struct page)，毫无疑问spinlock空间上的增大，对这些结构影响是很大的，这也正是spinlock不(直接)使用mcs spinlock的原因。

===== 结构
[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/locking/mcs_spinlock.h
----
struct mcs_spinlock {
	struct mcs_spinlock *next;
	int locked; /* 1 if lock acquired */
	int count;  /* nesting count, see qspinlock.c */
};
----
struct mcs_spinlock *next: 指向下一个锁的申请者，构成串行的等待队列的链表
int locked: 1表示持有锁
int count: 嵌套的计数

===== 操作
[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/locking/mcs_spinlock.h
----
#ifndef arch_mcs_spin_lock_contended
/*
 * Using smp_cond_load_acquire() provides the acquire semantics
 * required so that subsequent operations happen after the
 * lock is acquired. Additionally, some architectures such as
 * ARM64 would like to do spin-waiting instead of purely
 * spinning, and smp_cond_load_acquire() provides that behavior.
 */
#define arch_mcs_spin_lock_contended(l)					\
do {									\
	smp_cond_load_acquire(l, VAL);					\
} while (0)
#endif

#ifndef arch_mcs_spin_unlock_contended
/*
 * smp_store_release() provides a memory barrier to ensure all
 * operations in the critical section has been completed before
 * unlocking.
 */
#define arch_mcs_spin_unlock_contended(l)				\
	smp_store_release((l), 1)
#endif

/*
 * Note: the smp_load_acquire/smp_store_release pair is not
 * sufficient to form a full memory barrier across
 * cpus for many architectures (except x86) for mcs_unlock and mcs_lock.
 * For applications that need a full barrier across multiple cpus
 * with mcs_unlock and mcs_lock pair, smp_mb__after_unlock_lock() should be
 * used after mcs_lock.
 */

/*
 * In order to acquire the lock, the caller should declare a local node and
 * pass a reference of the node to this function in addition to the lock.
 * If the lock has already been acquired, then this will proceed to spin
 * on this node->locked until the previous lock holder sets the node->locked
 * in mcs_spin_unlock().
 */
static inline
void mcs_spin_lock(struct mcs_spinlock **lock, struct mcs_spinlock *node)
{
	struct mcs_spinlock *prev;

	/* Init node */
	node->locked = 0;
	node->next   = NULL;

	/*
	 * We rely on the full barrier with global transitivity implied by the
	 * below xchg() to order the initialization stores above against any
	 * observation of @node. And to provide the ACQUIRE ordering associated
	 * with a LOCK primitive.
	 */
	prev = xchg(lock, node);
	if (likely(prev == NULL)) {
		/*
		 * Lock acquired, don't need to set node->locked to 1. Threads
		 * only spin on its own node->locked value for lock acquisition.
		 * However, since this thread can immediately acquire the lock
		 * and does not proceed to spin on its own node->locked, this
		 * value won't be used. If a debug mode is needed to
		 * audit lock status, then set node->locked value here.
		 */
		return;
	}
	WRITE_ONCE(prev->next, node);

	/* Wait until the lock holder passes the lock down. */
	arch_mcs_spin_lock_contended(&node->locked);
}

/*
 * Releases the lock. The caller should pass in the corresponding node that
 * was used to acquire the lock.
 */
static inline
void mcs_spin_unlock(struct mcs_spinlock **lock, struct mcs_spinlock *node)
{
	struct mcs_spinlock *next = READ_ONCE(node->next);

	if (likely(!next)) {
		/*
		 * Release the lock by setting it to NULL
		 */
		if (likely(cmpxchg_release(lock, node, NULL) == node))
			return;
		/* Wait until the next pointer is set */
		while (!(next = READ_ONCE(node->next)))
			cpu_relax();
	}

	/* Pass lock to next waiter. */
	arch_mcs_spin_unlock_contended(&next->locked);
}
----

每个CPU使用原子指令将自己的锁结构的地址与MCS Lock的next指针交换来获取锁，因此MCS Lock的next指针始终指向等待锁队列的最后一个。
每个CPU在自己的locked值上自旋，从而避免绝大部分的cache颠簸。

===== 参考
https://lwn.net/Articles/590243/
https://blog.csdn.net/bemind1/article/details/118224344
https://bugzilla.kernel.org/show_bug.cgi?id=206115

==== qspinlock(queue spinlock)
===== 概览
▪ 历史与作用
since 4.2
基于mcs spinlock的思想但解决了mcs spinlock占用空间大的问题。

▪ 主要思想:
与传统的mcs相比，qspinlock通过存储队尾mcs节点的CPU号而不直接存储节点的地址将spinlock的结构大小压缩到了32bit。

▪ 参考:
https://lwn.net/Articles/590243/
https://lkml.org/lkml/2015/4/24/631

===== 结构
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/asm-generic/qspinlock_types.h
----
typedef struct qspinlock {
	union {
		atomic_t val;

		/*
		 * By using the whole 2nd least significant byte for the
		 * pending bit, we can allow better optimization of the lock
		 * acquisition for the pending bit holder.
		 */
#ifdef __LITTLE_ENDIAN
		struct {
			u8	locked;
			u8	pending;
		};
		struct {
			u16	locked_pending;
			u16	tail;
		};
#else
		struct {
			u16	tail;
			u16	locked_pending;
		};
		struct {
			u8	reserved[2];
			u8	pending;
			u8	locked;
		};
#endif
	};
} arch_spinlock_t;
//...
/*
 * Bitfields in the atomic value:
 *
 * When NR_CPUS < 16K
 *  0- 7: locked byte
 *     8: pending
 *  9-15: not used
 * 16-17: tail index
 * 18-31: tail cpu (+1)
 *
 * When NR_CPUS >= 16K
 *  0- 7: locked byte
 *     8: pending
 *  9-10: tail index
 * 11-31: tail cpu (+1)
 */
----
struct qspinlock为4个字节的联合体:

    atomic_t val: 整个32bit值
    u8	locked: 表示是否加锁，0表示未加锁，其余值表示已加锁
    u8	pending: 第一个等待锁的CPU需要先设置pending位，后续等待锁的CPU则全部进入MCS spinlock队列自旋等待。
    u16	locked_pending: 8个bit的locked + 1个bit的pending
    u16	tail:
        2bit: qnodes[MAX_NODES]的idx(即tail index)
        14bit(NR_CPUS < 16K)/21bit(NR_CPUS >= 16K): CPU idx+1(即tail cpu)
        Q: 为什么这里是tail呢?
        A: 这是由mcs_spinlock的性质决定的?
        Q: 为什么这里cpu idx需要加1?
        A: 如果不加1，便不能区分0号CPU与等待队列中没有成员的情况。
    __LITTLE_ENDIAN宏: 表示小端

[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/locking/qspinlock.c
----
#include "mcs_spinlock.h"
#define MAX_NODES	4

/*
 * On 64-bit architectures, the mcs_spinlock structure will be 16 bytes in
 * size and four of them will fit nicely in one 64-byte cacheline. For
 * pvqspinlock, however, we need more space for extra data. To accommodate
 * that, we insert two more long words to pad it up to 32 bytes. IOW, only
 * two of them can fit in a cacheline in this case. That is OK as it is rare
 * to have more than 2 levels of slowpath nesting in actual use. We don't
 * want to penalize pvqspinlocks to optimize for a rare case in native
 * qspinlocks.
 */
struct qnode {
	struct mcs_spinlock mcs;
#ifdef CONFIG_PARAVIRT_SPINLOCKS
	long reserved[2];
#endif
};
//...
/*
 * Per-CPU queue node structures; we can never have more than 4 nested
 * contexts: task, softirq, hardirq, nmi.
 *
 * Exactly fits one 64-byte cacheline on a 64-bit architecture.
 *
 * PV doubles the storage and uses the second cacheline for PV state.
 */
static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);
----
每个qnode就是一个mcs_spinlock。
每个CPU上有一个qnodes数组，该数组拥有4个qnode元素，分别用在task, softirq, hardirq, nmi上下文。

===== 操作
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/asm-generic/qspinlock.h
----
static __always_inline void queued_spin_lock(struct qspinlock *lock)
{
	int val = 0;

	if (likely(atomic_try_cmpxchg_acquire(&lock->val, &val, _Q_LOCKED_VAL)))
		return;

	queued_spin_lock_slowpath(lock, val);
}
//...
static __always_inline void queued_spin_unlock(struct qspinlock *lock)
{
	/*
	 * unlock() needs release semantics:
	 */
	smp_store_release(&lock->locked, 0);
}
----

[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/locking/qspinlock.c
----
/**
 * queued_spin_lock_slowpath - acquire the queued spinlock
 * @lock: Pointer to queued spinlock structure
 * @val: Current value of the queued spinlock 32-bit word
 *
 * (queue tail, pending bit, lock value)
 *
 *              fast     :    slow                                  :    unlock
 *                       :                                          :
 * uncontended  (0,0,0) -:--> (0,0,1) ------------------------------:--> (*,*,0)
 *                       :       | ^--------.------.             /  :
 *                       :       v           \      \            |  :
 * pending               :    (0,1,1) +--> (0,1,0)   \           |  :
 *                       :       | ^--'              |           |  :
 *                       :       v                   |           |  :
 * uncontended           :    (n,x,y) +--> (n,0,0) --'           |  :
 *   queue               :       | ^--'                          |  :
 *                       :       v                               |  :
 * contended             :    (*,x,y) +--> (*,0,0) ---> (*,0,1) -'  :
 *   queue               :         ^--'                             :
 */
void __lockfunc queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
{
	struct mcs_spinlock *prev, *next, *node;
	u32 old, tail;
	int idx;

	BUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));

	if (pv_enabled())
		goto pv_queue;

	if (virt_spin_lock(lock))
		return;

	/*
	 * Wait for in-progress pending->locked hand-overs with a bounded
	 * number of spins so that we guarantee forward progress.
	 *
	 * 0,1,0 -> 0,0,1
	 */
	if (val == _Q_PENDING_VAL) {
		int cnt = _Q_PENDING_LOOPS;
		val = atomic_cond_read_relaxed(&lock->val,
					       (VAL != _Q_PENDING_VAL) || !cnt--);
	}

	/*
	 * If we observe any contention; queue.
	 */
	if (val & ~_Q_LOCKED_MASK)
		goto queue;

	/*
	 * trylock || pending
	 *
	 * 0,0,* -> 0,1,* -> 0,0,1 pending, trylock
	 */
	val = queued_fetch_set_pending_acquire(lock);

	/*
	 * If we observe contention, there is a concurrent locker.
	 *
	 * Undo and queue; our setting of PENDING might have made the
	 * n,0,0 -> 0,0,0 transition fail and it will now be waiting
	 * on @next to become !NULL.
	 */
	if (unlikely(val & ~_Q_LOCKED_MASK)) {

		/* Undo PENDING if we set it. */
		if (!(val & _Q_PENDING_MASK))
			clear_pending(lock);

		goto queue;
	}

	/*
	 * We're pending, wait for the owner to go away.
	 *
	 * 0,1,1 -> *,1,0
	 *
	 * this wait loop must be a load-acquire such that we match the
	 * store-release that clears the locked bit and create lock
	 * sequentiality; this is because not all
	 * clear_pending_set_locked() implementations imply full
	 * barriers.
	 */
	if (val & _Q_LOCKED_MASK)
		smp_cond_load_acquire(&lock->locked, !VAL);

	/*
	 * take ownership and clear the pending bit.
	 *
	 * 0,1,0 -> 0,0,1
	 */
	clear_pending_set_locked(lock);
	lockevent_inc(lock_pending);
	return;

	/*
	 * End of pending bit optimistic spinning and beginning of MCS
	 * queuing.
	 */
queue:
	lockevent_inc(lock_slowpath);
pv_queue:
	node = this_cpu_ptr(&qnodes[0].mcs);
	idx = node->count++;
	tail = encode_tail(smp_processor_id(), idx);

	trace_contention_begin(lock, LCB_F_SPIN);

	/*
	 * 4 nodes are allocated based on the assumption that there will
	 * not be nested NMIs taking spinlocks. That may not be true in
	 * some architectures even though the chance of needing more than
	 * 4 nodes will still be extremely unlikely. When that happens,
	 * we fall back to spinning on the lock directly without using
	 * any MCS node. This is not the most elegant solution, but is
	 * simple enough.
	 */
	if (unlikely(idx >= MAX_NODES)) {
		lockevent_inc(lock_no_node);
		while (!queued_spin_trylock(lock))
			cpu_relax();
		goto release;
	}

	node = grab_mcs_node(node, idx);

	/*
	 * Keep counts of non-zero index values:
	 */
	lockevent_cond_inc(lock_use_node2 + idx - 1, idx);

	/*
	 * Ensure that we increment the head node->count before initialising
	 * the actual node. If the compiler is kind enough to reorder these
	 * stores, then an IRQ could overwrite our assignments.
	 */
	barrier();

	node->locked = 0;
	node->next = NULL;
	pv_init_node(node);

	/*
	 * We touched a (possibly) cold cacheline in the per-cpu queue node;
	 * attempt the trylock once more in the hope someone let go while we
	 * weren't watching.
	 */
	if (queued_spin_trylock(lock))
		goto release;

	/*
	 * Ensure that the initialisation of @node is complete before we
	 * publish the updated tail via xchg_tail() and potentially link
	 * @node into the waitqueue via WRITE_ONCE(prev->next, node) below.
	 */
	smp_wmb();

	/*
	 * Publish the updated tail.
	 * We have already touched the queueing cacheline; don't bother with
	 * pending stuff.
	 *
	 * p,*,* -> n,*,*
	 */
	old = xchg_tail(lock, tail);
	next = NULL;

	/*
	 * if there was a previous node; link it and wait until reaching the
	 * head of the waitqueue.
	 */
	if (old & _Q_TAIL_MASK) {
		prev = decode_tail(old);

		/* Link @node into the waitqueue. */
		WRITE_ONCE(prev->next, node);

		pv_wait_node(node, prev);
		arch_mcs_spin_lock_contended(&node->locked);

		/*
		 * While waiting for the MCS lock, the next pointer may have
		 * been set by another lock waiter. We optimistically load
		 * the next pointer & prefetch the cacheline for writing
		 * to reduce latency in the upcoming MCS unlock operation.
		 */
		next = READ_ONCE(node->next);
		if (next)
			prefetchw(next);
	}

	/*
	 * we're at the head of the waitqueue, wait for the owner & pending to
	 * go away.
	 *
	 * *,x,y -> *,0,0
	 *
	 * this wait loop must use a load-acquire such that we match the
	 * store-release that clears the locked bit and create lock
	 * sequentiality; this is because the set_locked() function below
	 * does not imply a full barrier.
	 *
	 * The PV pv_wait_head_or_lock function, if active, will acquire
	 * the lock and return a non-zero value. So we have to skip the
	 * atomic_cond_read_acquire() call. As the next PV queue head hasn't
	 * been designated yet, there is no way for the locked value to become
	 * _Q_SLOW_VAL. So both the set_locked() and the
	 * atomic_cmpxchg_relaxed() calls will be safe.
	 *
	 * If PV isn't active, 0 will be returned instead.
	 *
	 */
	if ((val = pv_wait_head_or_lock(lock, node)))
		goto locked;

	val = atomic_cond_read_acquire(&lock->val, !(VAL & _Q_LOCKED_PENDING_MASK));

locked:
	/*
	 * claim the lock:
	 *
	 * n,0,0 -> 0,0,1 : lock, uncontended
	 * *,*,0 -> *,*,1 : lock, contended
	 *
	 * If the queue head is the only one in the queue (lock value == tail)
	 * and nobody is pending, clear the tail code and grab the lock.
	 * Otherwise, we only need to grab the lock.
	 */

	/*
	 * In the PV case we might already have _Q_LOCKED_VAL set, because
	 * of lock stealing; therefore we must also allow:
	 *
	 * n,0,1 -> 0,0,1
	 *
	 * Note: at this point: (val & _Q_PENDING_MASK) == 0, because of the
	 *       above wait condition, therefore any concurrent setting of
	 *       PENDING will make the uncontended transition fail.
	 */
	if ((val & _Q_TAIL_MASK) == tail) {
		if (atomic_try_cmpxchg_relaxed(&lock->val, &val, _Q_LOCKED_VAL))
			goto release; /* No contention */
	}

	/*
	 * Either somebody is queued behind us or _Q_PENDING_VAL got set
	 * which will then detect the remaining tail and queue behind us
	 * ensuring we'll see a @next.
	 */
	set_locked(lock);

	/*
	 * contended path; wait for next if not observed yet, release.
	 */
	if (!next)
		next = smp_cond_load_relaxed(&node->next, (VAL));

	arch_mcs_spin_unlock_contended(&next->locked);
	pv_kick_node(lock, next);

release:
	trace_contention_end(lock, 0);

	/*
	 * release the node
	 */
	__this_cpu_dec(qnodes[0].mcs.count);
}
----

===== PV support
para-virtualization support to the qspinlock
https://elixir.bootlin.com/linux/latest/source/kernel/locking/qspinlock_paravirt.h

CONFIG_PARAVIRT_SPINLOCKS:
https://cateee.net/lkddb/web-lkddb/PARAVIRT_SPINLOCKS.html

参考:
https://lwn.net/Articles/641792/

===== 参考
https://lwn.net/Articles/561775/
https://bugzilla.kernel.org/show_bug.cgi?id=206115
http://www.wowotech.net/kernel_synchronization/queued_spinlock.html

==== osq_lock
OSQ Lock（Optimistic Spinning Queue Lock，乐观自旋队列锁）

▪ 作用
osq_lock采用了optimistic spinning自旋等待机制，类似MCS锁机制来实现的
用于mutex, rwsem(即rw_semaphore)等实现中(默认配置: CONFIG_MUTEX_SPIN_ON_OWNER=y, CONFIG_RWSEM_SPIN_ON_OWNER=y)。

▪ 结构
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/osq_lock.h
----
/*
 * An MCS like lock especially tailored for optimistic spinning for sleeping
 * lock implementations (mutex, rwsem, etc).
 */

struct optimistic_spin_queue {
	/*
	 * Stores an encoded value of the CPU # of the tail node in the queue.
	 * If the queue is empty, then it's set to OSQ_UNLOCKED_VAL.
	 */
	atomic_t tail;
};
----

▪ 实现
[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/locking/osq_lock.c
----
/*
 * An MCS like lock especially tailored for optimistic spinning for sleeping
 * lock implementations (mutex, rwsem, etc).
 *
 * Using a single mcs node per CPU is safe because sleeping locks should not be
 * called from interrupt context and we have preemption disabled while
 * spinning.
 */

struct optimistic_spin_node {
	struct optimistic_spin_node *next, *prev;
	int locked; /* 1 if lock acquired */
	int cpu; /* encoded CPU # + 1 value */
};

static DEFINE_PER_CPU_SHARED_ALIGNED(struct optimistic_spin_node, osq_node);
----
bool osq_lock(struct optimistic_spin_queue *lock):
void osq_unlock(struct optimistic_spin_queue *lock):
https://elixir.bootlin.com/linux/latest/source/kernel/locking/osq_lock.c

▪ 参考
https://github.com/freelancer-leon/notes/blob/master/kernel/lock/osq_lock.md
https://zhuanlan.zhihu.com/p/675770572
https://zhuanlan.zhihu.com/p/90508284

==== 参考
https://github.com/torvalds/linux/blob/master/Documentation/locking/spinlocks.rst
https://lwn.net/Kernel/Index/#Spinlocks

=== 信号量
==== 概念
信号量是一种睡眠锁，比自旋锁开销要大，适合需要长期持锁的场景。

信号量本质上是一个用于控制资源访问的整数计数器。需要访问的信号量代码必须首先减少计数器的值，但前提是计数器的值大于零；否则必须等待计数器的值增加。释放信号量其实就是增加计数器的值。
在 Linux 内核实现中，信号量的获取是通过调用down() （或它的一些变体之一）来实现的；如果信号量不可用，down()将等待直到其它线程将其释放。释放操作则被称为up()。在 Edsger Dijkstra 定义的经典文献中，这些操作被称为P()和V()。

注意:
信号量越来越少用了: https://lwn.net/Articles/928026/[The shrinking role of semaphores]

vs. mutex
mutex_unlock() 不能在中断上下文中调用，而 up() 可以。
https://lwn.net/ml/linux-kernel/20230331034209.GA12892@google.com/

==== 实现
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/semaphore.h
----
/* Please don't access any members of this structure directly */
struct semaphore {
	raw_spinlock_t		lock;
	unsigned int		count;
	struct list_head	wait_list;
};
//...
extern void down(struct semaphore *sem);
extern int __must_check down_interruptible(struct semaphore *sem);
extern int __must_check down_killable(struct semaphore *sem);
extern int __must_check down_trylock(struct semaphore *sem);
extern int __must_check down_timeout(struct semaphore *sem, long jiffies);
extern void up(struct semaphore *sem);
----

void down(struct semaphore *sem): acquire the semaphore
void up(struct semaphore *sem): release the semaphore

[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/locking/semaphore.c
----
/**
 * down - acquire the semaphore
 * @sem: the semaphore to be acquired
 *
 * Acquires the semaphore.  If no more tasks are allowed to acquire the
 * semaphore, calling this function will put the task to sleep until the
 * semaphore is released.
 *
 * Use of this function is deprecated, please use down_interruptible() or
 * down_killable() instead.
 */
void __sched down(struct semaphore *sem)
{
	unsigned long flags;

	might_sleep();
	raw_spin_lock_irqsave(&sem->lock, flags);
	if (likely(sem->count > 0))
		sem->count--;
	else
		__down(sem);
	raw_spin_unlock_irqrestore(&sem->lock, flags);
}
//...
/**
 * up - release the semaphore
 * @sem: the semaphore to release
 *
 * Release the semaphore.  Unlike mutexes, up() may be called from any
 * context and even by tasks which have never called down().
 */
void __sched up(struct semaphore *sem)
{
	unsigned long flags;

	raw_spin_lock_irqsave(&sem->lock, flags);
	if (likely(list_empty(&sem->wait_list)))
		sem->count++;
	else
		__up(sem);
	raw_spin_unlock_irqrestore(&sem->lock, flags);
}
----

==== rw_semaphore
rw_semaphore 是一种多读者单写者锁机制。

在非 PREEMPT_RT 内核上，实现是公平的，从而防止写入器饥饿。

rw_semaphore 默认遵循严格的所有者语义，但存在一些特殊用途的接口，允许非所有者的读者释放信号量。这些接口的工作方式与内核配置无关。

rw_semaphore 和 PREEMPT_RT
PREEMPT_RT 内核将 rw_semaphore 映射到单独的基于 rt_mutex 的实现，从而改变公平性:
由于 rw_semaphore 的写入者无法将其优先级授予多个读取者，因此被抢占的低优先级读取者将继续持有其锁，从而使高优先级写入者也处于饥饿状态。相反，由于读取者可以将其优先级授予写入者，因此被抢占的低优先级写入者的优先级将得到提升，直到其释放锁，从而防止该写入者导致读取者也处于饥饿状态。

[source, c]
----
/*
 * For an uncontended rwsem, count and owner are the only fields a task
 * needs to touch when acquiring the rwsem. So they are put next to each
 * other to increase the chance that they will share the same cacheline.
 *
 * In a contended rwsem, the owner is likely the most frequently accessed
 * field in the structure as the optimistic waiter that holds the osq lock
 * will spin on owner. For an embedded rwsem, other hot fields in the
 * containing structure should be moved further away from the rwsem to
 * reduce the chance that they will share the same cacheline causing
 * cacheline bouncing problem.
 */
struct rw_semaphore {
	atomic_long_t count;
	/*
	 * Write owner or one of the read owners as well flags regarding
	 * the current state of the rwsem. Can be used as a speculative
	 * check to see if the write owner is running on the cpu.
	 */
	atomic_long_t owner;
#ifdef CONFIG_RWSEM_SPIN_ON_OWNER
	struct optimistic_spin_queue osq; /* spinner MCS lock */
#endif
	raw_spinlock_t wait_lock;
	struct list_head wait_list;
#ifdef CONFIG_DEBUG_RWSEMS
	void *magic;
#endif
#ifdef CONFIG_DEBUG_LOCK_ALLOC
	struct lockdep_map	dep_map;
#endif
};
----

参考:
https://github.com/torvalds/linux/blob/master/Documentation/locking/locktypes.rst#rw_semaphore

==== 参考
https://lwn.net/Kernel/Index/#Semaphores
https://lwn.net/Kernel/Index/#Locking_mechanisms-Semaphores

=== 互斥锁
==== 经典互斥锁
===== 概念
当无法获得锁的时候，spinlock原地自旋，mutex则是挂起当前线程，进入阻塞状态。因此，mutex无法在中断上下文中使用。

互斥锁可以看作是0-1信号量，但它们在设计和用途上是有区别的。

===== 实现
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/mutex.h
----
#ifndef CONFIG_PREEMPT_RT

/*
 * Simple, straightforward mutexes with strict semantics:
 *
 * - only one task can hold the mutex at a time
 * - only the owner can unlock the mutex
 * - multiple unlocks are not permitted
 * - recursive locking is not permitted
 * - a mutex object must be initialized via the API
 * - a mutex object must not be initialized via memset or copying
 * - task may not exit with mutex held
 * - memory areas where held locks reside must not be freed
 * - held mutexes must not be reinitialized
 * - mutexes may not be used in hardware or software interrupt
 *   contexts such as tasklets and timers
 *
 * These semantics are fully enforced when DEBUG_MUTEXES is
 * enabled. Furthermore, besides enforcing the above rules, the mutex
 * debugging code also implements a number of additional features
 * that make lock debugging easier and faster:
 *
 * - uses symbolic names of mutexes, whenever they are printed in debug output
 * - point-of-acquire tracking, symbolic lookup of function names
 * - list of all locks held in the system, printout of them
 * - owner tracking
 * - detects self-recursing locks and prints out all relevant info
 * - detects multi-task circular deadlocks and prints out all affected
 *   locks and tasks (and only those tasks)
 */
struct mutex {
	atomic_long_t		owner;
	raw_spinlock_t		wait_lock;
#ifdef CONFIG_MUTEX_SPIN_ON_OWNER
	struct optimistic_spin_queue osq; /* Spinner MCS lock */
#endif
	struct list_head	wait_list;
#ifdef CONFIG_DEBUG_MUTEXES
	void			*magic;
#endif
#ifdef CONFIG_DEBUG_LOCK_ALLOC
	struct lockdep_map	dep_map;
#endif
};
//...
#else /* !CONFIG_PREEMPT_RT */
/*
 * Preempt-RT variant based on rtmutexes.
 */
#include <linux/rtmutex.h>

struct mutex {
	struct rt_mutex_base	rtmutex;
#ifdef CONFIG_DEBUG_LOCK_ALLOC
	struct lockdep_map	dep_map;
#endif
};
//...
#endif /* CONFIG_PREEMPT_RT */
----

===== 参考
https://github.com/torvalds/linux/blob/master/Documentation/locking/mutex-design.rst
https://lwn.net/Kernel/Index/#Locking_mechanisms-Mutexes
https://lwn.net/Articles/164802/
https://0xax.gitbooks.io/linux-insides/content/SyncPrim/linux-sync-4.html

==== 实时互斥锁(rtmutex)
===== 概念与原理
- 概念
since 5.15
配置: CONFIG_RT_MUTEXES=y
实时互斥锁实现了优先级继承(priority inheritance)，解决了优先级反转(priority inversion)的问题。

- 什么是优先级反转
低优先级的task获取到了CPU，称为优先级反转:
例如A，B，C三个task，优先级从高到低，当前运行C任务，A由于要获取C的某项资源(mutex)，由于资源被占用，此时A会进入睡眠等待资源释放，比A优先级低比C优先级高的B不需要获取资源，抢占了C，也抢占了优先级更高的A，获得了CPU。

- 如何解决优先级反转的问题
如果高优先级的任务阻塞在互斥量上，同时互斥量被低优先级的任务拥有，那么该低优先级的任务的动态优先级会提升到阻塞队列中的优先级最高之任务的优先级。
在上面的例子中，此时，C的动态优先级会提升到A，B就不再能够抢占C了。

- rtmutex与优先级继承
RT-mutex 是支持优先级继承 (PI) 的互斥锁。
由于抢占和中断禁用部分，PI 对非 PREEMPT_RT 内核有限制。
PI 显然无法抢占禁用抢占或禁用中断的代码区域，即使在 PREEMPT_RT 内核上也是如此。相反，PREEMPT_RT 内核会在可抢占任务上下文中执行大多数此类代码区域，尤其是中断处理程序和软中断。这种转换允许 spinlock_t 和 rwlock_t 通过 RT 互斥锁实现。
https://github.com/torvalds/linux/blob/master/Documentation/locking/locktypes.rst#rtmutex

===== 实现
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/rtmutex.h
----
struct rt_mutex_base {
	raw_spinlock_t		wait_lock;
	struct rb_root_cached   waiters;
	struct task_struct	*owner;
};
----
与经典互斥锁mutex不同的是，实时互斥锁的睡眠等待进程列表(waiters)是按优先级排序的struct rb_root_cached, 经典互斥锁则是struct list_head。

===== 参考
https://github.com/torvalds/linux/blob/master/Documentation/locking/rt-mutex.rst
https://github.com/torvalds/linux/blob/master/Documentation/locking/rt-mutex-design.rst

=== 大内核锁
BKL(Big Kernel Lock)从v2.6.39起被淘汰
2011年2月：Linux 2.6.38 版本中，最后一个依赖 BKL 的驱动程序被改写。从此，BKL 在通用内核中成为了历史。

接口:

	lock_kernel(): 获取大内核锁
	unlock_kernel(): 释放大内核锁
	kernel_locked(): 检查当前进程是否持有 BKL

https://elixir.bootlin.com/linux/v2.6.38/source/lib/kernel_lock.c

参考:
https://lwn.net/Kernel/Index/#Big_kernel_lock
https://en.wikipedia.org/wiki/Giant_lock

=== 读写锁
==== rwlock
也叫读写自旋锁

写饥饿问题:
如果读者很多，写者很难获取写锁，例如: 有一个reader占有读锁，然后writer申请写锁，writer需要自旋等待，此时另一个reader申请读锁，可以获取到读锁，如果有reader不停获取读锁，则会造成writer饿死。

现在的内核开发已经不建议再使用rwlock了。

https://elixir.bootlin.com/linux/latest/source/include/linux/rwlock.h

参考:
https://lwn.net/Articles/920224/
https://lore.kernel.org/lkml/87tu8eez22.fsf@email.froward.int.ebiederm.org/T/

==== 排队读写锁
v3.16

[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/asm-generic/qrwlock_types.h
----
typedef struct qrwlock {
	union {
		atomic_t cnts;
		struct {
#ifdef __LITTLE_ENDIAN
			u8 wlocked;	/* Locked for write? */
			u8 __lstate[3];
#else
			u8 __lstate[3];
			u8 wlocked;	/* Locked for write? */
#endif
		};
	};
	arch_spinlock_t		wait_lock;
} arch_rwlock_t;

#define	__ARCH_RW_LOCK_UNLOCKED {		\
	{ .cnts = ATOMIC_INIT(0), },		\
	.wait_lock = __ARCH_SPIN_LOCK_UNLOCKED,	\
}
----

▪ 参考:
https://lore.kernel.org/lkml/1373679249-27123-2-git-send-email-Waiman.Long@hp.com/

==== Seqlock顺序锁
▪ 历史与作用:
v2.6引入，writer优先。

▪ 主要思想:
sequence number的初始值是一个偶数。
writer持有spinlock时，sequence number的值将是一个奇数(sequence number+1), 释放后则又变成偶数(sequence number+1)。
reader在读取一个共享变量之前，需要先读取一下sequence number的值，如果为奇数，说明有writer正在修改这个变量，需要等待，直到sequence number变为偶数，才可以开始读取变量。
reader可以随时读，但可能需要多读几次。
writer只会被其他writer造成饥饿，不再被reader造成饥饿。

▪ 结构:
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/seqlock.h
----
/*
 * Sequential locks (seqlock_t)
 *
 * Sequence counters with an embedded spinlock for writer serialization
 * and non-preemptibility.
 *
 * For more info, see:
 *    - Comments on top of seqcount_t
 *    - Documentation/locking/seqlock.rst
 */
typedef struct {
	/*
	 * Make sure that readers don't starve writers on PREEMPT_RT: use
	 * seqcount_spinlock_t instead of seqcount_t. Check __SEQ_LOCK().
	 */
	seqcount_spinlock_t seqcount;
	spinlock_t lock;
} seqlock_t;
----

▪ 接口:

	read:
		do {
			seq = read_seqcount_begin(&foo_seqcount);
			... read-side critical section ...
		} while (read_seqcount_retry(&foo_seqcount, seq));

	write:
		write_seqcount_begin(&foo_seqcount);
		... write-side critical section ...
		write_seqcount_end(&foo_seqcount);

▪ 参考:
https://github.com/torvalds/linux/blob/master/Documentation/locking/seqlock.rst

=== RCU
==== 概念
read-copy-update
since 2.5.44
读-复制-更新: 其中的复制操作就是不同CPU上的读者复制了不同的数据值，或者说拥有同一个指针的不同拷贝值，也可以理解为: 在读者读取值的时候，写者复制并替换其内容。

RCU可以认为是一种经过改进的，高效且易扩展的读写锁:
Read-copy update (RCU) is a synchronization mechanism that was added to the Linux kernel in October of 2002. RCU achieves scalability improvements by allowing reads to occur concurrently with updates. In contrast with conventional locking primitives that ensure mutual exclusion among concurrent threads regardless of whether they be readers or updaters, or with reader-writer locks that allow concurrent reads but not in the presence of updates, RCU supports concurrency between a single updater and multiple readers. RCU ensures that reads are coherent by maintaining multiple versions of objects and ensuring that they are not freed up until all pre-existing read-side critical sections complete. RCU defines and uses efficient and scalable mechanisms for publishing and reading new versions of an object, and also for deferring the collection of old versions. These mechanisms distribute the work among read and update paths in such a way as to make read paths extremely fast. In some cases (non-preemptable kernels), RCU's read-side primitives have zero overhead.
RCU允许读操作可以与更新操作并发执行，提升了程序的可扩展性。常规的互斥锁让并发线程互斥执行，并不关心该线程是读者还是写者，而读/写锁在没有写者时允许并发的读者，相比于这些常规锁操作，RCU在维护对象的多个版本时确保读操作保持一致，同时保证只有所有当前读端临界区都执行完毕后才释放对象。RCU定义了高效且易于扩展的机制，用来发布和读取对象的新版本，还用于延后旧版本对象的垃圾收集工作。这些机制可以在读端和更新端并行工作，使得读端十分高效。在某些场合下(比如非抢占式内核里)，RCU读端完全是零开销。
参考:
https://lwn.net/Articles/262464/
https://lwn.net/Articles/263130/

主要适用于下面的场景:

    ▪ RCU只能保护动态分配的数据结构，并且必须是通过指针访问该数据结构
    ▪ 受RCU保护的临界区内不能sleep(SRCU?)
    ▪ writer对性能没有特别要求，reader对性能较高
    ▪ reader对新旧数据不敏感

应用示例:
Using RCU for linked lists — a case study: https://lwn.net/Articles/610972/
Lockless netlink_lookup() with new concurrent hash table: https://lkml.org/lkml/2014/8/2/35

同步原则:
reader如果读取这个节点的数据，得到的要么全是旧的数据，要么全是新的数据，而不会是半新半旧的数据。

与rwlock和seqlock的比较:
rwlock和seqlock的读与写是互斥的。
rwlock是reader优先，存在写饥饿问题。
seqlock是writer优先，writer操作时，reader需要循环等待。
RCU拥有更高(主要指读与写)的并行度，writer/updater更新时，reader读取旧的数据。

宽限期(grace period):
等待所有reader访问完成的时间成为宽限期。

Tiny RCU:
rcu有tiny rcu和tree rcu两种实现，tiny rcu更加简洁，通常用在小型嵌入式系统中。
参考: https://lwn.net/Articles/323929/

Sleepable RCU:
参考: https://lwn.net/Articles/202847/

==== 使用
RCU核心API设计比较精简，只有少数几个:

	rcu_read_lock()
	rcu_read_unlock()
	synchronize_rcu() / call_rcu()
	rcu_assign_pointer()
	rcu_dereference()

writer/updater:
(1) 在对copy的数据更新完成后，需要通过rcu_assign_pointer()，替换原节点并移除对原节点的引用
(2) 之后调用synchronize_rcu()或call_rcu()进入宽限期(grace period)。
synchronize_rcu()与call_rcu()的区别是:
synchronize_rcu()会阻塞等待，只能在进程上下文中使用；call_rcu()使用回调的形式，不会阻塞等待，可在中断上下文中使用。

reader:
(1) 调用rcu_read_lock()进入临界区后，所使用的节点可能被解除引用，需要通过rcu_dereference()保留一份对这个节点的指针指向的数据。
(2) 进入grace period意味着数据已经更新，在退出临界区之前，这些reader只能使用旧的数据。
(3) 之后这些reader都调用rcu_read_unlock()退出了临界区，宽限期(grace period)结束，原节点所在的内存区域被释放。

相关接口: https://elixir.bootlin.com/linux/latest/source/include/linux/rcupdate.h

==== 实现
RCU由三种基础机制组成，分别用于插入，删除，以及readers, 它们是:

- 1. Publish-Subscribe Mechanism (for insertion)
- 2. Wait For Pre-Existing RCU Readers to Complete (for deletion)
- 3. Maintain Multiple Versions of Recently Updated Objects (for readers)

===== Publish-Subscribe Mechanism
The rcu_dereference() primitive can thus be thought of as subscribing to a given value of the specified pointer, guaranteeing that subsequent dereference operations will see any initialization that occurred before the corresponding publish (rcu_assign_pointer()) operation. The rcu_read_lock() and rcu_read_unlock() calls are absolutely required: they define the extent of the RCU read-side critical section. Their purpose is explained in the next section, however, they never spin or block, nor do they prevent the list_add_rcu() from executing concurrently. In fact, in non-CONFIG_PREEMPT kernels, they generate absolutely no code.
rcu_dereference()可以认为是一种subscribing操作原语，rcu_assign_pointer()则可以看作是一种publish操作。
它们不会spin，也不会block，在没有配置non-CONFIG_PREEMPT的情况下，没有什么开销。
实际情况中往往会使用一些更高层次的API，例如list_add_rcu。

===== Wait For Pre-Existing RCU Readers to Complete
The following pseudocode shows the basic form of algorithms that use RCU to wait for readers:
step1. Make a change, for example, replace an element in a linked list.
step2. Wait for all pre-existing RCU read-side critical sections to completely finish (for example, by using the synchronize_rcu() primitive). The key observation here is that subsequent RCU read-side critical sections have no way to gain a reference to the newly removed element.
step3. Clean up, for example, free the element that was replaced above.
参考: http://www.wowotech.net/kernel_synchronization/223.html

=====  Maintain Multiple Versions of Recently Updated Objects
- Example 1: Maintaining Multiple Versions During Deletion

    1 p = search(head, key);
    2 if (p != NULL) {
    3   list_del_rcu(&p->list);
    4   synchronize_rcu();
    5   kfree(p);
    6 }

- Example 2: Maintaining Multiple Versions During Replacement

    1 q = kmalloc(sizeof(*p), GFP_KERNEL);
    2 *q = *p;
    3 q->b = 2;
    4 q->c = 3;
    5 list_replace_rcu(&p->list, &q->list);
    6 synchronize_rcu();
    7 kfree(p);

参考: https://lwn.net/Articles/262464/

===== 代码
https://elixir.bootlin.com/linux/latest/source/include/linux/rculist.h
https://elixir.bootlin.com/linux/latest/source/include/linux/rcutree.h

==== 参考
https://github.com/torvalds/linux/tree/master/Documentation/RCU
https://www.kernel.org/doc/html/latest/RCU/index.html
https://lwn.net/Kernel/Index/#Read-copy-update
https://lwn.net/Archives/GuestIndex/#McKenney_Paul_E.
《Is Parallel Programming Hard, And, If So, What Can You Do About It》9.5
http://www.rdrop.com/users/paulmck/RCU/RCUdissertation.2004.07.14e1.pdf
https://docs.google.com/document/d/1X0lThx8OK0ZgLMqVoXiR4ZrGURHrXK6NyLRbeXe3Xac/
https://hackmd.io/@sysprog/linux-rcu?type=view
https://en.wikipedia.org/wiki/Read-copy-update
https://lore.kernel.org/rcu/

=== per-cpu变量
==== 概念
per-cpu变量是为系统中的每一个“逻辑 CPU”分配一个独立的副本。

per-cpu变量可以静态分配(分为内核中的静态percpu变量和模块中的静态percpu变量)，也可以动态分配。

vs. per-thread
https://stackoverflow.com/questions/65964604/how-to-use-thread-local-storage-in-linux-kernel-module
Quick Quiz 4.34: What do you do if you need a per-thread (not per-CPU!) variable in the Linux kernel?
参考:
perfbook: 4.3.6 Per-CPU Variables

==== 数据结构
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/percpu.h
----
struct pcpu_group_info {
	int			nr_units;	/* aligned # of units */
	unsigned long		base_offset;	/* base address offset */
	unsigned int		*cpu_map;	/* unit->cpu map, empty
						 * entries contain NR_CPUS */
};

struct pcpu_alloc_info {
	size_t			static_size;
	size_t			reserved_size;
	size_t			dyn_size;
	size_t			unit_size;
	size_t			atom_size;
	size_t			alloc_size;
	size_t			__ai_size;	/* internal, don't use */
	int			nr_groups;	/* 0 if grouping unnecessary */
	struct pcpu_group_info	groups[];
};
----

[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/percpu-internal.h
----
/*
 * pcpu_block_md is the metadata block struct.
 * Each chunk's bitmap is split into a number of full blocks.
 * All units are in terms of bits.
 *
 * The scan hint is the largest known contiguous area before the contig hint.
 * It is not necessarily the actual largest contig hint though.  There is an
 * invariant that the scan_hint_start > contig_hint_start iff
 * scan_hint == contig_hint.  This is necessary because when scanning forward,
 * we don't know if a new contig hint would be better than the current one.
 */
struct pcpu_block_md {
	int			scan_hint;	/* scan hint for block */
	int			scan_hint_start; /* block relative starting
						    position of the scan hint */
	int                     contig_hint;    /* contig hint for block */
	int                     contig_hint_start; /* block relative starting
						      position of the contig hint */
	int                     left_free;      /* size of free space along
						   the left side of the block */
	int                     right_free;     /* size of free space along
						   the right side of the block */
	int                     first_free;     /* block position of first free */
	int			nr_bits;	/* total bits responsible for */
};

struct pcpu_chunk {
#ifdef CONFIG_PERCPU_STATS
	int			nr_alloc;	/* # of allocations */
	size_t			max_alloc_size; /* largest allocation size */
#endif

	struct list_head	list;		/* linked to pcpu_slot lists */
	int			free_bytes;	/* free bytes in the chunk */
	struct pcpu_block_md	chunk_md;
	void			*base_addr;	/* base address of this chunk */

	unsigned long		*alloc_map;	/* allocation map */
	unsigned long		*bound_map;	/* boundary map */
	struct pcpu_block_md	*md_blocks;	/* metadata blocks */

	void			*data;		/* chunk data */
	bool			immutable;	/* no [de]population allowed */
	bool			isolated;	/* isolated from active chunk
						   slots */
	int			start_offset;	/* the overlap with the previous
						   region to have a page aligned
						   base_addr */
	int			end_offset;	/* additional area required to
						   have the region end page
						   aligned */
#ifdef CONFIG_MEMCG_KMEM
	struct obj_cgroup	**obj_cgroups;	/* vector of object cgroups */
#endif

	int			nr_pages;	/* # of pages served by this chunk */
	int			nr_populated;	/* # of populated pages */
	int                     nr_empty_pop_pages; /* # of empty populated pages */
	unsigned long		populated[];	/* populated bitmap */
};
----

==== 类型
类型                    定义方式                  存放段/区域       内存分配时间
内核静态per-cpu变量    DEFINE_PER_CPU            .data..percpu     系统引导初始化期间
模块静态per-cpu变量    模块内的DEFINE_PER_CPU     Reserved Chunk    模块加载（insmod）期间
动态per-cpu变量        alloc_percpu()            Dynamic Chunk     运行期间由代码指定

==== 注意事项
Q: per-cpu变量存储在哪里？
A: https://stackoverflow.com/questions/53968755/where-are-declare-per-cpu-variables-stored-in-kernel
https://zhuanlan.zhihu.com/p/260986194

https://stackoverflow.com/questions/16978959/how-are-percpu-pointers-implemented-in-the-linux-kernel

https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/percpu.h

==== 参考
https://lwn.net/Kernel/Index/#Per-CPU_variables
https://zhuanlan.zhihu.com/p/260986194

=== futex
==== 概念
fast userspace mutexes
The futex mechanism, introduced in 2.5.7 by Rusty Russell, Hubertus Franke, and Mathew Kirkwood, is a fast, lightweight kernel-assisted locking primitive for user-space applications. It provides for very fast uncontended lock acquisition and release. The futex state is stored in a user-space variable (an unsigned 32-bit integer on all platforms). Atomic operations are used in order to change the state of the futex in the uncontended case without the overhead of a syscall. In the contended cases, the kernel is invoked to put tasks to sleep and wake them up.
Futexes are the basis of several mutual exclusion constructs commonly used in threaded programming. These include pthread mutexes, condvars, semaphores, rwlocks, and barriers. They have been through a lot of reconstructive and cosmetic surgery over the last several years, and are now more efficient, more functional, and better documented than ever before.
参考: https://lwn.net/Articles/360699/

https://en.wikipedia.org/wiki/Futex
https://man7.org/linux/man-pages/man2/futex.2.html

==== 实现
===== core
https://elixir.bootlin.com/linux/latest/source/kernel/futex/futex.h

https://elixir.bootlin.com/linux/latest/source/kernel/futex/core.c

===== FUTEX_REQUEUE
https://elixir.bootlin.com/linux/latest/source/kernel/futex/requeue.c

- 惊群效应
有一群线程（线程B1, B2, B3...）都在一个条件变量上等待某个资源。当另一个线程（线程A）释放资源并发出信号（pthread_cond_broadcast）时，传统的简单实现会唤醒所有等待的线程。它们会全部从睡眠中醒来，争抢锁，但最终只有一个线程能获得资源，其他线程发现自己白忙活一场，又得回去继续睡眠，这个过程产生了大量无意义的上下文切换、锁竞争和CPU消耗，严重影响性能。

- FUTEX_REQUEUE 如何解决这个问题
FUTEX_REQUEUE 操作通过一个 “重定向” 动作来优雅地解决此问题。它通常在一次系统调用中完成两个原子操作：
* 从等待队列A唤醒少量线程（通常为1个）。
* 将等待队列A中剩余的线程，整体“搬家”到另一个Futex（通常是关联的互斥锁）的等待队列B上。

- 典型应用: pthread_cond_broadcast / pthread_cond_signal
以条件变量（cond）和互斥锁（mutex）的配合为例：
* 等待侧 (pthread_cond_wait)：
    ** 线程先释放 mutex。
    ** 然后在条件变量 cond 的Futex上进入等待。
* 信号侧 (pthread_cond_signal 或 pthread_cond_broadcast)：
    ** pthread_cond_signal (唤醒一个)： 理想情况是只唤醒一个在 cond 上等待的线程。但唤醒后，该线程需要重新获取 mutex。如果此时 mutex 被其他线程持有，被唤醒的线程又会阻塞在 mutex 的Futex上。这导致了两次唤醒/阻塞（cond -> mutex）。
    ** pthread_cond_broadcast (唤醒所有)： 惊群效应，性能更差。

- 使用 FUTEX_REQUEUE 后，pthread_cond_signal 的优化流程如下：
* 线程A调用 pthread_cond_signal。
* 内核的 futex_requeue 函数被调用，参数是：
    ** uaddr1: 条件变量 cond 的地址。
    ** uaddr2: 互斥锁 mutex 的地址。
    ** nr_wake: 1 （只唤醒一个线程）。
    ** nr_requeue: 一个很大的数或当前等待数（将剩下的全搬走）。
* 内核原子地执行：
    ** 从 cond 的等待队列中，唤醒1个线程。
    ** 将 cond 队列里所有其他等待线程，移动到 mutex 的等待队列末尾。
* 结果：
    ** 被唤醒的1个线程将去尝试获取 mutex。
    ** 其他线程现在不是在 cond 上等待，而是在 mutex 上等待。当 mutex 的持有者释放它时，这些线程会按顺序被唤醒。
    ** 如此，避免了惊群：只有一个线程从 cond 上被真正唤醒。其他线程的等待状态被无缝转移到了它们接下来必然要竞争的 mutex 上，等待流程更自然，减少了不必要的唤醒。

===== Priority Inheritance
https://elixir.bootlin.com/linux/latest/source/kernel/futex/pi.c
处理优先级继承(Priority Inheritance, PI)的futex操作的源文件。它实现了FUTEX_LOCK_PI、FUTEX_TRYLOCK_PI、FUTEX_UNLOCK_PI和FUTEX_CMP_REQUEUE_PI等操作。

==== 参考
https://github.com/torvalds/linux/blob/master/Documentation/locking/futex-requeue-pi.rst
https://github.com/torvalds/linux/blob/master/Documentation/locking/pi-futex.rst
https://github.com/torvalds/linux/blob/master/Documentation/locking/robust-futex-ABI.rst
https://github.com/torvalds/linux/blob/master/Documentation/locking/robust-futexes.rst
https://lwn.net/Kernel/Index/#Futex
https://www.kernel.org/doc/ols/2002/ols2002-pages-479-495.pdf
https://mp.weixin.qq.com/s/X4NagwyfgazAmm0A6ndKpg
https://kernel.meizu.com/2024/03/15/Futex%E6%9C%BA%E5%88%B6%E7%9A%84%E5%86%85%E6%A0%B8%E4%BC%98%E5%8C%96/

=== 锁与抢占
https://github.com/torvalds/linux/blob/master/Documentation/locking/preempt-locking.rst

=== 参考
https://github.com/torvalds/linux/tree/master/Documentation/locking
https://docs.kernel.org/locking/index.html
https://lwn.net/Kernel/Index/#Locking_mechanisms
https://lwn.net/Kernel/Index/#lock_kernel
https://lwn.net/Kernel/Index/#Lockless_algorithms
https://lwn.net/Kernel/Index/#Race_conditions
https://github.com/freelancer-leon/notes#锁
https://github.com/0xAX/linux-insides/tree/master/SyncPrim
https://mirrors.edge.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook.html[Is Parallel Programming Hard, And, If So, What Can You Do About It?]