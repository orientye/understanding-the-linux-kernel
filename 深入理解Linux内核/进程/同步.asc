:toc:
:toclevels: 5
:hardbreaks-option:

== 同步

=== 概念
内核中可能造成并发执行的原因:

    中断
    内核抢占
    睡眠及调度
    多处理器

锁类型:

    Sleeping locks
    CPU local locks
    Spinning locks
    https://github.com/torvalds/linux/blob/master/Documentation/locking/locktypes.rst

=== 原子操作

==== 整数
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/types.h
----
typedef struct {
	int counter;
} atomic_t;

#define ATOMIC_INIT(i) { (i) }

#ifdef CONFIG_64BIT
typedef struct {
	s64 counter;
} atomic64_t;
#endif
----

操作: https://elixir.bootlin.com/linux/latest/source/include/asm-generic/atomic.h

x86:
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/atomic.h

ARM64:
https://elixir.bootlin.com/linux/latest/source/arch/arm64/include/asm/atomic.h
https://elixir.bootlin.com/linux/latest/source/arch/arm64/include/asm/atomic_ll_sc.h
https://elixir.bootlin.com/linux/latest/source/arch/arm64/include/asm/atomic_lse.h

==== 位操作
x86:
https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/bitops.h

ARM:
https://elixir.bootlin.com/linux/latest/source/arch/arm/include/asm/bitops.h

==== 参考
https://lwn.net/Kernel/Index/#Atomic_operations
https://lwn.net/Kernel/Index/#atomic_t
https://lwn.net/Kernel/Index/#C11_atomic_operations
https://lwn.net/Kernel/Index/#Atomic_spinlocks
https://lwn.net/Kernel/Index/#Atomic_IO_operations
https://lwn.net/Kernel/Index/#ACCESS_ONCE

=== 自旋锁

==== 概念
Q: 什么情况下使用

Q: 为什么要关闭中断?
A: https://stackoverflow.com/questions/37146637/why-is-interrupt-disabled-between-spin-lock-and-spin-unlock-in-linux

Q: 为什么要关闭抢占？

Q: 为什么自旋锁保护的代码不能进入睡眠状态？

spin_lock()与spin_unlock() — 禁止内核抢占
spin_lock_irq()与spin_unlock_irq() — 禁止内核抢占并屏蔽中断
spin_lock_irqsave()与spin_unlock_irqrestore() — 禁止内核抢占并屏蔽中断，事先保存中断屏蔽位并事后恢复原状

==== raw spinlock

==== ticket spinlock
https://en.wikipedia.org/wiki/Ticket_lock
主要解决公平性

==== mcs spinlock
per-CPU structure, 在cache-line上效率更高
https://lwn.net/Articles/590243/

==== queue spinlock
基于mcs spinlock的思想但解决了mcs spinlock占用空间大的问题

[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/asm-generic/qspinlock_types.h
----
typedef struct qspinlock {
	union {
		atomic_t val;

		/*
		 * By using the whole 2nd least significant byte for the
		 * pending bit, we can allow better optimization of the lock
		 * acquisition for the pending bit holder.
		 */
#ifdef __LITTLE_ENDIAN
		struct {
			u8	locked;
			u8	pending;
		};
		struct {
			u16	locked_pending;
			u16	tail;
		};
#else
		struct {
			u16	tail;
			u16	locked_pending;
		};
		struct {
			u8	reserved[2];
			u8	pending;
			u8	locked;
		};
#endif
	};
} arch_spinlock_t;
----

https://lwn.net/Articles/561775/
https://0xax.gitbooks.io/linux-insides/content/SyncPrim/linux-sync-2.html

=== Seqlock顺序锁
v2.6引入, 对写友好, 写总能成功。

https://elixir.bootlin.com/linux/latest/source/include/linux/seqlock.h

sequence number的初始值是一个偶数。
writer持有spinlock时，sequence number的值将是一个奇数(sequence number+1), 释放后则又变成偶数(sequence number+1)。
reader在读取一个共享变量之前, 需要先读取一下sequence number的值，如果为奇数，说明有writer正在修改这个变量，需要等待，直到sequence number变为偶数，才可以开始读取变量。
reader可以随时读, 但可能需要多读几次。
writer只会被其他writer造成饥饿，不再被reader造成饥饿。

[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/seqlock.h
----
/*
 * Sequential locks (seqlock_t)
 *
 * Sequence counters with an embedded spinlock for writer serialization
 * and non-preemptibility.
 *
 * For more info, see:
 *    - Comments on top of seqcount_t
 *    - Documentation/locking/seqlock.rst
 */
typedef struct {
	/*
	 * Make sure that readers don't starve writers on PREEMPT_RT: use
	 * seqcount_spinlock_t instead of seqcount_t. Check __SEQ_LOCK().
	 */
	seqcount_spinlock_t seqcount;
	spinlock_t lock;
} seqlock_t;
----

=== RCU
==== 概念
主要适用于下面的场景:

    ▪ RCU只能保护动态分配的数据结构, 并且必须是通过指针访问该数据结构
    ▪ 受RCU保护的临界区内不能sleep(SRCU?)
    ▪ 读写不对称，对writer的性能没有特别要求, 但是reader性能要求极高
    ▪ reader对新旧数据不敏感

==== 参考
https://www.kernel.org/doc/html/latest/RCU/index.html
https://lwn.net/Kernel/Index/#Read-copy-update
https://lwn.net/Archives/GuestIndex/#McKenney_Paul_E.
https://hackmd.io/@sysprog/linux-rcu?type=view

=== barrier
==== 概念
考虑如下情形：
（1）编译器在编译程序的过程中，对代码会进行调整；
（2）CPU在执行指令的过程中，对指令会进行重排。

显然，有时候，这些优化并不符合我们的预期，那么如何防止这两种情况的发生呢？这就需要compiler barrier和memory barrier。

==== compiler barrier
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/compiler.h
----
/* Optimization barrier */
#ifndef barrier
/* The "volatile" is due to gcc bugs */
# define barrier() __asm__ __volatile__("": : :"memory")
#endif

#ifndef barrier_data
/*
 * This version is i.e. to prevent dead stores elimination on @ptr
 * where gcc and llvm may behave differently when otherwise using
 * normal barrier(): while gcc behavior gets along with a normal
 * barrier(), llvm needs an explicit input variable to be assumed
 * clobbered. The issue is as follows: while the inline asm might
 * access any memory it wants, the compiler could have fit all of
 * @ptr into memory registers instead, and since @ptr never escaped
 * from that, it proved that the inline asm wasn't touching any of
 * it. This version works well with both compilers, i.e. we're telling
 * the compiler that the inline asm absolutely may see the contents
 * of @ptr. See also: https://llvm.org/bugs/show_bug.cgi?id=15495
 */
# define barrier_data(ptr) __asm__ __volatile__("": :"r"(ptr) :"memory")
#endif
----

==== memory order
CPU在什么情况下会reorder呢？

首先，有前后有依赖的指令，CPU一般不会reorder(Alpha架构除外)。
例如: a = 5; b = a + 1; 这两条指令存在依赖关系，不会被cpu重排顺序。

对于那些没有这种依赖关系的指令，CPU就有可能对这些指令进行重排(除非使用memory barrier进行一些显示控制)，具体的力度则与CPU体系结构相关:

x86是一种strong order(也叫TSO，total store order)
同一CPU执行的load指令后接load指令(L-L)，store指令后接store指令(S-S)，load指令后接store指令(L-S)，均不能交换指令的执行顺序，仅store指令后接load指令(S-L)才可以。

ARM则是一种weak order: 
只要没有依赖关系，load指令和store指令就可任意交换。

==== memory barrier
memory barrier就像一个栅栏一样，隔开了在其前面和后面的指令。
"barrier" 前面的指令不能与后面的指令进行调换；
如果指令均处在其前面，或者均处在其后面，只要没有违反当前CPU的memory order的规则，则是可以调换的。

memory barrier约束了CPU的行为，同时也约束了编译器的行为，即memory barrier也隐含了compiler barrier语义。

实现:
https://elixir.bootlin.com/linux/latest/source/include/asm-generic/barrier.h

▪ X86
[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/barrier.h
----
/*
 * Force strict CPU ordering.
 * And yes, this might be required on UP too when we're talking
 * to devices.
 */

#ifdef CONFIG_X86_32
#define mb() asm volatile(ALTERNATIVE("lock; addl $0,-4(%%esp)", "mfence", \
				      X86_FEATURE_XMM2) ::: "memory", "cc")
#define rmb() asm volatile(ALTERNATIVE("lock; addl $0,-4(%%esp)", "lfence", \
				       X86_FEATURE_XMM2) ::: "memory", "cc")
#define wmb() asm volatile(ALTERNATIVE("lock; addl $0,-4(%%esp)", "sfence", \
				       X86_FEATURE_XMM2) ::: "memory", "cc")
#else
#define __mb()	asm volatile("mfence":::"memory")
#define __rmb()	asm volatile("lfence":::"memory")
#define __wmb()	asm volatile("sfence" ::: "memory")
#endif

/**
 * array_index_mask_nospec() - generate a mask that is ~0UL when the
 * 	bounds check succeeds and 0 otherwise
 * @index: array element index
 * @size: number of elements in array
 *
 * Returns:
 *     0 - (index < size)
 */
static inline unsigned long array_index_mask_nospec(unsigned long index,
		unsigned long size)
{
	unsigned long mask;

	asm volatile ("cmp %1,%2; sbb %0,%0;"
			:"=r" (mask)
			:"g"(size),"r" (index)
			:"cc");
	return mask;
}

/* Override the default implementation from linux/nospec.h. */
#define array_index_mask_nospec array_index_mask_nospec

/* Prevent speculative execution past this barrier. */
#define barrier_nospec() alternative("", "lfence", X86_FEATURE_LFENCE_RDTSC)

#define __dma_rmb()	barrier()
#define __dma_wmb()	barrier()

#define __smp_mb()	asm volatile("lock; addl $0,-4(%%" _ASM_SP ")" ::: "memory", "cc")

#define __smp_rmb()	dma_rmb()
#define __smp_wmb()	barrier()
#define __smp_store_mb(var, value) do { (void)xchg(&var, value); } while (0)

#define __smp_store_release(p, v)					\
do {									\
	compiletime_assert_atomic_type(*p);				\
	barrier();							\
	WRITE_ONCE(*p, v);						\
} while (0)

#define __smp_load_acquire(p)						\
({									\
	typeof(*p) ___p1 = READ_ONCE(*p);				\
	compiletime_assert_atomic_type(*p);				\
	barrier();							\
	___p1;								\
})

/* Atomic operations are already serializing on x86 */
#define __smp_mb__before_atomic()	do { } while (0)
#define __smp_mb__after_atomic()	do { } while (0)

#include <asm-generic/barrier.h>

/*
 * Make previous memory operations globally visible before
 * a WRMSR.
 *
 * MFENCE makes writes visible, but only affects load/store
 * instructions.  WRMSR is unfortunately not a load/store
 * instruction and is unaffected by MFENCE.  The LFENCE ensures
 * that the WRMSR is not reordered.
 *
 * Most WRMSRs are full serializing instructions themselves and
 * do not require this barrier.  This is only required for the
 * IA32_TSC_DEADLINE and X2APIC MSRs.
 */
static inline void weak_wrmsr_fence(void)
{
	asm volatile("mfence; lfence" : : : "memory");
}
----

▪ ARM
[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/arm/include/asm/barrier.h
----
#if __LINUX_ARM_ARCH__ >= 7 ||		\
	(__LINUX_ARM_ARCH__ == 6 && defined(CONFIG_CPU_32v6K))
#define sev()	__asm__ __volatile__ ("sev" : : : "memory")
#define wfe()	__asm__ __volatile__ ("wfe" : : : "memory")
#define wfi()	__asm__ __volatile__ ("wfi" : : : "memory")
#else
#define wfe()	do { } while (0)
#endif

#if __LINUX_ARM_ARCH__ >= 7
#define isb(option) __asm__ __volatile__ ("isb " #option : : : "memory")
#define dsb(option) __asm__ __volatile__ ("dsb " #option : : : "memory")
#define dmb(option) __asm__ __volatile__ ("dmb " #option : : : "memory")
#ifdef CONFIG_THUMB2_KERNEL
#define CSDB	".inst.w 0xf3af8014"
#else
#define CSDB	".inst	0xe320f014"
#endif
#define csdb() __asm__ __volatile__(CSDB : : : "memory")
#elif defined(CONFIG_CPU_XSC3) || __LINUX_ARM_ARCH__ == 6
#define isb(x) __asm__ __volatile__ ("mcr p15, 0, %0, c7, c5, 4" \
				    : : "r" (0) : "memory")
#define dsb(x) __asm__ __volatile__ ("mcr p15, 0, %0, c7, c10, 4" \
				    : : "r" (0) : "memory")
#define dmb(x) __asm__ __volatile__ ("mcr p15, 0, %0, c7, c10, 5" \
				    : : "r" (0) : "memory")
#elif defined(CONFIG_CPU_FA526)
#define isb(x) __asm__ __volatile__ ("mcr p15, 0, %0, c7, c5, 4" \
				    : : "r" (0) : "memory")
#define dsb(x) __asm__ __volatile__ ("mcr p15, 0, %0, c7, c10, 4" \
				    : : "r" (0) : "memory")
#define dmb(x) __asm__ __volatile__ ("" : : : "memory")
#else
#define isb(x) __asm__ __volatile__ ("" : : : "memory")
#define dsb(x) __asm__ __volatile__ ("mcr p15, 0, %0, c7, c10, 4" \
				    : : "r" (0) : "memory")
#define dmb(x) __asm__ __volatile__ ("" : : : "memory")
#endif

#ifndef CSDB
#define CSDB
#endif
#ifndef csdb
#define csdb()
#endif

#ifdef CONFIG_ARM_HEAVY_MB
extern void (*soc_mb)(void);
extern void arm_heavy_mb(void);
#define __arm_heavy_mb(x...) do { dsb(x); arm_heavy_mb(); } while (0)
#else
#define __arm_heavy_mb(x...) dsb(x)
#endif

#if defined(CONFIG_ARM_DMA_MEM_BUFFERABLE) || defined(CONFIG_SMP)
#define mb()		__arm_heavy_mb()
#define rmb()		dsb()
#define wmb()		__arm_heavy_mb(st)
#define dma_rmb()	dmb(osh)
#define dma_wmb()	dmb(oshst)
#else
#define mb()		barrier()
#define rmb()		barrier()
#define wmb()		barrier()
#define dma_rmb()	barrier()
#define dma_wmb()	barrier()
#endif

#define __smp_mb()	dmb(ish)
#define __smp_rmb()	__smp_mb()
#define __smp_wmb()	dmb(ishst)

#ifdef CONFIG_CPU_SPECTRE
static inline unsigned long array_index_mask_nospec(unsigned long idx,
						    unsigned long sz)
{
	unsigned long mask;

	asm volatile(
		"cmp	%1, %2\n"
	"	sbc	%0, %1, %1\n"
	CSDB
	: "=r" (mask)
	: "r" (idx), "Ir" (sz)
	: "cc");

	return mask;
}
#define array_index_mask_nospec array_index_mask_nospec
#endif

#include <asm-generic/barrier.h>
----

==== 参考
https://github.com/torvalds/linux/blob/master/Documentation/memory-barriers.txt
https://lwn.net/Kernel/Index/#Memory_barriers
https://developer.arm.com/documentation/100941/0101/Barriers
https://zhuanlan.zhihu.com/p/96001570
https://mp.weixin.qq.com/s?__biz=MzAxMDM0NjExNA==&mid=2247487950&idx=1&sn=c6cb416efc2831c5666a5ae9a205dcf3&chksm=9b509a23ac2713352a6998b9ff34f29b02cc57a0cfe62bcc13a8aa3583c5e78d2d038aacf17c&mpshare=1&scene=1&srcid=0827lWWERv8vkNak5sANCSAc&sharer_sharetime=1667631485956&sharer_shareid=3d769dc0d221507247ffa710c89a2772&exportkey=n_ChQIAhIQ8I4w6FfIvqlzaen82GNKVxKZAgIE97dBBAEAAAAAAFWOLeiWGmcAAAAOpnltbLcz9gKNyK89dVj0hC%2FkJy43MLPqIOkmygOzvgdnTEDr65XdDo6dZ7SwCefoAImebvkCfI61eSTYahvaCP06FEy0n7UJvX9u%2FYIz1JS9%2BCCA7d5%2FKyMAjXkuHHt86AbasiIxztU5VUIa9lMcGL9zwaT6JlgIkO%2FIk6YccnxLN2UKLxcaBqJJeySCIuPYpjDSxg56oh3IgIsMMvIhwN96Lxu3LjEzNZIhOyr%2B88hqFYIZTFtAul%2BgftiyMUHcrW3Jhlg8obgZBYXDR1BC%2BgtBQQhATMXarg893oVKs7fJNkN69Ckjn23rqVgibQjnuMaNadco8Fw2bkVzDaCUkU2Z&acctmode=0&pass_ticket=QA7RhEsMegR3%2FRIcRqL6%2FVpdMlpaht0VwPJpMgvvdoPO%2FeAeWFcoxw0fS41vu6Qx&wx_header=0#rd

=== 读写锁
也叫读写自旋锁
写饥饿问题
现在的内核开发已经不建议再使用rwlock了, 之前使用到的rwlock也在逐渐被移除或者替换为普通的spinlock或者RCU

=== 信号量
==== 概念
信号量是一种睡眠锁，比自旋锁开销要大，适合需要长期持锁的场景。

注意：
信号量越来越少用了: https://lwn.net/Articles/928026/[The shrinking role of semaphores]

vs. mutex
https://lwn.net/ml/linux-kernel/20230331034209.GA12892@google.com/

==== 实现
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/semaphore.h
----
/* Please don't access any members of this structure directly */
struct semaphore {
	raw_spinlock_t		lock;
	unsigned int		count;
	struct list_head	wait_list;
};
//...
extern void down(struct semaphore *sem);
extern int __must_check down_interruptible(struct semaphore *sem);
extern int __must_check down_killable(struct semaphore *sem);
extern int __must_check down_trylock(struct semaphore *sem);
extern int __must_check down_timeout(struct semaphore *sem, long jiffies);
extern void up(struct semaphore *sem);
----

void down(struct semaphore *sem): acquire the semaphore
void up(struct semaphore *sem): release the semaphore

[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/locking/semaphore.c
----
/**
 * down - acquire the semaphore
 * @sem: the semaphore to be acquired
 *
 * Acquires the semaphore.  If no more tasks are allowed to acquire the
 * semaphore, calling this function will put the task to sleep until the
 * semaphore is released.
 *
 * Use of this function is deprecated, please use down_interruptible() or
 * down_killable() instead.
 */
void __sched down(struct semaphore *sem)
{
	unsigned long flags;

	might_sleep();
	raw_spin_lock_irqsave(&sem->lock, flags);
	if (likely(sem->count > 0))
		sem->count--;
	else
		__down(sem);
	raw_spin_unlock_irqrestore(&sem->lock, flags);
}
//...
/**
 * up - release the semaphore
 * @sem: the semaphore to release
 *
 * Release the semaphore.  Unlike mutexes, up() may be called from any
 * context and even by tasks which have never called down().
 */
void __sched up(struct semaphore *sem)
{
	unsigned long flags;

	raw_spin_lock_irqsave(&sem->lock, flags);
	if (likely(list_empty(&sem->wait_list)))
		sem->count++;
	else
		__up(sem);
	raw_spin_unlock_irqrestore(&sem->lock, flags);
}
----

==== 参考
https://lwn.net/Kernel/Index/#Semaphores

=== 互斥锁
==== 概念
当无法获得锁的时候，spinlock原地自旋，mutex挂起当前线程，进入阻塞状态。因此，mutex无法在中断上下文中使用。

互斥锁可以看作是0-1信号量。

==== 经典互斥锁
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/mutex.h
----
#ifndef CONFIG_PREEMPT_RT

/*
 * Simple, straightforward mutexes with strict semantics:
 *
 * - only one task can hold the mutex at a time
 * - only the owner can unlock the mutex
 * - multiple unlocks are not permitted
 * - recursive locking is not permitted
 * - a mutex object must be initialized via the API
 * - a mutex object must not be initialized via memset or copying
 * - task may not exit with mutex held
 * - memory areas where held locks reside must not be freed
 * - held mutexes must not be reinitialized
 * - mutexes may not be used in hardware or software interrupt
 *   contexts such as tasklets and timers
 *
 * These semantics are fully enforced when DEBUG_MUTEXES is
 * enabled. Furthermore, besides enforcing the above rules, the mutex
 * debugging code also implements a number of additional features
 * that make lock debugging easier and faster:
 *
 * - uses symbolic names of mutexes, whenever they are printed in debug output
 * - point-of-acquire tracking, symbolic lookup of function names
 * - list of all locks held in the system, printout of them
 * - owner tracking
 * - detects self-recursing locks and prints out all relevant info
 * - detects multi-task circular deadlocks and prints out all affected
 *   locks and tasks (and only those tasks)
 */
struct mutex {
	atomic_long_t		owner;
	raw_spinlock_t		wait_lock;
#ifdef CONFIG_MUTEX_SPIN_ON_OWNER
	struct optimistic_spin_queue osq; /* Spinner MCS lock */
#endif
	struct list_head	wait_list;
#ifdef CONFIG_DEBUG_MUTEXES
	void			*magic;
#endif
#ifdef CONFIG_DEBUG_LOCK_ALLOC
	struct lockdep_map	dep_map;
#endif
};
//...
#else /* !CONFIG_PREEMPT_RT */
/*
 * Preempt-RT variant based on rtmutexes.
 */
#include <linux/rtmutex.h>

struct mutex {
	struct rt_mutex_base	rtmutex;
#ifdef CONFIG_DEBUG_LOCK_ALLOC
	struct lockdep_map	dep_map;
#endif
};
//...
#endif /* CONFIG_PREEMPT_RT */
----

==== 实时互斥锁
===== 概念与原理
- 概念
since 5.15
配置: CONFIG_RT_MUTEXES=y
实时互斥锁实现了优先级继承(priority inheritance)，解决了优先级反转(priority inversion)的问题。

- 什么是优先级反转?
低优先级的task获取到了CPU，称为优先级反转：
例如A，B，C三个task，优先级从高到低，当前运行C任务，A由于要获取C的某项资源(mutex)，由于资源被占用，此时A会进入睡眠等待资源释放，比A优先级低比C优先级高的B不需要获取资源，抢占了C，也抢占了优先级更高的A，获得了CPU。

- 如何解决优先级反转的问题？
如果高优先级的任务阻塞在互斥量上，同时互斥量被低优先级的任务拥有，那么该低优先级的任务的动态优先级会提升到阻塞队列中的优先级最高之任务的优先级。
在上面的例子中，此时，C的动态优先级会提升到A，B就不再能够抢占C了。

===== 实现
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/rtmutex.h
----
struct rt_mutex_base {
	raw_spinlock_t		wait_lock;
	struct rb_root_cached   waiters;
	struct task_struct	*owner;
};
----
与经典互斥锁mutex不同的是，实时互斥锁的睡眠等待进程列表(waiters)是按优先级排序的struct rb_root_cached, 经典互斥锁则是struct list_head。

==== 参考
https://github.com/torvalds/linux/blob/master/Documentation/locking/rt-mutex.rst
https://github.com/torvalds/linux/blob/master/Documentation/locking/rt-mutex-design.rst

=== 大内核锁
BKL(Big Kernel Lock)已经被淘汰(v2.6.39)

实现:
__lock_kernel(), __unlock_kernel():
https://elixir.bootlin.com/linux/v2.6.38/source/lib/kernel_lock.c

参考: https://lwn.net/Kernel/Index/#Big_kernel_lock
参考: https://en.wikipedia.org/wiki/Giant_lock

=== per-cpu变量

==== 概念
per-cpu变量可以静态分配(分为内核中的静态percpu变量和模块中的静态percpu变量)，也可以动态分配。

==== 数据结构
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/percpu.h
----
struct pcpu_group_info {
	int			nr_units;	/* aligned # of units */
	unsigned long		base_offset;	/* base address offset */
	unsigned int		*cpu_map;	/* unit->cpu map, empty
						 * entries contain NR_CPUS */
};

struct pcpu_alloc_info {
	size_t			static_size;
	size_t			reserved_size;
	size_t			dyn_size;
	size_t			unit_size;
	size_t			atom_size;
	size_t			alloc_size;
	size_t			__ai_size;	/* internal, don't use */
	int			nr_groups;	/* 0 if grouping unnecessary */
	struct pcpu_group_info	groups[];
};
----

[source, c]
.https://elixir.bootlin.com/linux/latest/source/mm/percpu-internal.h
----
/*
 * pcpu_block_md is the metadata block struct.
 * Each chunk's bitmap is split into a number of full blocks.
 * All units are in terms of bits.
 *
 * The scan hint is the largest known contiguous area before the contig hint.
 * It is not necessarily the actual largest contig hint though.  There is an
 * invariant that the scan_hint_start > contig_hint_start iff
 * scan_hint == contig_hint.  This is necessary because when scanning forward,
 * we don't know if a new contig hint would be better than the current one.
 */
struct pcpu_block_md {
	int			scan_hint;	/* scan hint for block */
	int			scan_hint_start; /* block relative starting
						    position of the scan hint */
	int                     contig_hint;    /* contig hint for block */
	int                     contig_hint_start; /* block relative starting
						      position of the contig hint */
	int                     left_free;      /* size of free space along
						   the left side of the block */
	int                     right_free;     /* size of free space along
						   the right side of the block */
	int                     first_free;     /* block position of first free */
	int			nr_bits;	/* total bits responsible for */
};

struct pcpu_chunk {
#ifdef CONFIG_PERCPU_STATS
	int			nr_alloc;	/* # of allocations */
	size_t			max_alloc_size; /* largest allocation size */
#endif

	struct list_head	list;		/* linked to pcpu_slot lists */
	int			free_bytes;	/* free bytes in the chunk */
	struct pcpu_block_md	chunk_md;
	void			*base_addr;	/* base address of this chunk */

	unsigned long		*alloc_map;	/* allocation map */
	unsigned long		*bound_map;	/* boundary map */
	struct pcpu_block_md	*md_blocks;	/* metadata blocks */

	void			*data;		/* chunk data */
	bool			immutable;	/* no [de]population allowed */
	bool			isolated;	/* isolated from active chunk
						   slots */
	int			start_offset;	/* the overlap with the previous
						   region to have a page aligned
						   base_addr */
	int			end_offset;	/* additional area required to
						   have the region end page
						   aligned */
#ifdef CONFIG_MEMCG_KMEM
	struct obj_cgroup	**obj_cgroups;	/* vector of object cgroups */
#endif

	int			nr_pages;	/* # of pages served by this chunk */
	int			nr_populated;	/* # of populated pages */
	int                     nr_empty_pop_pages; /* # of empty populated pages */
	unsigned long		populated[];	/* populated bitmap */
};
----

==== 内核静态per-cpu变量

==== 模块静态per-cpu变量

==== 动态per-cpu变量

==== 注意事项
Q: per-cpu变量存储在哪里？
A: https://stackoverflow.com/questions/53968755/where-are-declare-per-cpu-variables-stored-in-kernel

https://stackoverflow.com/questions/16978959/how-are-percpu-pointers-implemented-in-the-linux-kernel

https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/percpu.h

==== 参考
https://lwn.net/Kernel/Index/#Per-CPU_variables
https://zhuanlan.zhihu.com/p/260986194

=== volatile
https://github.com/torvalds/linux/blob/master/Documentation/process/volatile-considered-harmful.rst

参考: https://lwn.net/Kernel/Index/#volatile

=== 参考
https://github.com/torvalds/linux/tree/master/Documentation/locking
https://docs.kernel.org/locking/index.html
https://lwn.net/Kernel/Index/#Locking_mechanisms
https://lwn.net/Kernel/Index/#lock_kernel
https://lwn.net/Kernel/Index/#Lockless_algorithms
https://lwn.net/Kernel/Index/#Race_conditions