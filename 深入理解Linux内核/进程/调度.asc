:toc:
:toclevels: 5
:hardbreaks-option:

== 调度

=== 演进
Traditional Scheduler:
v1.0 - v2.4
这一阶段的调度器和传统的UNIX上的调度器逻辑是一样的。全局只有一个运行队列，所有进程都放在一个队列上。进程区分IO密集型和CPU密集型，根据进程的睡眠时间计算动态优先级，按照动态优先级决定进程的调度顺序，按照优先级分配进程的时间片大小，时间片大小是等差数列。进程在运行队列上并没有特定的排序，每次选择下一个进程的时候都要遍历整个队列，因此算法的时间复杂度是O(n)。在SMP上也只有一个运行队列，当CPU比较多进程也比较多的时候，锁冲突比较严重。

O(1) Scheduler:
v2.5 - v2.6.22
此调度器主要是针对传统调度器进行了改进。首先把运行队列从单一变量变成了per-CPU变量，每个CPU一个运行队列，这样就不会有锁冲突了，不过这样就需要调度均衡了。其次把运行队列的一个链表分成了两个链表数组：活动数组和过期数组。时间片没用耗尽的进程放在活动数组中，时间片耗尽的进程放在过期数组中，当所有进程的时间片都耗尽的时候交换两个数组，重新分配时间片。两个数组都使用动态优先级排序，并用bitmap来表示哪个优先级队列中有可运行的进程，把选择进程的算法复杂度降低到了O(1)。对进程区分IO密集型和CPU密集型并计算动态优先级这一点和传统调度器一样没有变。

SD Scheduler:
未合入
楼梯调度器，它是对O(1)调度器的改进，算法复杂还是O(1)。之前的调度器都区分IO密集型和CPU密集型，算法要对进程的类型进行探测，根据探测结果调整动态优先级。这就有很大的问题，探测不一定准确，而且进程还可以欺骗调度器，最终会导致调度有很大的不公平性。楼梯调度器是第一次尝试使用公平调度算法，它废除了动态优先级，不再区分IO密集型进程和CPU密集型进程，但是仍然让IO密集型进程保持较高的响应性。在实现上，楼梯调度算法废弃了过期数组，只使用一个数组。当进程使用完自己的时间片之后，其时间片就会被减半并下降到下一个优先级，其本身的优先级还是不变的。当进程下降到最后一个优先级之后就再回到它本来的优先级队列并重新分配时间片。整个过程就像下楼梯一样，所以这个算法就叫做楼梯调度器。此算法虽然没有合入到标准内核，但是它第一次证明了可以采取完全公平的思想进行调度，也就是不区分IO密集型和CPU密集型进程。

RSDL Scheduler:
未合入
旋转楼梯调度器，是对楼梯调度器的改进。它又重新引入了过期数组，为每个优先级都分配一个组时间配额记为Tg，进程本身的时间片记为Tp。当进程用完自己的时间片时会下降一个优先级，当一个优先级的Tg被用完时，组内所有的进程都会下降一个优先级。进程下降到最低优先级之后会被放入过期数组，当活动数组为空时就会交换活动数组和过期数组。由于加了Tg，使得低优先级进程的调度延迟可控，进一步增加了公平性。

CFS(Completely Fair Scheduler):
v2.6.23 - now (Ingo_Molnár，匈牙利人，O(1)、CFS、PREEMPT_RT的作者，https://en.wikipedia.org/wiki/Ingo_Moln%C3%A1r)
完全公平调度器，从SD/RSDL中吸取了完全公平的思想，不再区分IO密集型进程与CPU密集型进程，不再根据睡眠时间调整动态优先级，所有普通进程都按优先级相对平分CPU时间，算法复杂度是O(logn)。此时对调度器框架也进行了重构，和之前的有很大的不同。之前的调度器是一个算法调度所有进程，在算法内部区别对待实时进程和普通进程。现在的调度器框架是先区分不同类型的进程，普通进程一个调度类，实时进程一个调度类，调度类里面再去实现具体的调度算法。CFS是普通调度类的算法。

▪ 抢占(preemptive)
Prior to Linux kernel version 2.5.4, Linux Kernel was not preemptive which means a process running in kernel mode cannot be moved out of processor until it itself leaves the processor or it starts waiting for some input output operation to get complete.
参考: https://stackoverflow.com/questions/5283501/what-does-it-mean-to-say-linux-kernel-is-preemptive

▪ 普通进程调度器(SCHED_OTHER)
O(n) -> O(1)调度器 -> CFS调度器(Completely Fair Scheduler)
具体来说:
基于时间片轮询调度算法O(n)(2.6之前的版本)
O(1)调度算法(2.6.23之前的版本): 在常数时间内完成工作，不依赖于运行的进程数量
完全公平调度算法(2.6.23以及之后的版本)

▪ 调度域(scheduling domains)
2.6.7
主要解决NUMA架构上的调度问题
https://lwn.net/Articles/80911/#:~:text=A%20scheduling%20domain%20(struct%20sched_domain,have%20multiple%20levels%20of%20domains.
https://www.kernel.org/doc/html/latest/scheduler/sched-domains.html

▪ SCHED_BATCH
Since 2.6.16
适合非交互性, CPU密集型的任务

▪ SCHED_IDLE
Since 2.6.23
系统在空闲时，每个CPU都有一个idle线程在跑，它什么也不做，就是把CPU放入硬件睡眠状态以节能(需要特定CPU的driver支持), 并等待新的任务到来，以将CPU从睡眠状态中唤醒。

▪ 普通进程的组调度支持(Fair Group Scheduling)
2.6.24
假设有两个用户，一个用户跑了9个进程，另一个用户只跑了一个进程，这将导致第二个用户的体验可能会比较差。
Group Scheduling解决了这一问题，它为每个用户建立一个组，组里放该用户所有进程，从而保证用户间的公平性。

▪ 实时进程的组调度支持(RT Group Scheduling)
2.6.25
类似普通进程的组调度，只不过针对实时进程

▪ 更精确的调度时钟(HRTICK)
2.6.25
CPU的周期性调度和基于时间片的调度，都是基于时钟中断来触发的。
High Resolution Tick能够提供更精确的调度时钟中断，它基于高精度时钟(High Resolution Timer)，即可以提供纳秒级别精度的硬件时钟。

▪ 自动组调度(Auto Group Scheduling)
2.6.38
跟某一项任务相关的所有进程可以放在一个会话里，把这些不同会话自动分成不同的调度组，从而利用组调度的优势。

▪ SCHED_DEADLINE
Since version 3.14
Deadline scheduling is thus useful for realtime tasks, where completion by a deadline is a key requirement. It is also applicable to periodic tasks like streaming media processing.
In order to fulfill the guarantees that are made when a thread is admitted to the SCHED_DEADLINE policy, SCHED_DEADLINE threads are
the highest priority (user controllable) threads in the system; if any SCHED_DEADLINE thread is runnable, it will preempt any thread scheduled under one of the other policies.
https://lwn.net/Articles/575497/

▪ 组调度带宽控制(CFS bandwidth control)
3.2
让管理员控制在一段时间内一个组可以使用CPU的最长时间

▪ 自动NUMA均衡(Automatic NUMA balancing)
3.8
https://lwn.net/Articles/524977/

▪ CPU调度与节能
https://lwn.net/Articles/655479/

参考: https://www.zhihu.com/question/35484429
参考: https://www.kernel.org/doc/html/latest/scheduler/index.html
参考: https://man7.org/linux/man-pages/man7/sched.7.html
参考: https://lwn.net/Kernel/Index/#Group_scheduling

=== 目标
调度器的任务是分配CPU资源，linux作为通用操作系统，需要兼顾效率与公平:
效率主要体现在让CPU都(均衡性)忙碌起来，并具有较高的吞吐量和较低的延迟(吞吐量和延迟通常是一对相互矛盾的指标)。不同的任务的要求可能是不同的，有几种比较典型的任务: 强调吞吐量对延迟不敏感的CPU密集型如批处理进程，以及延迟敏感不太在意吞吐量的IO密集型如交互式进程，还有实时任务，它注重对程序执行的可预测性。
公平的意义是让每个任务都有机会获得调度，保证每个进程获得合理的CPU时间。

=== 示意图
image::img/schedule.png[]

=== 数据结构

==== task_struct调度相关信息
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/sched.h
----
struct task_struct {
    //...
	int				on_rq;

	int				prio;
	int				static_prio;
	int				normal_prio;
	unsigned int			rt_priority;

	struct sched_entity		se;
	struct sched_rt_entity		rt;
	struct sched_dl_entity		dl;
	const struct sched_class	*sched_class;

#ifdef CONFIG_SCHED_CORE
	struct rb_node			core_node;
	unsigned long			core_cookie;
	unsigned int			core_occupation;
#endif

#ifdef CONFIG_CGROUP_SCHED
	struct task_group		*sched_task_group;
#endif
    //...
}
----

int				on_rq: { 0, 1 = TASK_ON_RQ_QUEUED, 2 = TASK_ON_RQ_MIGRATING }

优先级:
int				prio: 进程的有效优先级(effective priority)，判断进程优先级时使用该参数，其取值范围为[0,139]，值越小，优先级越低
int				static_prio: 普通进程和实时进程的静态优先级，代表进程的固有属性，值越小优先级越高
int				normal_prio: 基于static_prio或rt_priority，会被统一为值越小优先级越高
unsigned int			rt_priority: 实时进程的静态优先级，代表进程的固有属性，值越大优先级越高

调度实体:
	struct sched_entity		se;
	struct sched_rt_entity		rt;
	struct sched_dl_entity		dl;

const struct sched_class	*sched_class: 调度类

==== struct rq
[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/sched/sched.h
----
/*
 * This is the main, per-CPU runqueue data structure.
 *
 * Locking rule: those places that want to lock multiple runqueues
 * (such as the load balancing or the thread migration code), lock
 * acquire operations must be ordered by ascending &runqueue.
 */
struct rq {
	/* runqueue lock: */
	raw_spinlock_t		__lock;

	/*
	 * nr_running and cpu_load should be in the same cacheline because
	 * remote CPUs use both these fields when doing load calculation.
	 */
	unsigned int		nr_running;
#ifdef CONFIG_NUMA_BALANCING
	unsigned int		nr_numa_running;
	unsigned int		nr_preferred_running;
	unsigned int		numa_migrate_on;
#endif
#ifdef CONFIG_NO_HZ_COMMON
#ifdef CONFIG_SMP
	unsigned long		last_blocked_load_update_tick;
	unsigned int		has_blocked_load;
	call_single_data_t	nohz_csd;
#endif /* CONFIG_SMP */
	unsigned int		nohz_tick_stopped;
	atomic_t		nohz_flags;
#endif /* CONFIG_NO_HZ_COMMON */

#ifdef CONFIG_SMP
	unsigned int		ttwu_pending;
#endif
	u64			nr_switches;

#ifdef CONFIG_UCLAMP_TASK
	/* Utilization clamp values based on CPU's RUNNABLE tasks */
	struct uclamp_rq	uclamp[UCLAMP_CNT] ____cacheline_aligned;
	unsigned int		uclamp_flags;
#define UCLAMP_FLAG_IDLE 0x01
#endif

	struct cfs_rq		cfs;
	struct rt_rq		rt;
	struct dl_rq		dl;

#ifdef CONFIG_FAIR_GROUP_SCHED
	/* list of leaf cfs_rq on this CPU: */
	struct list_head	leaf_cfs_rq_list;
	struct list_head	*tmp_alone_branch;
#endif /* CONFIG_FAIR_GROUP_SCHED */

	/*
	 * This is part of a global counter where only the total sum
	 * over all CPUs matters. A task can increase this counter on
	 * one CPU and if it got migrated afterwards it may decrease
	 * it on another CPU. Always updated under the runqueue lock:
	 */
	unsigned int		nr_uninterruptible;

	struct task_struct __rcu	*curr;
	struct task_struct	*idle;
	struct task_struct	*stop;
	unsigned long		next_balance;
	struct mm_struct	*prev_mm;

	unsigned int		clock_update_flags;
	u64			clock;
	/* Ensure that all clocks are in the same cache line */
	u64			clock_task ____cacheline_aligned;
	u64			clock_pelt;
	unsigned long		lost_idle_time;
	u64			clock_pelt_idle;
	u64			clock_idle;
#ifndef CONFIG_64BIT
	u64			clock_pelt_idle_copy;
	u64			clock_idle_copy;
#endif

	atomic_t		nr_iowait;

#ifdef CONFIG_SCHED_DEBUG
	u64 last_seen_need_resched_ns;
	int ticks_without_resched;
#endif

#ifdef CONFIG_MEMBARRIER
	int membarrier_state;
#endif

#ifdef CONFIG_SMP
	struct root_domain		*rd;
	struct sched_domain __rcu	*sd;

	unsigned long		cpu_capacity;
	unsigned long		cpu_capacity_orig;
	unsigned long		cpu_capacity_inverted;

	struct balance_callback *balance_callback;

	unsigned char		nohz_idle_balance;
	unsigned char		idle_balance;

	unsigned long		misfit_task_load;

	/* For active balancing */
	int			active_balance;
	int			push_cpu;
	struct cpu_stop_work	active_balance_work;

	/* CPU of this runqueue: */
	int			cpu;
	int			online;

	struct list_head cfs_tasks;

	struct sched_avg	avg_rt;
	struct sched_avg	avg_dl;
#ifdef CONFIG_HAVE_SCHED_AVG_IRQ
	struct sched_avg	avg_irq;
#endif
#ifdef CONFIG_SCHED_THERMAL_PRESSURE
	struct sched_avg	avg_thermal;
#endif
	u64			idle_stamp;
	u64			avg_idle;

	unsigned long		wake_stamp;
	u64			wake_avg_idle;

	/* This is used to determine avg_idle's max value */
	u64			max_idle_balance_cost;

#ifdef CONFIG_HOTPLUG_CPU
	struct rcuwait		hotplug_wait;
#endif
#endif /* CONFIG_SMP */

#ifdef CONFIG_IRQ_TIME_ACCOUNTING
	u64			prev_irq_time;
#endif
#ifdef CONFIG_PARAVIRT
	u64			prev_steal_time;
#endif
#ifdef CONFIG_PARAVIRT_TIME_ACCOUNTING
	u64			prev_steal_time_rq;
#endif

	/* calc_load related fields */
	unsigned long		calc_load_update;
	long			calc_load_active;

#ifdef CONFIG_SCHED_HRTICK
#ifdef CONFIG_SMP
	call_single_data_t	hrtick_csd;
#endif
	struct hrtimer		hrtick_timer;
	ktime_t 		hrtick_time;
#endif

#ifdef CONFIG_SCHEDSTATS
	/* latency stats */
	struct sched_info	rq_sched_info;
	unsigned long long	rq_cpu_time;
	/* could above be rq->cfs_rq.exec_clock + rq->rt_rq.rt_runtime ? */

	/* sys_sched_yield() stats */
	unsigned int		yld_count;

	/* schedule() stats */
	unsigned int		sched_count;
	unsigned int		sched_goidle;

	/* try_to_wake_up() stats */
	unsigned int		ttwu_count;
	unsigned int		ttwu_local;
#endif

#ifdef CONFIG_CPU_IDLE
	/* Must be inspected within a rcu lock section */
	struct cpuidle_state	*idle_state;
#endif

#ifdef CONFIG_SMP
	unsigned int		nr_pinned;
#endif
	unsigned int		push_busy;
	struct cpu_stop_work	push_work;

#ifdef CONFIG_SCHED_CORE
	/* per rq */
	struct rq		*core;
	struct task_struct	*core_pick;
	unsigned int		core_enabled;
	unsigned int		core_sched_seq;
	struct rb_root		core_tree;

	/* shared state -- careful with sched_core_cpu_deactivate() */
	unsigned int		core_task_seq;
	unsigned int		core_pick_seq;
	unsigned long		core_cookie;
	unsigned int		core_forceidle_count;
	unsigned int		core_forceidle_seq;
	unsigned int		core_forceidle_occupation;
	u64			core_forceidle_start;
#endif

	/* Scratch cpumask to be temporarily used under rq_lock */
	cpumask_var_t		scratch_mask;
};
----

每一个CPU上有一个struct rq。
struct cfs_rq		cfs: 公平调度运行/就绪队列
struct rt_rq		rt: 实时调度运行/就绪队列
struct dl_rq		dl: 限时调度运行/就绪队列

==== sched_class
[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/sched/sched.h
----
struct sched_class {

#ifdef CONFIG_UCLAMP_TASK
	int uclamp_enabled;
#endif

	void (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags);
	void (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags);
	void (*yield_task)   (struct rq *rq);
	bool (*yield_to_task)(struct rq *rq, struct task_struct *p);

	void (*check_preempt_curr)(struct rq *rq, struct task_struct *p, int flags);

	struct task_struct *(*pick_next_task)(struct rq *rq);

	void (*put_prev_task)(struct rq *rq, struct task_struct *p);
	void (*set_next_task)(struct rq *rq, struct task_struct *p, bool first);

#ifdef CONFIG_SMP
	int (*balance)(struct rq *rq, struct task_struct *prev, struct rq_flags *rf);
	int  (*select_task_rq)(struct task_struct *p, int task_cpu, int flags);

	struct task_struct * (*pick_task)(struct rq *rq);

	void (*migrate_task_rq)(struct task_struct *p, int new_cpu);

	void (*task_woken)(struct rq *this_rq, struct task_struct *task);

	void (*set_cpus_allowed)(struct task_struct *p, struct affinity_context *ctx);

	void (*rq_online)(struct rq *rq);
	void (*rq_offline)(struct rq *rq);

	struct rq *(*find_lock_rq)(struct task_struct *p, struct rq *rq);
#endif

	void (*task_tick)(struct rq *rq, struct task_struct *p, int queued);
	void (*task_fork)(struct task_struct *p);
	void (*task_dead)(struct task_struct *p);

	/*
	 * The switched_from() call is allowed to drop rq->lock, therefore we
	 * cannot assume the switched_from/switched_to pair is serialized by
	 * rq->lock. They are however serialized by p->pi_lock.
	 */
	void (*switched_from)(struct rq *this_rq, struct task_struct *task);
	void (*switched_to)  (struct rq *this_rq, struct task_struct *task);
	void (*prio_changed) (struct rq *this_rq, struct task_struct *task,
			      int oldprio);

	unsigned int (*get_rr_interval)(struct rq *rq,
					struct task_struct *task);

	void (*update_curr)(struct rq *rq);

#ifdef CONFIG_FAIR_GROUP_SCHED
	void (*task_change_group)(struct task_struct *p);
#endif
};
//...
extern const struct sched_class stop_sched_class;
extern const struct sched_class dl_sched_class;
extern const struct sched_class rt_sched_class;
extern const struct sched_class fair_sched_class;
extern const struct sched_class idle_sched_class;
----

CONFIG_UCLAMP_TASK: https://lwn.net/Articles/788428/, 默认CONFIG_UCLAMP_TASK=y, 参考: https://cateee.net/lkddb/web-lkddb/UCLAMP_TASK.html

==== sched_entity
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/sched.h
----
struct sched_entity {
	/* For load-balancing: */
	struct load_weight		load;
	struct rb_node			run_node;
	struct list_head		group_node;
	unsigned int			on_rq;

	u64				exec_start;
	u64				sum_exec_runtime;
	u64				vruntime;
	u64				prev_sum_exec_runtime;

	u64				nr_migrations;

#ifdef CONFIG_FAIR_GROUP_SCHED
	int				depth;
	struct sched_entity		*parent;
	/* rq on which this entity is (to be) queued: */
	struct cfs_rq			*cfs_rq;
	/* rq "owned" by this entity/group: */
	struct cfs_rq			*my_q;
	/* cached value of my_q->h_nr_running */
	unsigned long			runnable_weight;
#endif

#ifdef CONFIG_SMP
	/*
	 * Per entity load average tracking.
	 *
	 * Put into separate cache line so it does not
	 * collide with read-mostly values above.
	 */
	struct sched_avg		avg;
#endif
};
----

sched_entity是fair_sched_class的调度实体。
此外还有：
struct sched_rt_entity: 对应rt_sched_clas调度实体
struct sched_dl_entity: 对应dl_sched_class调度实体

struct load_weight		load: 权重信息，在计算虚拟时间的时候会用到inv_weight成员。
struct rb_node			run_node: 红黑树节点，CFS调度器的每个就绪队列维护了一颗红黑树，挂载就绪等待执行的task。
unsigned int			on_rq: 调度实体se加入就绪队列后，on_rq置1；从就绪队列删除后，on_rq置0。
u64				sum_exec_runtime: 调度实体已经运行实际时间总和。
u64				vruntime: 调度实体已经运行的虚拟时间总和。

=== 接口

==== 调度
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/sched.h
----
extern void scheduler_tick(void);

#define	MAX_SCHEDULE_TIMEOUT		LONG_MAX

extern long schedule_timeout(long timeout);
extern long schedule_timeout_interruptible(long timeout);
extern long schedule_timeout_killable(long timeout);
extern long schedule_timeout_uninterruptible(long timeout);
extern long schedule_timeout_idle(long timeout);
asmlinkage void schedule(void);
extern void schedule_preempt_disabled(void);
asmlinkage void preempt_schedule_irq(void);
#ifdef CONFIG_PREEMPT_RT
 extern void schedule_rtlock(void);
#endif

extern int __must_check io_schedule_prepare(void);
extern void io_schedule_finish(int token);
extern long io_schedule_timeout(long timeout);
extern void io_schedule(void);
----

周期性调度scheduler_tick:
周期性调度scheduler_tick由内核时钟中断周期性的触发, 周期性调度以固定的频率激活负责当前进程调度类的周期性调度方法, 以保证系统的并发性, 周期性调度通过调用进程所属调度类的task_tick操作完成周期性调度的通知和配置工作, 通过设置标识TIF_NEED_RESCHED来通知内核在必要的时间由主调度函数完成真正的调度工作, 此种做法称之为延迟调度策略。

主调度schedule:
void schedule()就是主调度的工作函数

==== 抢占
preempt_enable()
preempt_disable()

void preempt_schedule(void);

=== 时机
▪ 自愿调度/主动调度
    
    内核空间:
        schedule()相关调用
        或调用schedule前设置进程状态为TASK_INTERRUPTIBLE/TASK_UNINTERRUPTIBLE
    用户空间:
        nanosleep()
        pause()
		sched_yield(): https://www.quora.com/What-is-the-difference-between-sleep-0-and-sched_yield
        几乎所有涉及外设系统调用函数，如open/read/write/select等

▪ 非自愿调度/被动调度/强制性调度

    每次从系统调用返回的前夕:
        do_syscall_64(): https://elixir.bootlin.com/linux/latest/source/arch/x86/entry/common.c
        syscall_exit_to_user_mode: https://elixir.bootlin.com/linux/latest/source/kernel/entry/common.c
            exit_to_user_mode_prepare() 
                exit_to_user_mode_loop()
                    schedule()
    每次从中断或异常处理返回:
        irqentry_exit(): https://elixir.bootlin.com/linux/latest/source/kernel/entry/common.c
            到用户空间的前夕:
                irqentry_exit_to_user_mode()
                    exit_to_user_mode_prepare()
                        exit_to_user_mode_loop()
                            schedule()
            到内核空间的前夕:
                irqentry_exit_cond_resched()
                    raw_irqentry_exit_cond_resched()
                        preempt_schedule_irq()
                            __schedule(SM_PREEMPT);

        旧: ret_from_intr(): https://elixir.bootlin.com/linux/2.4.0/source/arch/i386/kernel/entry.S#L273
            ret_with_reschedule()

=== 实现
[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/sched/core.c
----
/*
 * __schedule() is the main scheduler function.
 *
 * The main means of driving the scheduler and thus entering this function are:
 *
 *   1. Explicit blocking: mutex, semaphore, waitqueue, etc.
 *
 *   2. TIF_NEED_RESCHED flag is checked on interrupt and userspace return
 *      paths. For example, see arch/x86/entry_64.S.
 *
 *      To drive preemption between tasks, the scheduler sets the flag in timer
 *      interrupt handler scheduler_tick().
 *
 *   3. Wakeups don't really cause entry into schedule(). They add a
 *      task to the run-queue and that's it.
 *
 *      Now, if the new task added to the run-queue preempts the current
 *      task, then the wakeup sets TIF_NEED_RESCHED and schedule() gets
 *      called on the nearest possible occasion:
 *
 *       - If the kernel is preemptible (CONFIG_PREEMPTION=y):
 *
 *         - in syscall or exception context, at the next outmost
 *           preempt_enable(). (this might be as soon as the wake_up()'s
 *           spin_unlock()!)
 *
 *         - in IRQ context, return from interrupt-handler to
 *           preemptible context
 *
 *       - If the kernel is not preemptible (CONFIG_PREEMPTION is not set)
 *         then at the next:
 *
 *          - cond_resched() call
 *          - explicit schedule() call
 *          - return from syscall or exception to user-space
 *          - return from interrupt-handler to user-space
 *
 * WARNING: must be called with preemption disabled!
 */
static void __sched notrace __schedule(unsigned int sched_mode)
{
	struct task_struct *prev, *next;
	unsigned long *switch_count;
	unsigned long prev_state;
	struct rq_flags rf;
	struct rq *rq;
	int cpu;

	cpu = smp_processor_id();
	rq = cpu_rq(cpu);
	prev = rq->curr;

	schedule_debug(prev, !!sched_mode);

	if (sched_feat(HRTICK) || sched_feat(HRTICK_DL))
		hrtick_clear(rq);

	local_irq_disable();
	rcu_note_context_switch(!!sched_mode);

	/*
	 * Make sure that signal_pending_state()->signal_pending() below
	 * can't be reordered with __set_current_state(TASK_INTERRUPTIBLE)
	 * done by the caller to avoid the race with signal_wake_up():
	 *
	 * __set_current_state(@state)		signal_wake_up()
	 * schedule()				  set_tsk_thread_flag(p, TIF_SIGPENDING)
	 *					  wake_up_state(p, state)
	 *   LOCK rq->lock			    LOCK p->pi_state
	 *   smp_mb__after_spinlock()		    smp_mb__after_spinlock()
	 *     if (signal_pending_state())	    if (p->state & @state)
	 *
	 * Also, the membarrier system call requires a full memory barrier
	 * after coming from user-space, before storing to rq->curr.
	 */
	rq_lock(rq, &rf);
	smp_mb__after_spinlock();

	/* Promote REQ to ACT */
	rq->clock_update_flags <<= 1;
	update_rq_clock(rq);

	switch_count = &prev->nivcsw;

	/*
	 * We must load prev->state once (task_struct::state is volatile), such
	 * that we form a control dependency vs deactivate_task() below.
	 */
	prev_state = READ_ONCE(prev->__state);
	if (!(sched_mode & SM_MASK_PREEMPT) && prev_state) {
		if (signal_pending_state(prev_state, prev)) {
			WRITE_ONCE(prev->__state, TASK_RUNNING);
		} else {
			prev->sched_contributes_to_load =
				(prev_state & TASK_UNINTERRUPTIBLE) &&
				!(prev_state & TASK_NOLOAD) &&
				!(prev->flags & PF_FROZEN);

			if (prev->sched_contributes_to_load)
				rq->nr_uninterruptible++;

			/*
			 * __schedule()			ttwu()
			 *   prev_state = prev->state;    if (p->on_rq && ...)
			 *   if (prev_state)		    goto out;
			 *     p->on_rq = 0;		  smp_acquire__after_ctrl_dep();
			 *				  p->state = TASK_WAKING
			 *
			 * Where __schedule() and ttwu() have matching control dependencies.
			 *
			 * After this, schedule() must not care about p->state any more.
			 */
			deactivate_task(rq, prev, DEQUEUE_SLEEP | DEQUEUE_NOCLOCK);

			if (prev->in_iowait) {
				atomic_inc(&rq->nr_iowait);
				delayacct_blkio_start();
			}
		}
		switch_count = &prev->nvcsw;
	}

	next = pick_next_task(rq, prev, &rf);
	clear_tsk_need_resched(prev);
	clear_preempt_need_resched();
#ifdef CONFIG_SCHED_DEBUG
	rq->last_seen_need_resched_ns = 0;
#endif

	if (likely(prev != next)) {
		rq->nr_switches++;
		/*
		 * RCU users of rcu_dereference(rq->curr) may not see
		 * changes to task_struct made by pick_next_task().
		 */
		RCU_INIT_POINTER(rq->curr, next);
		/*
		 * The membarrier system call requires each architecture
		 * to have a full memory barrier after updating
		 * rq->curr, before returning to user-space.
		 *
		 * Here are the schemes providing that barrier on the
		 * various architectures:
		 * - mm ? switch_mm() : mmdrop() for x86, s390, sparc, PowerPC.
		 *   switch_mm() rely on membarrier_arch_switch_mm() on PowerPC.
		 * - finish_lock_switch() for weakly-ordered
		 *   architectures where spin_unlock is a full barrier,
		 * - switch_to() for arm64 (weakly-ordered, spin_unlock
		 *   is a RELEASE barrier),
		 */
		++*switch_count;

		migrate_disable_switch(rq, prev);
		psi_sched_switch(prev, next, !task_on_rq_queued(prev));

		trace_sched_switch(sched_mode & SM_MASK_PREEMPT, prev, next, prev_state);

		/* Also unlocks the rq: */
		rq = context_switch(rq, prev, next, &rf);
	} else {
		rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);

		rq_unpin_lock(rq, &rf);
		__balance_callbacks(rq);
		raw_spin_rq_unlock_irq(rq);
	}
}
----

可见__schedule()流程的核心是: pick_next_task()与context_switch()，即挑选任务与任务切换。

pick_next_task()的核心函数是__pick_next_task():
[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/sched/core.c
----
/*
 * Pick up the highest-prio task:
 */
static inline struct task_struct *
__pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
{
	const struct sched_class *class;
	struct task_struct *p;

	/*
	 * Optimization: we know that if all tasks are in the fair class we can
	 * call that function directly, but only if the @prev task wasn't of a
	 * higher scheduling class, because otherwise those lose the
	 * opportunity to pull in more work from other CPUs.
	 */
	if (likely(!sched_class_above(prev->sched_class, &fair_sched_class) &&
		   rq->nr_running == rq->cfs.h_nr_running)) {

		p = pick_next_task_fair(rq, prev, rf);
		if (unlikely(p == RETRY_TASK))
			goto restart;

		/* Assume the next prioritized class is idle_sched_class */
		if (!p) {
			put_prev_task(rq, prev);
			p = pick_next_task_idle(rq);
		}

		return p;
	}

restart:
	put_prev_task_balance(rq, prev, rf);

	for_each_class(class) {
		p = class->pick_next_task(rq);
		if (p)
			return p;
	}

	BUG(); /* The idle class should always have a runnable task. */
}
----

context_switch(): link:./切换.asc#过程[切换]

=== 策略
ps -eLfc命令可以查看调度策略(CLS列)

	cls         CLS       scheduling class of the process.(alias policy, cls). 
	Field's possible values are:
		-   not reported
		TS  SCHED_OTHER
		FF  SCHED_FIFO
		RR  SCHED_RR
		B   SCHED_BATCH
		ISO SCHED_ISO
		IDL SCHED_IDLE
		DLN SCHED_DEADLINE
		?   unknown value

- SCHED_FIFO与SCHED_RR

	这两个调度策略定义了对实时任务，即对延时和限定性的完成时间的高敏感度的任务。
	前者提供FIFO语义，相同优先级的任务先到先服务，高优先级的任务可以抢占低优先级的任务；
	后者提供Round-Robin语义，采用时间片，相同优先级的任务用完时间片后会被放到队列尾部以保证公平性，高优先级可以抢占低优先级的任务。
	不同要求的实时任务可以根据需要使用sched_setscheduler()函数设置策略。

- SCHED_OTHER

	此调度策略包含除上述实时进程之外的其他进程，亦称普通进程。
	采用分时策略，根据动态优先级(可用nice()函数设置)，分配CPU运算资源。
	注意：这类进程比上述两类实时进程优先级低，在有实时进程存在时，实时进程会优先调度。

除了实现上述策略，linux还额外支持以下策略：

- SCHED_IDLE

	优先级最低，在系统空闲时才跑这类进程

- SCHED_BATCH

	SCHED_OTHER策略的分化，与SCHED_OTHER策略一样，但针对吞吐量优化

- SCHED_DEADLINE

	新支持的实时进程调度策略，针对突发型计算，且对延迟和完成时间高度敏感的任务适用。

▪ 定义
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/uapi/linux/sched.h
----
/*
 * Scheduling policies
 */
#define SCHED_NORMAL    0
#define SCHED_FIFO		1
#define SCHED_RR		2
#define SCHED_BATCH		3
/* SCHED_ISO: reserved but not implemented yet */
#define SCHED_IDLE		5
#define SCHED_DEADLINE  6
----

=== 调度类

==== 概念
从2.6.23开始，linux引入了调度类，其目的是将调度模块化，提高了扩展性，添加一个新的调度也变得简单一些。

▪ 调度类:
调度类一共五种: stop(禁令调度类)、deadline(限时调度类)、realtime(实时调度类)、time-share(分时调度类)、idle(闲时调度类)。
其紧迫性从上到下，依次降低。
禁令调度类和闲时调度类仅用于内核，在内核启动时就设置好了，每个CPU一个相应的进程，因此也不需要调度算法。
其它调度类可以用于用户空间进程，有相应的调度策略和调度算法。
在有高调度类进程可运行的情况下，不会去调度低调度类的进程。例外: 内核为了防止实时进程饿死普通进程，提供了一个配置参数，超过一定的CPU时间，就会把剩余的CPU时间分给普通进程，默认值是95%。
显然，一个系统中可以共存多种调度。

▪ 调度类与优先级
五种调度类里:
禁令调度类和闲时调度类只在内核里使用，每CPU只有一个线程，用不到进程优先级;
限时调度类使用进程设置的调度参数作为调度依据，也用不到进程优先级;
只有实时调度类和公平调度类会用到进程优先级，不过它们使用优先级的方法并不相同。

==== 禁令调度(stop)
▪ 禁令调度类(stop_sched_class):
内核用来执行一些特别紧急的事物所使用的。
禁令调度类的进程是内核在启动时就创建好的，每个CPU一个进程即[migration/n]。
调度均衡要迁移线程的时候会用到这类进程，因此取名migration。
https://elixir.bootlin.com/linux/latest/source/include/linux/stop_machine.h
https://elixir.bootlin.com/linux/latest/source/kernel/stop_machine.c
禁令调度类的进程优先级是最高的，只要此类进程变得runnable了，就会立马抢占当前进程来运行。

https://elixir.bootlin.com/linux/latest/source/kernel/sched/stop_task.c

==== 限时调度(deadline)
▪ 限时调度类(dl_sched_class):
软实时，适用于对调度时间有明确要求的进程。
限时调度类只有一个调度策略，限时调度策略。
一个进程必须通过系统调用才能把自己设置为限时调度策略，并且还要提供三个参数：运行周期(多长时间内想要运行一次)、运行时间(每次想要运行多长时间)和截止时间。
同时设置不一定能成功，内核需要测与已有的限时调度类进程是否冲突，如果有冲突则只能返回失败。
另外如果实际运行时间超过了预估时间则进程就会被切走，这可能会导致灾难性的后果。

https://elixir.bootlin.com/linux/latest/source/kernel/sched/deadline.c

==== 实时调度(rt)
▪ 实时调度类(rt_sched_class):
属于软实时，适用于只要可运行就希望立马能执行的进程。
实时调度类有两个调度策略，SCHED_FIFO和SCHED_RR。
实时调度类的内部逻辑是让实时优先级大的进程先运行，只要有实时优先级大的进程可运行，就不会去调度实时优先级小的进程。
当两个实时进程的优先级相同时，SCHED_RR和SCHED_FIFO这两个策略就有区别了:
SCHED_FIFO进程如果抢占了CPU，它就会一直占着CPU，不会给同优先级的实时进程让CPU的，而SCHED_RR进程会在运行了一定的时间片之后主动让给同优先级的实时进程。

https://elixir.bootlin.com/linux/latest/source/kernel/sched/rt.c

==== 完全公平调度(cfs)
▪ 分时调度类/公平调度类(fair_sched_class):
广大的普通进程用来共同分享CPU。
根据优先级的不同，可能有的进程分的多有的进程分的少，但是不会出现一个进程霸占CPU的情况。
分时调度类有三个调度策略：SCHED_NORMAL、SCHED_BATCH和SCHED_IDLE。
SCHED_BATCH进程希望减少调度次数，每次调度能执行的时间长一点。
SCHED_IDLE是优先级特别低的进程，其分到的CPU时间的比例非常低，但是也总是能保证分到。

▪ 为什么需要完全公平调度？
O(n)调度器需要遍历全局的就绪列表。
O(1)调度器虽然改善了效率，但不能实现公平，一个调度周期内，低优先级的应用可能一直无法响应，直到高优先级应用结束。
CFS保证了公平调度，即保证在一个调度周期内每个任务都有执行的机会，而执行时间的长短，取决于任务的权重。

▪ 主要思想
每个进程得到同样多的CPU时间(实际运行时间/物理运行时间)，是公平的吗？是公平，但没有考虑到优先级。

分配给进程的运行时间 = 调度周期 * 进程权重 / 所有可运行进程权重之和

CFS并没有用纯粹的实际运行时间(物理运行时间)这个维度，而是加上了优先级的维度，即Nice值，Nice值越高，权重越低/优先级越低。

CFS引入了虚拟时间(virtual runtime)的概念: the per-task p->se.vruntime (nanosec-unit) value(即task_struct里的struct sched_entity se里的vruntime字段)。
每次调度挑选最小vruntime的调度实体/红黑树节点。
挑选最小的vruntime是通过维护rq->cfs.min_vruntime和红黑树来实现的。

CFS的公平性体现在(同一个CPU上):
在一个调度周期内每个任务都有执行的机会;
实际执行时间的长短，取决于任务的权重; 同一个nice值/权重的任务，获得的实际执行时间相当;
尽量让不同任务的虚拟执行时间相等。

[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/sched/core.c
----
/*
 * Nice levels are multiplicative, with a gentle 10% change for every
 * nice level changed. I.e. when a CPU-bound task goes from nice 0 to
 * nice 1, it will get ~10% less CPU time than another CPU-bound task
 * that remained on nice 0.
 *
 * The "10% effect" is relative and cumulative: from _any_ nice level,
 * if you go up 1 level, it's -10% CPU usage, if you go down 1 level
 * it's +10% CPU usage. (to achieve that we use a multiplier of 1.25.
 * If a task goes up by ~10% and another task goes down by ~10% then
 * the relative distance between them is ~25%.)
 */
const int sched_prio_to_weight[40] = {
 /* -20 */     88761,     71755,     56483,     46273,     36291,
 /* -15 */     29154,     23254,     18705,     14949,     11916,
 /* -10 */      9548,      7620,      6100,      4904,      3906,
 /*  -5 */      3121,      2501,      1991,      1586,      1277,
 /*   0 */      1024,       820,       655,       526,       423,
 /*   5 */       335,       272,       215,       172,       137,
 /*  10 */       110,        87,        70,        56,        45,
 /*  15 */        36,        29,        23,        18,        15,
};

/*
 * Inverse (2^32/x) values of the sched_prio_to_weight[] array, precalculated.
 *
 * In cases where the weight does not change often, we can use the
 * precalculated inverse to speed up arithmetics by turning divisions
 * into multiplications:
 */
const u32 sched_prio_to_wmult[40] = {
 /* -20 */     48388,     59856,     76040,     92818,    118348,
 /* -15 */    147320,    184698,    229616,    287308,    360437,
 /* -10 */    449829,    563644,    704093,    875809,   1099582,
 /*  -5 */   1376151,   1717300,   2157191,   2708050,   3363326,
 /*   0 */   4194304,   5237765,   6557202,   8165337,  10153587,
 /*   5 */  12820798,  15790321,  19976592,  24970740,  31350126,
 /*  10 */  39045157,  49367440,  61356676,  76695844,  95443717,
 /*  15 */ 119304647, 148102320, 186737708, 238609294, 286331153,
};
----
sched_prio_to_weight: 转换nice值和权重

nice值就是一个具体的数字，取值范围是[-20, 19]。数值越小代表优先级越大，同时也意味着权重值越大，nice值和权重之间可以互相转换。
公式: weight = 1024 / 1.25^nice
nice值每减小1，weight值增长为1.25倍。
公式中的1024: 1024权重对应nice值为0，其权重被称为NICE_0_LOAD，称为基准值。默认情况下，大部分进程的权重基本都是NICE_0_LOAD。
公式中的1.25取值依据: 进程每降低一个nice值，将多获得10% cpu的时间。

例如:
两个进程A和B在nice级别0，即静态优先级120运行，因此两个进程的CPU份额相同，都是50%，nice级别为0的进程，其权重是1024。每个进程的份额是1024/(1024+1024)=0.5，即50%。
如果进程B的优先级降低, 例如nice升为1, 则其CPU份额应该减少10%: 进程B的nice升为1导致权重减少, 即1024/1.25=820, 进程A权重还是1024, 那么进程A现在将得到的CPU份额是1024/(1024+820)=0.55, 进程B的CPU份额则变为820/(1024+820)=0.45, 这样正好产生了10%的差值。
如果进程B的优先级再降低，nice值升到2: 则进程A: 1024/(1024+655)=0.61，进程B: 655/(1024+655)=0.39, 产生了20%左右的差值，即每次nice升1，会累积10%左右的差值。

                                    NICE_0_LOAD
    virtual_runtime = wall_time * ---------------- (wall_time: 实际运行时间)
                                        weight

                                    NICE_0_LOAD * 2^32
                    = (wall_time * -------------------------) >> 32 (避免浮点数运算，先放大再缩小以保证计算精度)
                                            weight
                                                                                            2^32
                    = (wall_time * NICE_0_LOAD * inv_weight) >> 32        (inv_weight = ------------ )
                                                                                            weight 

sched_prio_to_wmult: 就是为了提升计算速度，对应上面公式中的inv_weight。

▪ 公平调度的实现
[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/sched/fair.c
----
/*
 * All the scheduling class methods:
 */
DEFINE_SCHED_CLASS(fair) = {

	.enqueue_task		= enqueue_task_fair,
	.dequeue_task		= dequeue_task_fair,
	.yield_task		= yield_task_fair,
	.yield_to_task		= yield_to_task_fair,

	.check_preempt_curr	= check_preempt_wakeup,

	.pick_next_task		= __pick_next_task_fair,
	.put_prev_task		= put_prev_task_fair,
	.set_next_task          = set_next_task_fair,

#ifdef CONFIG_SMP
	.balance		= balance_fair,
	.pick_task		= pick_task_fair,
	.select_task_rq		= select_task_rq_fair,
	.migrate_task_rq	= migrate_task_rq_fair,

	.rq_online		= rq_online_fair,
	.rq_offline		= rq_offline_fair,

	.task_dead		= task_dead_fair,
	.set_cpus_allowed	= set_cpus_allowed_common,
#endif

	.task_tick		= task_tick_fair,
	.task_fork		= task_fork_fair,

	.prio_changed		= prio_changed_fair,
	.switched_from		= switched_from_fair,
	.switched_to		= switched_to_fair,

	.get_rr_interval	= get_rr_interval_fair,

	.update_curr		= update_curr_fair,

#ifdef CONFIG_FAIR_GROUP_SCHED
	.task_change_group	= task_change_group_fair,
#endif

#ifdef CONFIG_UCLAMP_TASK
	.uclamp_enabled		= 1,
#endif
};
----

[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/sched/sched.h
----
/* CFS-related fields in a runqueue */
struct cfs_rq {
	struct load_weight	load;
	unsigned int		nr_running;
	unsigned int		h_nr_running;      /* SCHED_{NORMAL,BATCH,IDLE} */
	unsigned int		idle_nr_running;   /* SCHED_IDLE */
	unsigned int		idle_h_nr_running; /* SCHED_IDLE */

	u64			exec_clock;
	u64			min_vruntime;
#ifdef CONFIG_SCHED_CORE
	unsigned int		forceidle_seq;
	u64			min_vruntime_fi;
#endif

#ifndef CONFIG_64BIT
	u64			min_vruntime_copy;
#endif

	struct rb_root_cached	tasks_timeline;

	/*
	 * 'curr' points to currently running entity on this cfs_rq.
	 * It is set to NULL otherwise (i.e when none are currently running).
	 */
	struct sched_entity	*curr;
	struct sched_entity	*next;
	struct sched_entity	*last;
	struct sched_entity	*skip;

#ifdef	CONFIG_SCHED_DEBUG
	unsigned int		nr_spread_over;
#endif

#ifdef CONFIG_SMP
	/*
	 * CFS load tracking
	 */
	struct sched_avg	avg;
#ifndef CONFIG_64BIT
	u64			last_update_time_copy;
#endif
	struct {
		raw_spinlock_t	lock ____cacheline_aligned;
		int		nr;
		unsigned long	load_avg;
		unsigned long	util_avg;
		unsigned long	runnable_avg;
	} removed;

#ifdef CONFIG_FAIR_GROUP_SCHED
	unsigned long		tg_load_avg_contrib;
	long			propagate;
	long			prop_runnable_sum;

	/*
	 *   h_load = weight * f(tg)
	 *
	 * Where f(tg) is the recursive weight fraction assigned to
	 * this group.
	 */
	unsigned long		h_load;
	u64			last_h_load_update;
	struct sched_entity	*h_load_next;
#endif /* CONFIG_FAIR_GROUP_SCHED */
#endif /* CONFIG_SMP */

#ifdef CONFIG_FAIR_GROUP_SCHED
	struct rq		*rq;	/* CPU runqueue to which this cfs_rq is attached */

	/*
	 * leaf cfs_rqs are those that hold tasks (lowest schedulable entity in
	 * a hierarchy). Non-leaf lrqs hold other higher schedulable entities
	 * (like users, containers etc.)
	 *
	 * leaf_cfs_rq_list ties together list of leaf cfs_rq's in a CPU.
	 * This list is used during load balance.
	 */
	int			on_list;
	struct list_head	leaf_cfs_rq_list;
	struct task_group	*tg;	/* group that "owns" this runqueue */

	/* Locally cached copy of our task_group's idle value */
	int			idle;

#ifdef CONFIG_CFS_BANDWIDTH
	int			runtime_enabled;
	s64			runtime_remaining;

	u64			throttled_pelt_idle;
#ifndef CONFIG_64BIT
	u64                     throttled_pelt_idle_copy;
#endif
	u64			throttled_clock;
	u64			throttled_clock_pelt;
	u64			throttled_clock_pelt_time;
	int			throttled;
	int			throttle_count;
	struct list_head	throttled_list;
#endif /* CONFIG_CFS_BANDWIDTH */
#endif /* CONFIG_FAIR_GROUP_SCHED */
};
----

struct load_weight	load: 就绪队列权重，就绪队列管理的所有调度实体权重之和。
unsigned int		nr_running：就绪队列上调度实体的个数。
u64			min_vruntime: 跟踪就绪队列上所有调度实体的最小虚拟时间。

struct rb_root_cached	tasks_timeline: 按照虚拟时间排序的红黑树
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/rbtree_types.h
----
/*
 * Leftmost-cached rbtrees.
 *
 * We do not cache the rightmost node based on footprint
 * size vs number of potential users that could benefit
 * from O(1) rb_last(). Just not worth it, users that want
 * this feature can always implement the logic explicitly.
 * Furthermore, users that want to cache both pointers may
 * find it a bit asymmetric, but that's ok.
 */
struct rb_root_cached {
	struct rb_root rb_root;
	struct rb_node *rb_leftmost;
};
----
tasks_timeline->rb_root: 红黑树的根
tasks_timeline->rb_leftmost: 红黑树中最左边的调度实体，即虚拟时间最小的调度实体。
每个就绪态的调度实体sched_entity包含插入红黑树中使用的节点rb_node，vruntime则记录已经运行的虚拟时间。

参考: https://github.com/torvalds/linux/blob/master/Documentation/scheduler/sched-design-CFS.rst
参考: https://lwn.net/Kernel/Index/#Completely_fair_scheduler
参考: http://www.wowotech.net/process_management/447.html
参考: https://s3.shizhz.me/linux-sched/cfs-sched
参考: https://cloud.tencent.com/developer/article/1603981

==== 闲时调度(idle)
▪ 闲时调度类(idle_sched_class):
内核当CPU没有其它进程可以执行的时候就会运行闲时调度类的进程。
闲时调度类的进程是在内核启动时就创建好的，每个CPU一个进程即idle进程(注意，闲时调度类和分时调度类中SCHED_IDLE调度策略不是一个概念，二者之间没有关系)。

https://elixir.bootlin.com/linux/latest/source/kernel/sched/idle.c

=== 均衡
个体均衡: 选择到一个相对清闲的CPU上去运行。
总体均衡: 从其它CPU上拉取一些进程到自己这来运行。

个体均衡的时机：
新进程刚创建时
进程要执行新程序时
进程被唤醒时
在这几个时机上进程可以选择去哪个CPU的运行队列上去等待执行。

总体均衡的时机：
CPU即将idle前会去找到最忙的CPU然后拉取一些任务过来；
定时器中断的周期性检测是否所有的CPU的忙闲成都，如果差别太大就会迁移进程；
在idle进程中如果CPU发现自己太忙而有的CPU在idle就会将其唤醒进行负载均衡。

load_balance(): https://elixir.bootlin.com/linux/latest/source/kernel/sched/fair.c

scheduler_tick(): https://elixir.bootlin.com/linux/latest/source/kernel/sched/core.c

https://docs.kernel.org/scheduler/sched-domains.html
https://zhuanlan.zhihu.com/p/499169668

=== 亲和性
系统调用:
https://man7.org/linux/man-pages/man2/sched_setaffinity.2.html
int sched_setaffinity(pid_t pid, size_t cpusetsize, const cpu_set_t *mask);
int sched_getaffinity(pid_t pid, size_t cpusetsize, cpu_set_t *mask);

实现:

    SYSCALL_DEFINE3(sched_setaffinity: https://elixir.bootlin.com/linux/latest/source/kernel/sched/core.c
        sched_setaffinity(pid, new_mask)
            __sched_setaffinity()
                __set_cpus_allowed_ptr()
                    __set_cpus_allowed_ptr_locked()

=== PREEMPT_RT
https://lwn.net/Articles/146861/
https://wiki.linuxfoundation.org/realtime/start

=== v0.12
https://elixir.bootlin.com/linux/0.12/source/kernel/sched.c

=== 参考
https://github.com/torvalds/linux/tree/master/Documentation/scheduler
https://docs.kernel.org/scheduler/index.html
https://www.cnblogs.com/linhaostudy/p/9847909.html
https://github.com/oska874/process-scheduling-in-linux/blob/master/linux_scheduler_notes_final.pdf
https://zhuanlan.zhihu.com/p/548740742
https://trepo.tuni.fi/bitstream/handle/10024/96864/GRADU-1428493916.pdf
https://z.itpub.net/article/detail/4D5A28D92B6F801DF6E4C7F793969996
https://lwn.net/Kernel/Index/#Realtime
https://lwn.net/Kernel/Index/#Scheduler