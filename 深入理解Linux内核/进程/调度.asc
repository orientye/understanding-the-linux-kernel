:toc:
:toclevels: 5
:hardbreaks-option:

== 调度

=== 演进
- Traditional Scheduler
v1.0 - v2.4
这一阶段的调度器和传统的UNIX上的调度器逻辑是一样的。全局只有一个运行队列，所有进程都放在一个队列上。进程区分IO密集型和CPU密集型，根据进程的睡眠时间计算动态优先级，按照动态优先级决定进程的调度顺序，按照优先级分配进程的时间片大小，时间片大小是等差数列。进程在运行队列上并没有特定的排序，每次选择下一个进程的时候都要遍历整个队列，因此算法的时间复杂度是O(n)。在SMP上也只有一个运行队列，当CPU比较多进程也比较多的时候，锁冲突比较严重。

- O(1) Scheduler:
v2.5 - v2.6.22
此调度器主要是针对传统调度器进行了改进。首先把运行队列从单一变量变成了per-CPU变量，每个CPU一个运行队列，这样就不会有锁冲突了，不过这样就需要调度均衡了。其次把运行队列的一个链表分成了两个链表数组：活动数组和过期数组。时间片没用耗尽的进程放在活动数组中，时间片耗尽的进程放在过期数组中，当所有进程的时间片都耗尽的时候交换两个数组，重新分配时间片。两个数组都使用动态优先级排序，并用bitmap来表示哪个优先级队列中有可运行的进程，把选择进程的算法复杂度降低到了O(1)。对进程区分IO密集型和CPU密集型并计算动态优先级这一点和传统调度器一样没有变。

- SD Scheduler:
未合入
楼梯调度器，它是对O(1)调度器的改进，算法复杂还是O(1)。之前的调度器都区分IO密集型和CPU密集型，算法要对进程的类型进行探测，根据探测结果调整动态优先级。这就有很大的问题，探测不一定准确，而且进程还可以欺骗调度器，最终会导致调度有很大的不公平性。楼梯调度器是第一次尝试使用公平调度算法，它废除了动态优先级，不再区分IO密集型进程和CPU密集型进程，但是仍然让IO密集型进程保持较高的响应性。在实现上，楼梯调度算法废弃了过期数组，只使用一个数组。当进程使用完自己的时间片之后，其时间片就会被减半并下降到下一个优先级，其本身的优先级还是不变的。当进程下降到最后一个优先级之后就再回到它本来的优先级队列并重新分配时间片。整个过程就像下楼梯一样，所以这个算法就叫做楼梯调度器。此算法虽然没有合入到标准内核，但是它第一次证明了可以采取完全公平的思想进行调度，也就是不区分IO密集型和CPU密集型进程。

- RSDL Scheduler(Rotating Staircase Deadline Scheduler):
未合入
旋转楼梯调度器，是对楼梯调度器的改进。它又重新引入了过期数组，为每个优先级都分配一个组时间配额记为Tg，进程本身的时间片记为Tp。当进程用完自己的时间片时会下降一个优先级，当一个优先级的Tg被用完时，组内所有的进程都会下降一个优先级。进程下降到最低优先级之后会被放入过期数组，当活动数组为空时就会交换活动数组和过期数组。由于加了Tg，使得低优先级进程的调度延迟可控，进一步增加了公平性。
https://lwn.net/Articles/224654/
尽管 RSDL 在测试中表现出色，特别是桌面响应性方面，但最终 Linus Torvalds 选择了 Ingo Molnár 的 CFS 作为主线调度器。原因可能涉及代码融合、维护性以及CFS更通用的设计。
不过 Con Kolivas 的工作极大地推动了调度器的发展，证明了公平调度器的可行性和巨大优势，直接或间接地促进了 O(1) 调度器的淘汰和 CFS 的诞生。
[cols="1,2,2", options="header"]
|===
| 特性 | 旋转楼梯调度器 (RSDL) | 完全公平调度器 (CFS)
| 核心抽象 | 楼梯和优先级台阶 | 红黑树和虚拟运行时间（vruntime）
| 公平性实现 | 周期性的优先级重置 | 总是选择 vruntime 最小的进程运行
| 交互性处理 | 唤醒的进程放到最高优先级 | 唤醒的进程获得一个 vruntime 补偿，使其能尽快被调度
| 数据结构 | 优先级数组（多个链表） | 一棵红黑树 
| 复杂度 | O(1)（在每个优先级链表上） | O(log N)（红黑树操作）
| 现状 | 未被主线采纳，作为历史概念和补丁存在 | Linux 内核默认的进程调度器
|===

- CFS(Completely Fair Scheduler):
v2.6.23 - now (Ingo_Molnár，匈牙利人，O(1)、CFS、PREEMPT_RT的作者，https://en.wikipedia.org/wiki/Ingo_Moln%C3%A1r)
完全公平调度器，从SD/RSDL中吸取了完全公平的思想，不再区分IO密集型进程与CPU密集型进程，不再根据睡眠时间调整动态优先级，所有普通进程都按优先级相对平分CPU时间，算法复杂度是O(logn)。此时对调度器框架也进行了重构，和之前的有很大的不同。之前的调度器是一个算法调度所有进程，在算法内部区别对待实时进程和普通进程。现在的调度器框架是先区分不同类型的进程，普通进程一个调度类，实时进程一个调度类，调度类里面再去实现具体的调度算法。CFS是普通调度类的算法。

- 抢占(preemptive)
Prior to Linux kernel version 2.5.4, Linux Kernel was not preemptive which means a process running in kernel mode cannot be moved out of processor until it itself leaves the processor or it starts waiting for some input output operation to get complete.
参考: https://stackoverflow.com/questions/5283501/what-does-it-mean-to-say-linux-kernel-is-preemptive

- 普通进程调度器(SCHED_OTHER)
O(n) -> O(1)调度器 -> CFS调度器(Completely Fair Scheduler)
具体来说:
基于时间片轮询调度算法O(n)(2.6之前的版本)
O(1)调度算法(2.6.23之前的版本): 在常数时间内完成工作，不依赖于运行的进程数量
完全公平调度算法(2.6.23以及之后的版本)

- 调度域(scheduling domains)
2.6.7
主要解决NUMA架构上的调度问题
https://lwn.net/Articles/80911/
https://www.kernel.org/doc/html/latest/scheduler/sched-domains.html

- SCHED_BATCH
Since 2.6.16
适合非交互性, CPU密集型的任务

- SCHED_IDLE
Since 2.6.23
系统在空闲时，每个CPU都有一个idle线程在跑，它什么也不做，就是把CPU放入硬件睡眠状态以节能(需要特定CPU的driver支持), 并等待新的任务到来，以将CPU从睡眠状态中唤醒。

- 普通进程的组调度支持(Fair Group Scheduling)
2.6.24
假设有两个用户，一个用户跑了9个进程，另一个用户只跑了一个进程，这将导致第二个用户的体验可能会比较差。
Group Scheduling解决了这一问题，它为每个用户建立一个组，组里放该用户所有进程，从而保证用户间的公平性。

- 实时进程的组调度支持(RT Group Scheduling)
2.6.25
类似普通进程的组调度，只不过针对实时进程

- 更精确的调度时钟(HRTICK)
2.6.25
CPU的周期性调度和基于时间片的调度，都是基于时钟中断来触发的。
High Resolution Tick能够提供更精确的调度时钟中断，它基于高精度时钟(High Resolution Timer)，即可以提供纳秒级别精度的硬件时钟。

- 自动组调度(Auto Group Scheduling)
2.6.38
跟某一项任务相关的所有进程可以放在一个会话里，把这些不同会话自动分成不同的调度组，从而利用组调度的优势。

- SCHED_DEADLINE
Since version 3.14
Deadline scheduling is thus useful for realtime tasks, where completion by a deadline is a key requirement. It is also applicable to periodic tasks like streaming media processing.
In order to fulfill the guarantees that are made when a thread is admitted to the SCHED_DEADLINE policy, SCHED_DEADLINE threads are
the highest priority (user controllable) threads in the system; if any SCHED_DEADLINE thread is runnable, it will preempt any thread scheduled under one of the other policies.
https://lwn.net/Articles/575497/

- 组调度带宽控制(CFS bandwidth control)
3.2
让管理员控制在一段时间内一个组可以使用CPU的最长时间

- 自动NUMA均衡(Automatic NUMA balancing)
3.8
https://lwn.net/Articles/524977/

- CPU调度与节能
https://lwn.net/Articles/655479/
https://lwn.net/Kernel/Index/#Scheduler-and_power_management
https://lwn.net/Kernel/Index/#Power_management-CPU_scheduling

- Real Time 支持
6.12 September 20, 2024
PREEMPT_RT补丁正式并入主分支
https://en.wikipedia.org/wiki/PREEMPT_RT
https://kernelnewbies.org/Linux_6.12#Real_Time_support

- EEVDF scheduler
6.12
Earliest Eligible Virtual Deadline First是一种实时调度算法。
在EEVDF中，每个任务都有一个虚拟的截止日期。调度器会根据任务的虚拟截止日期来确定任务的优先级。任务的虚拟截止日期是基于任务的到达时间、执行时间和周期（如果是周期性任务）等因素计算出来的。
当处理器空闲时，调度器会选择具有最早虚拟截止日期的任务来执行。例如，假设有三个任务T1、T2和T3，它们的虚拟截止日期分别为t1、t2和t3，且t1 < t2 < t3，那么在处理器空闲时，T1会首先被调度执行。
https://kernelnewbies.org/Linux_6.12#Complete_the_EEVDF_task_scheduler
https://github.com/torvalds/linux/blob/master/Documentation/scheduler/sched-eevdf.rst
https://lwn.net/Articles/925371/
https://lore.kernel.org/lkml/20230306132521.968182689@infradead.org/T/

- sched_ext
6.12
sched_ext is a scheduler class whose behavior can be defined by a set of BPF
programs - the BPF scheduler.
https://kernelnewbies.org/Linux_6.12#BPF_based_task_scheduling_algorithms_with_sched_ext
https://www.kernel.org/doc/Documentation/scheduler/sched-ext.rst
https://lwn.net/Articles/926501/
https://github.com/torvalds/linux/commit/bc8198dc7ebc492ec3e9fa1617dcdfbe98e73b17
https://github.com/torvalds/linux/commit/feacb1774bd5eac6382990d0f6d1378dc01dd78f

- 参考
https://www.kernel.org/doc/html/latest/scheduler/index.html
https://man7.org/linux/man-pages/man7/sched.7.html
https://lwn.net/Kernel/Index/#Scheduler
https://www.zhihu.com/question/35484429
http://www.wowotech.net/process_management/scheduler-history.html

=== 目标
调度器的任务是分配CPU资源，linux作为通用操作系统，需要兼顾效率与公平:
效率主要体现在让CPU都(均衡性)忙碌起来，并具有较高的吞吐量和较低的延迟(吞吐量和延迟通常是一对相互矛盾的指标)。
公平的意义是让每个任务都有机会获得调度，保证每个进程获得合理的CPU时间。

不同的任务的要求可能是不同的，有几种比较典型的任务: 强调吞吐量对延迟不敏感的CPU密集型任务如批处理进程，以及延迟敏感不太在意吞吐量的IO密集型任务如交互式进程，另外还有一种实时任务，它注重对程序执行的可预测性。

- Linux 调度器的设计不是一个单一目标的优化问题，而是一个持续的权衡问题:
** 吞吐量 vs 响应延迟
    给任务更长的时间片有利于提高缓存命中率和吞吐量，但会降低响应速度。CFS 通过给交互式任务（经常睡眠等待I/O）以奖励（vruntime 增长更慢）来巧妙平衡这一点。
** 公平性 vs 吞吐量
    绝对的公平可能会增加上下文切换次数，从而降低吞吐量。
** 负载均衡 vs 缓存亲和性
    为了平衡负载，可能需要将任务迁移到另一个空闲核心，但这会牺牲该任务的缓存局部性，可能导致性能暂时下降。
** 性能 vs 功耗
    为了提升性能，需要唤醒所有核心；为了节省功耗，则需要让尽可能多的核心休眠。
** 默认的 CFS 调度器，正是这些设计目标的一种体现:
    它通过红黑树数据结构来高效管理任务，通过虚拟运行时间 (vruntime) 的概念来精确实现加权公平，并通过启发式方法识别交互式任务以提升响应性，同时很好地支持了 SMP 和 NUMA 系统。

=== 示意图
image::img/schedule.png[]

=== 数据结构

==== task_struct中的调度相关信息
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/sched.h
----
struct task_struct {
    //...
	int				on_rq;

	int				prio;
	int				static_prio;
	int				normal_prio;
	unsigned int			rt_priority;

	struct sched_entity		se;
	struct sched_rt_entity		rt;
	struct sched_dl_entity		dl;
	struct sched_dl_entity		*dl_server;
#ifdef CONFIG_SCHED_CLASS_EXT
	struct sched_ext_entity		scx;
#endif
	const struct sched_class	*sched_class;

#ifdef CONFIG_SCHED_CORE
	struct rb_node			core_node;
	unsigned long			core_cookie;
	unsigned int			core_occupation;
#endif

#ifdef CONFIG_CGROUP_SCHED
	struct task_group		*sched_task_group;
#endif
    //...
}
----

int				on_rq: { 0, 1 = TASK_ON_RQ_QUEUED, 2 = TASK_ON_RQ_MIGRATING }

优先级:
int				prio: 进程的有效优先级(effective priority)，判断进程优先级时使用该参数，其取值范围为[0,139]，值越小，优先级越低
int				static_prio: 普通进程的静态优先级，代表进程的固有属性，值越小优先级越高
int				normal_prio: 基于static_prio或rt_priority，会被统一为值越小优先级越高
unsigned int			rt_priority: 实时进程的静态优先级，代表进程的固有属性，值越大优先级越高

调度实体:
	struct sched_entity		se;
	struct sched_rt_entity		rt;
	struct sched_dl_entity		dl;

const struct sched_class	*sched_class: 调度类

==== struct rq
[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/sched/sched.h
----
/*
 * This is the main, per-CPU runqueue data structure.
 *
 * Locking rule: those places that want to lock multiple runqueues
 * (such as the load balancing or the thread migration code), lock
 * acquire operations must be ordered by ascending &runqueue.
 */
struct rq {
	/* runqueue lock: */
	raw_spinlock_t		__lock;

	unsigned int		nr_running;
#ifdef CONFIG_NUMA_BALANCING
	unsigned int		nr_numa_running;
	unsigned int		nr_preferred_running;
	unsigned int		numa_migrate_on;
#endif
#ifdef CONFIG_NO_HZ_COMMON
#ifdef CONFIG_SMP
	unsigned long		last_blocked_load_update_tick;
	unsigned int		has_blocked_load;
	call_single_data_t	nohz_csd;
#endif /* CONFIG_SMP */
	unsigned int		nohz_tick_stopped;
	atomic_t		nohz_flags;
#endif /* CONFIG_NO_HZ_COMMON */

#ifdef CONFIG_SMP
	unsigned int		ttwu_pending;
#endif
	u64			nr_switches;

#ifdef CONFIG_UCLAMP_TASK
	/* Utilization clamp values based on CPU's RUNNABLE tasks */
	struct uclamp_rq	uclamp[UCLAMP_CNT] ____cacheline_aligned;
	unsigned int		uclamp_flags;
#define UCLAMP_FLAG_IDLE 0x01
#endif

	struct cfs_rq		cfs;
	struct rt_rq		rt;
	struct dl_rq		dl;
#ifdef CONFIG_SCHED_CLASS_EXT
	struct scx_rq		scx;
#endif

	struct sched_dl_entity	fair_server;

#ifdef CONFIG_FAIR_GROUP_SCHED
	/* list of leaf cfs_rq on this CPU: */
	struct list_head	leaf_cfs_rq_list;
	struct list_head	*tmp_alone_branch;
#endif /* CONFIG_FAIR_GROUP_SCHED */

	/*
	 * This is part of a global counter where only the total sum
	 * over all CPUs matters. A task can increase this counter on
	 * one CPU and if it got migrated afterwards it may decrease
	 * it on another CPU. Always updated under the runqueue lock:
	 */
	unsigned long 		nr_uninterruptible;

	union {
		struct task_struct __rcu *donor; /* Scheduler context */
		struct task_struct __rcu *curr;  /* Execution context */
	};
	struct sched_dl_entity	*dl_server;
	struct task_struct	*idle;
	struct task_struct	*stop;
	unsigned long		next_balance;
	struct mm_struct	*prev_mm;

	unsigned int		clock_update_flags;
	u64			clock;
	/* Ensure that all clocks are in the same cache line */
	u64			clock_task ____cacheline_aligned;
	u64			clock_pelt;
	unsigned long		lost_idle_time;
	u64			clock_pelt_idle;
	u64			clock_idle;
#ifndef CONFIG_64BIT
	u64			clock_pelt_idle_copy;
	u64			clock_idle_copy;
#endif

	atomic_t		nr_iowait;

	u64 last_seen_need_resched_ns;
	int ticks_without_resched;

#ifdef CONFIG_MEMBARRIER
	int membarrier_state;
#endif

#ifdef CONFIG_SMP
	struct root_domain		*rd;
	struct sched_domain __rcu	*sd;

	unsigned long		cpu_capacity;

	struct balance_callback *balance_callback;

	unsigned char		nohz_idle_balance;
	unsigned char		idle_balance;

	unsigned long		misfit_task_load;

	/* For active balancing */
	int			active_balance;
	int			push_cpu;
	struct cpu_stop_work	active_balance_work;

	/* CPU of this runqueue: */
	int			cpu;
	int			online;

	struct list_head cfs_tasks;

	struct sched_avg	avg_rt;
	struct sched_avg	avg_dl;
#ifdef CONFIG_HAVE_SCHED_AVG_IRQ
	struct sched_avg	avg_irq;
#endif
#ifdef CONFIG_SCHED_HW_PRESSURE
	struct sched_avg	avg_hw;
#endif
	u64			idle_stamp;
	u64			avg_idle;

	/* This is used to determine avg_idle's max value */
	u64			max_idle_balance_cost;

#ifdef CONFIG_HOTPLUG_CPU
	struct rcuwait		hotplug_wait;
#endif
#endif /* CONFIG_SMP */

#ifdef CONFIG_IRQ_TIME_ACCOUNTING
	u64			prev_irq_time;
	u64			psi_irq_time;
#endif
#ifdef CONFIG_PARAVIRT
	u64			prev_steal_time;
#endif
#ifdef CONFIG_PARAVIRT_TIME_ACCOUNTING
	u64			prev_steal_time_rq;
#endif

	/* calc_load related fields */
	unsigned long		calc_load_update;
	long			calc_load_active;

#ifdef CONFIG_SCHED_HRTICK
#ifdef CONFIG_SMP
	call_single_data_t	hrtick_csd;
#endif
	struct hrtimer		hrtick_timer;
	ktime_t			hrtick_time;
#endif

#ifdef CONFIG_SCHEDSTATS
	/* latency stats */
	struct sched_info	rq_sched_info;
	unsigned long long	rq_cpu_time;

	/* sys_sched_yield() stats */
	unsigned int		yld_count;

	/* schedule() stats */
	unsigned int		sched_count;
	unsigned int		sched_goidle;

	/* try_to_wake_up() stats */
	unsigned int		ttwu_count;
	unsigned int		ttwu_local;
#endif

#ifdef CONFIG_CPU_IDLE
	/* Must be inspected within a RCU lock section */
	struct cpuidle_state	*idle_state;
#endif

#ifdef CONFIG_SMP
	unsigned int		nr_pinned;
#endif
	unsigned int		push_busy;
	struct cpu_stop_work	push_work;

#ifdef CONFIG_SCHED_CORE
	/* per rq */
	struct rq		*core;
	struct task_struct	*core_pick;
	struct sched_dl_entity	*core_dl_server;
	unsigned int		core_enabled;
	unsigned int		core_sched_seq;
	struct rb_root		core_tree;

	/* shared state -- careful with sched_core_cpu_deactivate() */
	unsigned int		core_task_seq;
	unsigned int		core_pick_seq;
	unsigned long		core_cookie;
	unsigned int		core_forceidle_count;
	unsigned int		core_forceidle_seq;
	unsigned int		core_forceidle_occupation;
	u64			core_forceidle_start;
#endif

	/* Scratch cpumask to be temporarily used under rq_lock */
	cpumask_var_t		scratch_mask;

#if defined(CONFIG_CFS_BANDWIDTH) && defined(CONFIG_SMP)
	call_single_data_t	cfsb_csd;
	struct list_head	cfsb_csd_list;
#endif
};
----

struct rq: 运行队列即Run Queue，每一个CPU上有一个struct rq。
struct cfs_rq		cfs: 公平调度运行/就绪队列
struct rt_rq		rt: 实时调度运行/就绪队列
struct dl_rq		dl: 限时调度运行/就绪队列
#ifdef CONFIG_SCHED_CLASS_EXT
	struct scx_rq		scx;
#endif

==== 调度实体(sched_entity)
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/sched.h
----
struct sched_entity {
	/* For load-balancing: */
	struct load_weight		load;
	struct rb_node			run_node;
	u64				deadline;
	u64				min_vruntime;
	u64				min_slice;

	struct list_head		group_node;
	unsigned char			on_rq;
	unsigned char			sched_delayed;
	unsigned char			rel_deadline;
	unsigned char			custom_slice;
					/* hole */

	u64				exec_start;
	u64				sum_exec_runtime;
	u64				prev_sum_exec_runtime;
	u64				vruntime;
	s64				vlag;
	u64				slice;

	u64				nr_migrations;

#ifdef CONFIG_FAIR_GROUP_SCHED
	int				depth;
	struct sched_entity		*parent;
	/* rq on which this entity is (to be) queued: */
	struct cfs_rq			*cfs_rq;
	/* rq "owned" by this entity/group: */
	struct cfs_rq			*my_q;
	/* cached value of my_q->h_nr_running */
	unsigned long			runnable_weight;
#endif

#ifdef CONFIG_SMP
	/*
	 * Per entity load average tracking.
	 *
	 * Put into separate cache line so it does not
	 * collide with read-mostly values above.
	 */
	struct sched_avg		avg;
#endif
};
----

sched_entity是fair_sched_class的调度实体。
此外还有:
struct sched_rt_entity: 对应rt_sched_clas调度实体
struct sched_dl_entity: 对应dl_sched_class调度实体

struct load_weight		load: 权重信息，在计算虚拟时间的时候会用到inv_weight成员。
struct rb_node			run_node: 红黑树节点，CFS调度器的每个就绪队列维护了一颗红黑树，挂载就绪等待执行的task。
unsigned int			on_rq: 调度实体se加入就绪队列后，on_rq置1；从就绪队列删除后，on_rq置0。
u64				sum_exec_runtime: 调度实体已经运行实际时间总和。
u64				vruntime: 调度实体已经运行的虚拟时间总和。

vs. 调度类
调度类 (sched_class):
一个包含函数指针的结构体，定义了调度算法(如: 如何选择下一个任务、如何将任务加入队列等)。
调度实体 (sched_entity 及其变体例如sched_rt_entity、sched_dl_entity):
一个数据结构，存储了特定调度算法所需的信息和状态。

==== 调度类(scheduler class)
===== sched_class接口
[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/sched/sched.h
----
struct sched_class {

#ifdef CONFIG_UCLAMP_TASK
	int uclamp_enabled;
#endif

	void (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags);
	bool (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags);
	void (*yield_task)   (struct rq *rq);
	bool (*yield_to_task)(struct rq *rq, struct task_struct *p);

	void (*wakeup_preempt)(struct rq *rq, struct task_struct *p, int flags);

	int (*balance)(struct rq *rq, struct task_struct *prev, struct rq_flags *rf);
	struct task_struct *(*pick_task)(struct rq *rq);
	/*
	 * Optional! When implemented pick_next_task() should be equivalent to:
	 *
	 *   next = pick_task();
	 *   if (next) {
	 *       put_prev_task(prev);
	 *       set_next_task_first(next);
	 *   }
	 */
	struct task_struct *(*pick_next_task)(struct rq *rq, struct task_struct *prev);

	void (*put_prev_task)(struct rq *rq, struct task_struct *p, struct task_struct *next);
	void (*set_next_task)(struct rq *rq, struct task_struct *p, bool first);

#ifdef CONFIG_SMP
	int  (*select_task_rq)(struct task_struct *p, int task_cpu, int flags);

	void (*migrate_task_rq)(struct task_struct *p, int new_cpu);

	void (*task_woken)(struct rq *this_rq, struct task_struct *task);

	void (*set_cpus_allowed)(struct task_struct *p, struct affinity_context *ctx);

	void (*rq_online)(struct rq *rq);
	void (*rq_offline)(struct rq *rq);

	struct rq *(*find_lock_rq)(struct task_struct *p, struct rq *rq);
#endif

	void (*task_tick)(struct rq *rq, struct task_struct *p, int queued);
	void (*task_fork)(struct task_struct *p);
	void (*task_dead)(struct task_struct *p);

	/*
	 * The switched_from() call is allowed to drop rq->lock, therefore we
	 * cannot assume the switched_from/switched_to pair is serialized by
	 * rq->lock. They are however serialized by p->pi_lock.
	 */
	void (*switching_to) (struct rq *this_rq, struct task_struct *task);
	void (*switched_from)(struct rq *this_rq, struct task_struct *task);
	void (*switched_to)  (struct rq *this_rq, struct task_struct *task);
	void (*reweight_task)(struct rq *this_rq, struct task_struct *task,
			      const struct load_weight *lw);
	void (*prio_changed) (struct rq *this_rq, struct task_struct *task,
			      int oldprio);

	unsigned int (*get_rr_interval)(struct rq *rq,
					struct task_struct *task);

	void (*update_curr)(struct rq *rq);

#ifdef CONFIG_FAIR_GROUP_SCHED
	void (*task_change_group)(struct task_struct *p);
#endif

#ifdef CONFIG_SCHED_CORE
	int (*task_is_throttled)(struct task_struct *p, int cpu);
#endif
};
//...
extern const struct sched_class stop_sched_class;
extern const struct sched_class dl_sched_class;
extern const struct sched_class rt_sched_class;
extern const struct sched_class fair_sched_class;
extern const struct sched_class idle_sched_class;
----

- CONFIG_UCLAMP_TASK
** 是一个内核编译选项，默认CONFIG_UCLAMP_TASK=y。
启用它后，引入了两个新的每任务 (per-task) 的钳位 (Clamp) 属性:
** sched_util_min (uclamp_min): 性能需求下限
    *** 可以防止任务因为系统负载高而性能不足，确保其最低性能水平
** sched_util_max (uclamp_max): 性能需求上限
    *** 可以防止任务被过度加速，避免性能过剩，从而节省电量
** 参考
https://lwn.net/Articles/788428/
https://cateee.net/lkddb/web-lkddb/UCLAMP_TASK.html

===== 概念
从2.6.23开始，linux引入了调度类，其目的是将调度模块化，提高了扩展性，添加一个新的调度也变得简单一些。

▪ 调度类
调度类一共五种: stop(禁令调度类)、deadline(限时调度类)、realtime(实时调度类)、cfs(公平调度类)、idle(闲时调度类)。
其紧迫性从上到下，依次降低。
禁令调度类和闲时调度类仅用于内核，在内核启动时就设置好了，每个CPU一个相应的进程，因此也不需要调度算法。
其它调度类可以用于用户空间进程，有相应的调度策略和调度算法。
在有高调度类进程可运行的情况下，不会去调度低调度类的进程(也就是说，高优先级调度类的任务总是可以抢占低优先级调度类的任务)。例外: 内核为了防止实时进程饿死普通进程，提供了一个配置参数，超过一定的CPU时间，就会把剩余的CPU时间分给普通进程，默认值是95%。
显然，一个系统中可以共存多种调度。

▪ 调度类与优先级
五种调度类里:
禁令调度类和闲时调度类只在内核里使用，每CPU只有一个线程，用不到进程优先级;
限时调度类使用进程设置的调度参数作为调度依据，也用不到进程优先级;
只有实时调度类和公平调度类会用到进程优先级，不过它们使用优先级的方法并不相同。

===== 禁令调度(stop)
▪ 禁令调度类(stop_sched_class):
内核用来执行一些特别紧急的事物所使用的。
禁令调度类的进程是内核在启动时就创建好的，每个CPU一个进程即[migration/n]。
调度均衡要迁移线程的时候会用到这类进程，因此取名migration。
https://elixir.bootlin.com/linux/latest/source/include/linux/stop_machine.h
https://elixir.bootlin.com/linux/latest/source/kernel/stop_machine.c
禁令调度类的进程优先级是最高的，只要此类进程变得runnable了，就会立马抢占当前进程来运行。

https://elixir.bootlin.com/linux/latest/source/kernel/sched/stop_task.c

===== 限时调度(deadline)
▪ 限时调度类(dl_sched_class):
软实时，适用于对调度时间有明确要求的进程。
限时调度类只有一个调度策略，即限时调度策略SCHED_DEADLINE。
一个进程必须通过系统调用才能把自己设置为限时调度策略，并且还要提供三个参数：运行周期(多长时间内想要运行一次)、运行时间(每次想要运行多长时间)和截止时间。
同时设置不一定能成功，内核需要测与已有的限时调度类进程是否冲突，如果有冲突则只能返回失败。
另外如果实际运行时间超过了预估时间则进程就会被切走，这可能会导致灾难性的后果。

https://elixir.bootlin.com/linux/latest/source/kernel/sched/deadline.c

===== 实时调度(rt)
▪ 实时调度类(rt_sched_class):
属于软实时，适用于只要可运行就希望立马能执行的进程。
实时调度类有两个调度策略，SCHED_FIFO和SCHED_RR。
实时调度类的内部逻辑是让实时优先级大的进程先运行，只要有实时优先级大的进程可运行，就不会去调度实时优先级小的进程。
当两个实时进程的优先级相同时，SCHED_RR和SCHED_FIFO这两个策略就有区别了:
SCHED_FIFO进程如果抢占了CPU，它就会一直占着CPU，不会给同优先级的实时进程让CPU的，而SCHED_RR进程会在运行了一定的时间片之后主动让给同优先级的实时进程。

https://elixir.bootlin.com/linux/latest/source/kernel/sched/rt.c

===== 完全公平调度(cfs)
====== 概念
▪ 公平调度类(fair_sched_class)/分时调度类:
众多普通进程共同分享CPU。
根据优先级的不同，可能有的进程分的多有的进程分的少，但是不会出现一个进程霸占CPU的情况。
分时调度类有三个调度策略：SCHED_NORMAL、SCHED_BATCH和SCHED_IDLE。
SCHED_BATCH进程希望减少调度次数，每次调度能执行的时间长一点。
SCHED_IDLE是优先级特别低的进程，其分到的CPU时间的比例非常低，但是也总是能保证分到。

▪ 为什么需要完全公平调度？
1、传统调度器时间片悖论
在O(n)和O(1)调度器中，时间片是固定分配的，静态优先级高的进程获取更大的time slice。例如nice value等于20的进程获取的default time slice是5ms，而nice value等于0的进程获取的是100ms。直观上，这样的策略没有毛病（高优先级的获取更多的执行时间），但是，这样的设定潜台词就是: 高优先级的进程会获得更多的连续执行的机会，这是CPU-bound进程期望的，但是实际上，CPU-bound进程往往在后台执行，其优先级都是比较低的。
因此，假设调度策略就是根据进程静态优先级确定一个固定大小的时间片，这时候如何分配时间片上会遇到两难的状况:
想要给用户交互型进程设定高优先级，以便它能有更好的用户体验，但是分配一个大的时间片是毫无意义的，因为这种进程多半是处于阻塞态，等待用户的输入。而后台进程的优先级一般不高，但是根据其优先级分配一个较小的时间片往往会影响其性能。
怎么办？或者传统调度器固定分配时间片这个设计概念就是错误的。
2、传统调度器的卡顿问题
O(1)并非完美，在实际的使用过程中，还是有不少的桌面用户在抱怨用户交互性比较差。当一个相当消耗CPU资源的进程启动的时候，现存的那些用户交互程序（例如通过浏览器查看网页）都可以感觉到明显的延迟。针对这些issue，很多天才工程师试图通过对用户交互指数算法的的修改来解决问题，这里面就包括公平调度思想的提出者Con Kolivas。不过无论如何调整算法，总是有点拆东墙补西墙的感觉，一个场景的issue修复了，另外一个场景又冒出来交互性不好的issue，刁钻的客户总是能够在边边角角的场景中找到让用户感觉到的响应延迟。
参考: http://www.wowotech.net/process_management/scheduler-history.html

▪ 主要思想
通过追踪每个任务的虚拟运行时间(virtual runtime)，并总是让 vruntime 最小的任务运行，来在宏观上模拟一个所有任务同时获得相等虚拟运行时间的"理想公平"CPU。

▪ 实现原理
虚拟时间(virtual runtime):
the per-task p->se.vruntime (nanosec-unit) value，即task_struct里的struct sched_entity se里的vruntime字段。
每次调度挑选最小vruntime的调度实体/红黑树节点。
挑选最小的vruntime是通过维护rq->cfs.min_vruntime和红黑树来实现的。

▪ CFS的公平性体现在(同一个CPU上):
在一个调度周期内每个任务都有执行的机会;
实际执行时间的长短，取决于任务的权重;
同一个nice值/权重的任务，获得的实际执行时间相当;
尽量让不同任务的虚拟执行时间相等。

▪ 处理睡眠任务（I/O密集型）的公平性:
I/O 密集型任务经常睡眠（例如等待用户输入或磁盘读写）。当它们醒来时，其 vruntime 会远远小于一直在 CPU 上运行的计算密集型任务（因为睡眠时 vruntime 不增加）。
根据选择 vruntime 最小的任务规则，这个刚醒来的 I/O 密集型任务会立刻获得 CPU，因为它的 vruntime 最小。这提供了出色的交互性能，用户会感觉响应很快。
一旦它开始运行，其 vruntime 会迅速增加，很快又会把 CPU 让给其他任务，从而不会长时间霸占 CPU。

[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/sched/core.c
----
/*
 * Nice levels are multiplicative, with a gentle 10% change for every
 * nice level changed. I.e. when a CPU-bound task goes from nice 0 to
 * nice 1, it will get ~10% less CPU time than another CPU-bound task
 * that remained on nice 0.
 *
 * The "10% effect" is relative and cumulative: from _any_ nice level,
 * if you go up 1 level, it's -10% CPU usage, if you go down 1 level
 * it's +10% CPU usage. (to achieve that we use a multiplier of 1.25.
 * If a task goes up by ~10% and another task goes down by ~10% then
 * the relative distance between them is ~25%.)
 */
const int sched_prio_to_weight[40] = {
 /* -20 */     88761,     71755,     56483,     46273,     36291,
 /* -15 */     29154,     23254,     18705,     14949,     11916,
 /* -10 */      9548,      7620,      6100,      4904,      3906,
 /*  -5 */      3121,      2501,      1991,      1586,      1277,
 /*   0 */      1024,       820,       655,       526,       423,
 /*   5 */       335,       272,       215,       172,       137,
 /*  10 */       110,        87,        70,        56,        45,
 /*  15 */        36,        29,        23,        18,        15,
};

/*
 * Inverse (2^32/x) values of the sched_prio_to_weight[] array, pre-calculated.
 *
 * In cases where the weight does not change often, we can use the
 * pre-calculated inverse to speed up arithmetics by turning divisions
 * into multiplications:
 */
const u32 sched_prio_to_wmult[40] = {
 /* -20 */     48388,     59856,     76040,     92818,    118348,
 /* -15 */    147320,    184698,    229616,    287308,    360437,
 /* -10 */    449829,    563644,    704093,    875809,   1099582,
 /*  -5 */   1376151,   1717300,   2157191,   2708050,   3363326,
 /*   0 */   4194304,   5237765,   6557202,   8165337,  10153587,
 /*   5 */  12820798,  15790321,  19976592,  24970740,  31350126,
 /*  10 */  39045157,  49367440,  61356676,  76695844,  95443717,
 /*  15 */ 119304647, 148102320, 186737708, 238609294, 286331153,
};
----
sched_prio_to_weight: 转换nice值和权重

nice值就是一个具体的数字，取值范围是[-20, 19]。数值越小代表优先级越大，同时也意味着权重值越大，nice值和权重之间可以互相转换。
公式: weight = 1024 / 1.25^nice
nice值每减小1，weight值增长为1.25倍。
公式中的1024: 1024权重对应nice值为0，其权重被称为NICE_0_LOAD，称为基准值。默认情况下，大部分进程的权重基本都是NICE_0_LOAD。
公式中的1.25取值依据: 进程每降低一个nice值，将多获得10% cpu的时间。

例如:
两个进程A和B在nice级别0，即静态优先级120运行，因此两个进程的CPU份额相同，都是50%，nice级别为0的进程，其权重是1024。每个进程的份额是1024/(1024+1024)=0.5，即50%。
如果进程B的优先级降低，例如nice升为1，则其CPU份额应该减少10%: 进程B的nice升为1导致权重减少，即1024/1.25=820，进程A权重还是1024，那么进程A现在将得到的CPU份额是1024/(1024+820)=0.55，进程B的CPU份额则变为820/(1024+820)=0.45，这样正好产生了10%的差值。
如果进程B的优先级再降低，nice值升到2: 则进程A: 1024/(1024+655)=0.61，进程B: 655/(1024+655)=0.39，产生了20%左右的差值，即每次nice升1，会累积10%左右的差值。

                                    NICE_0_LOAD
    virtual_runtime = wall_time * ---------------- (wall_time: 实际运行时间)
                                        weight

                                    NICE_0_LOAD * 2^32
                    = (wall_time * -------------------------) >> 32 (避免浮点数运算，先放大再缩小以保证计算精度)
                                            weight
                                                                                     2^32
                    = (wall_time * NICE_0_LOAD * inv_weight) >> 32  (inv_weight = ------------)
                                                                                    weight 

sched_prio_to_wmult: 就是为了提升计算速度，对应上面公式中的inv_weight。

====== 实现
[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/sched/fair.c
----
/*
 * All the scheduling class methods:
 */
DEFINE_SCHED_CLASS(fair) = {

	.enqueue_task		= enqueue_task_fair,
	.dequeue_task		= dequeue_task_fair,
	.yield_task		= yield_task_fair,
	.yield_to_task		= yield_to_task_fair,

	.wakeup_preempt		= check_preempt_wakeup_fair,

	.pick_task		= pick_task_fair,
	.pick_next_task		= __pick_next_task_fair,
	.put_prev_task		= put_prev_task_fair,
	.set_next_task          = set_next_task_fair,

#ifdef CONFIG_SMP
	.balance		= balance_fair,
	.select_task_rq		= select_task_rq_fair,
	.migrate_task_rq	= migrate_task_rq_fair,

	.rq_online		= rq_online_fair,
	.rq_offline		= rq_offline_fair,

	.task_dead		= task_dead_fair,
	.set_cpus_allowed	= set_cpus_allowed_fair,
#endif

	.task_tick		= task_tick_fair,
	.task_fork		= task_fork_fair,

	.reweight_task		= reweight_task_fair,
	.prio_changed		= prio_changed_fair,
	.switched_from		= switched_from_fair,
	.switched_to		= switched_to_fair,

	.get_rr_interval	= get_rr_interval_fair,

	.update_curr		= update_curr_fair,

#ifdef CONFIG_FAIR_GROUP_SCHED
	.task_change_group	= task_change_group_fair,
#endif

#ifdef CONFIG_SCHED_CORE
	.task_is_throttled	= task_is_throttled_fair,
#endif

#ifdef CONFIG_UCLAMP_TASK
	.uclamp_enabled		= 1,
#endif
};
----

[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/sched/sched.h
----
/* CFS-related fields in a runqueue */
struct cfs_rq {
	struct load_weight	load;
	unsigned int		nr_queued;
	unsigned int		h_nr_queued;       /* SCHED_{NORMAL,BATCH,IDLE} */
	unsigned int		h_nr_runnable;     /* SCHED_{NORMAL,BATCH,IDLE} */
	unsigned int		h_nr_idle; /* SCHED_IDLE */

	s64			avg_vruntime;
	u64			avg_load;

	u64			min_vruntime;
#ifdef CONFIG_SCHED_CORE
	unsigned int		forceidle_seq;
	u64			min_vruntime_fi;
#endif

	struct rb_root_cached	tasks_timeline;

	/*
	 * 'curr' points to currently running entity on this cfs_rq.
	 * It is set to NULL otherwise (i.e when none are currently running).
	 */
	struct sched_entity	*curr;
	struct sched_entity	*next;

#ifdef CONFIG_SMP
	/*
	 * CFS load tracking
	 */
	struct sched_avg	avg;
#ifndef CONFIG_64BIT
	u64			last_update_time_copy;
#endif
	struct {
		raw_spinlock_t	lock ____cacheline_aligned;
		int		nr;
		unsigned long	load_avg;
		unsigned long	util_avg;
		unsigned long	runnable_avg;
	} removed;

#ifdef CONFIG_FAIR_GROUP_SCHED
	u64			last_update_tg_load_avg;
	unsigned long		tg_load_avg_contrib;
	long			propagate;
	long			prop_runnable_sum;

	/*
	 *   h_load = weight * f(tg)
	 *
	 * Where f(tg) is the recursive weight fraction assigned to
	 * this group.
	 */
	unsigned long		h_load;
	u64			last_h_load_update;
	struct sched_entity	*h_load_next;
#endif /* CONFIG_FAIR_GROUP_SCHED */
#endif /* CONFIG_SMP */

#ifdef CONFIG_FAIR_GROUP_SCHED
	struct rq		*rq;	/* CPU runqueue to which this cfs_rq is attached */

	/*
	 * leaf cfs_rqs are those that hold tasks (lowest schedulable entity in
	 * a hierarchy). Non-leaf lrqs hold other higher schedulable entities
	 * (like users, containers etc.)
	 *
	 * leaf_cfs_rq_list ties together list of leaf cfs_rq's in a CPU.
	 * This list is used during load balance.
	 */
	int			on_list;
	struct list_head	leaf_cfs_rq_list;
	struct task_group	*tg;	/* group that "owns" this runqueue */

	/* Locally cached copy of our task_group's idle value */
	int			idle;

#ifdef CONFIG_CFS_BANDWIDTH
	int			runtime_enabled;
	s64			runtime_remaining;

	u64			throttled_pelt_idle;
#ifndef CONFIG_64BIT
	u64                     throttled_pelt_idle_copy;
#endif
	u64			throttled_clock;
	u64			throttled_clock_pelt;
	u64			throttled_clock_pelt_time;
	u64			throttled_clock_self;
	u64			throttled_clock_self_time;
	int			throttled;
	int			throttle_count;
	struct list_head	throttled_list;
	struct list_head	throttled_csd_list;
#endif /* CONFIG_CFS_BANDWIDTH */
#endif /* CONFIG_FAIR_GROUP_SCHED */
};
----

struct load_weight	load: 就绪队列权重，就绪队列管理的所有调度实体权重之和。
unsigned int		nr_running: 就绪队列上调度实体的个数。
u64			min_vruntime: 跟踪就绪队列上所有调度实体的最小虚拟时间。

struct rb_root_cached	tasks_timeline: 按照虚拟时间排序的红黑树
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/rbtree_types.h
----
/*
 * Leftmost-cached rbtrees.
 *
 * We do not cache the rightmost node based on footprint
 * size vs number of potential users that could benefit
 * from O(1) rb_last(). Just not worth it, users that want
 * this feature can always implement the logic explicitly.
 * Furthermore, users that want to cache both pointers may
 * find it a bit asymmetric, but that's ok.
 */
struct rb_root_cached {
	struct rb_root rb_root;
	struct rb_node *rb_leftmost;
};
----
tasks_timeline->rb_root: 红黑树的根
tasks_timeline->rb_leftmost: 红黑树中最左边的调度实体，即虚拟时间最小的调度实体。
每个就绪态的调度实体sched_entity包含插入红黑树中使用的节点rb_node，vruntime则记录已经运行的虚拟时间。

====== 参考
https://github.com/torvalds/linux/blob/master/Documentation/scheduler/sched-design-CFS.rst
https://lwn.net/Kernel/Index/#Completely_fair_scheduler
http://www.wowotech.net/process_management/447.html
https://s3.shizhz.me/linux-sched/cfs-sched

===== 闲时调度(idle)
▪ 闲时调度类(idle_sched_class):
内核当CPU没有其它进程可以执行的时候就会运行闲时调度类的进程。
闲时调度类的进程是在内核启动时就创建好的，每个CPU一个进程即idle进程(注意，闲时调度类和分时调度类中SCHED_IDLE调度策略不是一个概念，二者之间没有关系)。

https://elixir.bootlin.com/linux/latest/source/kernel/sched/idle.c

==== 调度策略(scheduling policy)
- vs. 调度类
[cols="2,6,6", options="header"]
|===
| 特性 | 调度策略 (Scheduling Policy) | 调度类 (Scheduler Class)
| 面向对象 | 用户空间 (User-Space) | 内核内部 (Kernel-Space)
| 角色 | 定义行为 (What) - 需求的规格说明 | 实现算法 (How) - 执行的工具
| 数量 | 少数几种策略 (SCHED_*) | 少数几个类 (*_sched_class)
| 关系 | 多对一: 多个策略可能由同一个类实现 | 一对多: 一个类可以实现多种策略的行为
| 协作方式 | 用户为每个任务选择一种策略 | 内核按照固定的优先级顺序遍历调度类链表
|===
这种将策略和机制分离的设计，使得 Linux 调度器既能为用户提供清晰简单的接口，又能在内核内部保持高度模块化和可扩展性。

- 命令

    ps -eLfc命令可以查看调度策略(CLS列)
    cls         CLS       scheduling class of the process.(alias policy, cls). 
    Field possible values are:
        -   not reported
        TS  SCHED_OTHER
        FF  SCHED_FIFO
        RR  SCHED_RR
        B   SCHED_BATCH
        ISO SCHED_ISO
        IDL SCHED_IDLE
        DLN SCHED_DEADLINE
        ?   unknown value

- SCHED_FIFO与SCHED_RR

    这两个调度策略定义了对实时任务，即对延时和限定性的完成时间的高敏感度的任务。
    前者提供FIFO语义:
        相同优先级的任务先到先服务，高优先级的任务可以抢占低优先级的任务；
    后者提供Round-Robin语义，采用时间片:
        相同优先级的任务用完时间片后会被放到队列尾部以保证公平性，高优先级可以抢占低优先级的任务。
    不同要求的实时任务可以根据需要使用sched_setscheduler()函数设置策略。

- SCHED_OTHER

    实际上就是SCHED_NORMAL: https://man7.org/linux/man-pages/man7/sched.7.html
    此调度策略包含除上述实时进程之外的其它进程，亦称普通进程。
    采用分时策略，根据动态优先级(可用nice()函数设置)，分配CPU运算资源。
    注意: 这类进程比上述两类实时进程优先级低，在有实时进程存在时，实时进程会优先调度。

除了实现上述策略，linux还额外支持以下策略:

- SCHED_IDLE

    优先级最低，在系统空闲时才跑这类进程

- SCHED_BATCH

    SCHED_OTHER策略的分化，与SCHED_OTHER策略一样，但针对吞吐量优化

- SCHED_DEADLINE

    新支持的实时进程调度策略，针对突发型计算，且对延迟和完成时间高度敏感的任务适用

▪ 定义
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/uapi/linux/sched.h
----
/*
 * Scheduling policies
 */
#define SCHED_NORMAL	0
#define SCHED_FIFO		1
#define SCHED_RR		2
#define SCHED_BATCH		3
/* SCHED_ISO: reserved but not implemented yet */
#define SCHED_IDLE		5
#define SCHED_DEADLINE	6
#define SCHED_EXT		7
----

=== 调度时机
- 根据进程是否自愿放弃cpu，调度方式可分为 主动调度 和 抢占调度/被动调度/强制调度 两类:

    主动调度: 进程需要等待IO、锁等资源，主动放弃cpu
    抢占调度: 被优先级更高的任务抢占，被强制剥夺cpu

    内核中那些由于等待资源而需要阻塞的场景，会直接调用schedule()执行实际的调度流程。
    其它需要调度的场景一般都只是设置一个TIF_NEED_RESCHED标志，并在下一个抢占点到来时才执行实际的抢占操作。

- 自愿调度/主动调度

    内核空间:
        schedule()相关调用
        或调用schedule前设置进程状态为TASK_INTERRUPTIBLE/TASK_UNINTERRUPTIBLE
    用户空间:
        nanosleep()
        pause()
        sched_yield(): https://www.quora.com/What-is-the-difference-between-sleep-0-and-sched_yield
        几乎所有涉及外设系统调用函数，如open/read/write/select等

- 非自愿调度/被动调度/强制性调度

    每次从系统调用返回的前夕:
        do_syscall_64(): https://elixir.bootlin.com/linux/latest/source/arch/x86/entry/common.c
        	syscall_exit_to_user_mode: https://elixir.bootlin.com/linux/latest/source/kernel/entry/common.c
            	exit_to_user_mode_prepare() 
                	exit_to_user_mode_loop()
                    	schedule()
    每次从中断或异常处理返回:
        irqentry_exit(): https://elixir.bootlin.com/linux/latest/source/kernel/entry/common.c
            到用户空间的前夕:
                irqentry_exit_to_user_mode()
                    exit_to_user_mode_prepare()
                        exit_to_user_mode_loop()
                            schedule()
            到内核空间的前夕:
                irqentry_exit_cond_resched()
                    raw_irqentry_exit_cond_resched()
                        preempt_schedule_irq()
                            __schedule(SM_PREEMPT)

        旧: ret_from_intr(): https://elixir.bootlin.com/linux/2.4.0/source/arch/i386/kernel/entry.S#L273
            ret_with_reschedule()

    时钟中断:
        时钟中断处理程序 sched_tick()(以前叫scheduler_tick()) 会周期性触发。

    ARM64:
        exit_to_user_mode(): https://elixir.bootlin.com/linux/latest/source/arch/arm64/kernel/entry-common.c
            prepare_exit_to_user_mode()
                do_notify_resume(): https://elixir.bootlin.com/linux/latest/source/arch/arm64/kernel/signal.c
                    schedule()

=== 主要接口
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/sched.h
----
extern void sched_tick(void);
//...
extern long schedule_timeout(long timeout);
extern long schedule_timeout_interruptible(long timeout);
extern long schedule_timeout_killable(long timeout);
extern long schedule_timeout_uninterruptible(long timeout);
extern long schedule_timeout_idle(long timeout);
asmlinkage void schedule(void);
extern void schedule_preempt_disabled(void);
asmlinkage void preempt_schedule_irq(void);
#ifdef CONFIG_PREEMPT_RT
 extern void schedule_rtlock(void);
#endif

extern int __must_check io_schedule_prepare(void);
extern void io_schedule_finish(int token);
extern long io_schedule_timeout(long timeout);
extern void io_schedule(void);
----

[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/asm-generic/preempt.h
----
#ifdef CONFIG_PREEMPTION
extern asmlinkage void preempt_schedule(void);
extern asmlinkage void preempt_schedule_notrace(void);
//...
#endif /* CONFIG_PREEMPTION */
----

- preempt_schedule_irq()
在中断处理程序退出前夕，来检查并执行内核抢占。它通过临时打开中断并调用调度器，确保了因为当前中断而变得可运行的、更高优先级的任务能够立即得到调度，而不是等到中断返回到被中断的代码之后，这极大地减少了调度延迟。
https://elixir.bootlin.com/linux/latest/source/kernel/sched/core.c

=== 主调度schedule
==== 接口
void schedule()就是主调度的工作函数
[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/sched/core.c
----
static __always_inline void __schedule_loop(int sched_mode)
{
	do {
		preempt_disable();
		__schedule(sched_mode);
		sched_preempt_enable_no_resched();
	} while (need_resched());
}

asmlinkage __visible void __sched schedule(void)
{
	struct task_struct *tsk = current;

#ifdef CONFIG_RT_MUTEXES
	lockdep_assert(!tsk->sched_rt_mutex);
#endif

	if (!task_is_running(tsk))
		sched_submit_work(tsk);
	__schedule_loop(SM_NONE);
	sched_update_worker(tsk);
}
----
核心实现: __schedule(SM_NONE)

==== 实现
[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/sched/core.c
----
/*
 * __schedule() is the main scheduler function.
 *
 * The main means of driving the scheduler and thus entering this function are:
 *
 *   1. Explicit blocking: mutex, semaphore, waitqueue, etc.
 *
 *   2. TIF_NEED_RESCHED flag is checked on interrupt and userspace return
 *      paths. For example, see arch/x86/entry_64.S.
 *
 *      To drive preemption between tasks, the scheduler sets the flag in timer
 *      interrupt handler scheduler_tick().
 *
 *   3. Wakeups don't really cause entry into schedule(). They add a
 *      task to the run-queue and that's it.
 *
 *      Now, if the new task added to the run-queue preempts the current
 *      task, then the wakeup sets TIF_NEED_RESCHED and schedule() gets
 *      called on the nearest possible occasion:
 *
 *       - If the kernel is preemptible (CONFIG_PREEMPTION=y):
 *
 *         - in syscall or exception context, at the next outmost
 *           preempt_enable(). (this might be as soon as the wake_up()'s
 *           spin_unlock()!)
 *
 *         - in IRQ context, return from interrupt-handler to
 *           preemptible context
 *
 *       - If the kernel is not preemptible (CONFIG_PREEMPTION is not set)
 *         then at the next:
 *
 *          - cond_resched() call
 *          - explicit schedule() call
 *          - return from syscall or exception to user-space
 *          - return from interrupt-handler to user-space
 *
 * WARNING: must be called with preemption disabled!
 */
static void __sched notrace __schedule(unsigned int sched_mode)
{
	struct task_struct *prev, *next;
	unsigned long *switch_count;
	unsigned long prev_state;
	struct rq_flags rf;
	struct rq *rq;
	int cpu;

	cpu = smp_processor_id();
	rq = cpu_rq(cpu);
	prev = rq->curr;

	schedule_debug(prev, !!sched_mode);

	if (sched_feat(HRTICK) || sched_feat(HRTICK_DL))
		hrtick_clear(rq);

	local_irq_disable();
	rcu_note_context_switch(!!sched_mode);

	/*
	 * Make sure that signal_pending_state()->signal_pending() below
	 * can't be reordered with __set_current_state(TASK_INTERRUPTIBLE)
	 * done by the caller to avoid the race with signal_wake_up():
	 *
	 * __set_current_state(@state)		signal_wake_up()
	 * schedule()				  set_tsk_thread_flag(p, TIF_SIGPENDING)
	 *					  wake_up_state(p, state)
	 *   LOCK rq->lock			    LOCK p->pi_state
	 *   smp_mb__after_spinlock()		    smp_mb__after_spinlock()
	 *     if (signal_pending_state())	    if (p->state & @state)
	 *
	 * Also, the membarrier system call requires a full memory barrier
	 * after coming from user-space, before storing to rq->curr.
	 */
	rq_lock(rq, &rf);
	smp_mb__after_spinlock();

	/* Promote REQ to ACT */
	rq->clock_update_flags <<= 1;
	update_rq_clock(rq);

	switch_count = &prev->nivcsw;

	/*
	 * We must load prev->state once (task_struct::state is volatile), such
	 * that we form a control dependency vs deactivate_task() below.
	 */
	prev_state = READ_ONCE(prev->__state);
	if (!(sched_mode & SM_MASK_PREEMPT) && prev_state) {
		if (signal_pending_state(prev_state, prev)) {
			WRITE_ONCE(prev->__state, TASK_RUNNING);
		} else {
			prev->sched_contributes_to_load =
				(prev_state & TASK_UNINTERRUPTIBLE) &&
				!(prev_state & TASK_NOLOAD) &&
				!(prev_state & TASK_FROZEN);

			if (prev->sched_contributes_to_load)
				rq->nr_uninterruptible++;

			/*
			 * __schedule()			ttwu()
			 *   prev_state = prev->state;    if (p->on_rq && ...)
			 *   if (prev_state)		    goto out;
			 *     p->on_rq = 0;		  smp_acquire__after_ctrl_dep();
			 *				  p->state = TASK_WAKING
			 *
			 * Where __schedule() and ttwu() have matching control dependencies.
			 *
			 * After this, schedule() must not care about p->state any more.
			 */
			deactivate_task(rq, prev, DEQUEUE_SLEEP | DEQUEUE_NOCLOCK);

			if (prev->in_iowait) {
				atomic_inc(&rq->nr_iowait);
				delayacct_blkio_start();
			}
		}
		switch_count = &prev->nvcsw;
	}

	next = pick_next_task(rq, prev, &rf);
	clear_tsk_need_resched(prev);
	clear_preempt_need_resched();
#ifdef CONFIG_SCHED_DEBUG
	rq->last_seen_need_resched_ns = 0;
#endif

	if (likely(prev != next)) {
		rq->nr_switches++;
		/*
		 * RCU users of rcu_dereference(rq->curr) may not see
		 * changes to task_struct made by pick_next_task().
		 */
		RCU_INIT_POINTER(rq->curr, next);
		/*
		 * The membarrier system call requires each architecture
		 * to have a full memory barrier after updating
		 * rq->curr, before returning to user-space.
		 *
		 * Here are the schemes providing that barrier on the
		 * various architectures:
		 * - mm ? switch_mm() : mmdrop() for x86, s390, sparc, PowerPC.
		 *   switch_mm() rely on membarrier_arch_switch_mm() on PowerPC.
		 * - finish_lock_switch() for weakly-ordered
		 *   architectures where spin_unlock is a full barrier,
		 * - switch_to() for arm64 (weakly-ordered, spin_unlock
		 *   is a RELEASE barrier),
		 */
		++*switch_count;

		migrate_disable_switch(rq, prev);
		psi_sched_switch(prev, next, !task_on_rq_queued(prev));

		trace_sched_switch(sched_mode & SM_MASK_PREEMPT, prev, next, prev_state);

		/* Also unlocks the rq: */
		rq = context_switch(rq, prev, next, &rf);
	} else {
		rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);

		rq_unpin_lock(rq, &rf);
		__balance_callbacks(rq);
		raw_spin_rq_unlock_irq(rq);
	}
}
----

可见__schedule()流程的核心是: pick_next_task()与context_switch()，即挑选任务与任务切换。

pick_next_task()的核心函数是__pick_next_task():
[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/sched/core.c
----
/*
 * Pick up the highest-prio task:
 */
static inline struct task_struct *
__pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
{
	const struct sched_class *class;
	struct task_struct *p;

	/*
	 * Optimization: we know that if all tasks are in the fair class we can
	 * call that function directly, but only if the @prev task wasn't of a
	 * higher scheduling class, because otherwise those lose the
	 * opportunity to pull in more work from other CPUs.
	 */
	if (likely(!sched_class_above(prev->sched_class, &fair_sched_class) &&
		   rq->nr_running == rq->cfs.h_nr_running)) {

		p = pick_next_task_fair(rq, prev, rf);
		if (unlikely(p == RETRY_TASK))
			goto restart;

		/* Assume the next prioritized class is idle_sched_class */
		if (!p) {
			put_prev_task(rq, prev);
			p = pick_next_task_idle(rq);
		}

		return p;
	}

restart:
	put_prev_task_balance(rq, prev, rf);

	for_each_class(class) {
		p = class->pick_next_task(rq);
		if (p)
			return p;
	}

	BUG(); /* The idle class should always have a runnable task. */
}
----

context_switch(): link:./切换.asc#过程[切换]

[[周期性调度]]
=== 周期性调度sched_tick
==== 概念
周期性调度sched_tick由内核时钟中断周期性的触发，周期性调度以固定的频率激活负责当前进程调度类的周期性调度方法，即通过调用进程所属调度类的task_tick操作完成周期性调度的通知和配置工作，设置标识TIF_NEED_RESCHED来通知内核在必要的时间由主调度函数完成真正的调度工作。

注意，sched_tick函数由scheduler_tick函数重命名而来: https://github.com/torvalds/linux/commit/86dd6c04ef9f213e14d60c9f64bce1cc019f816e

==== 流程
timer interrupt -> update_process_times() -> sched_tick() -> curr->sched_class->task_tick()

==== 实现
[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/sched/core.c
----
/*
 * This function gets called by the timer code, with HZ frequency.
 * We call it with interrupts disabled.
 */
void sched_tick(void)
{
	int cpu = smp_processor_id();
	struct rq *rq = cpu_rq(cpu);
	/* accounting goes to the donor task */
	struct task_struct *donor;
	struct rq_flags rf;
	unsigned long hw_pressure;
	u64 resched_latency;

	if (housekeeping_cpu(cpu, HK_TYPE_KERNEL_NOISE))
		arch_scale_freq_tick();

	sched_clock_tick();

	rq_lock(rq, &rf);
	donor = rq->donor;

	psi_account_irqtime(rq, donor, NULL);

	update_rq_clock(rq);
	hw_pressure = arch_scale_hw_pressure(cpu_of(rq));
	update_hw_load_avg(rq_clock_task(rq), rq, hw_pressure);

	if (dynamic_preempt_lazy() && tif_test_bit(TIF_NEED_RESCHED_LAZY))
		resched_curr(rq);

	donor->sched_class->task_tick(rq, donor, 0);
	if (sched_feat(LATENCY_WARN))
		resched_latency = cpu_resched_latency(rq);
	calc_global_load_tick(rq);
	sched_core_tick(rq);
	task_tick_mm_cid(rq, donor);
	scx_tick(rq);

	rq_unlock(rq, &rf);

	if (sched_feat(LATENCY_WARN) && resched_latency)
		resched_latency_warn(cpu, resched_latency);

	perf_event_task_tick();

	if (donor->flags & PF_WQ_WORKER)
		wq_worker_tick(donor);

#ifdef CONFIG_SMP
	if (!scx_switched_all()) {
		rq->idle_balance = idle_cpu(cpu);
		sched_balance_trigger(rq);
	}
#endif
}
----

如果是CFS调度类，对应的task_tick为task_tick_fair():

	task_tick_fair(): https://elixir.bootlin.com/linux/latest/source/kernel/sched/fair.c
		entity_tick()
			update_curr()

[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/sched/fair.c
----
static void task_tick_fair(struct rq *rq, struct task_struct *curr, int queued)
{
	struct cfs_rq *cfs_rq;
	struct sched_entity *se = &curr->se;

	for_each_sched_entity(se) {
		cfs_rq = cfs_rq_of(se);
		entity_tick(cfs_rq, se, queued);
	}

	//...
}
//...
static void
entity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr, int queued)
{
	/*
	 * Update run-time statistics of the 'current'.
	 */
	update_curr(cfs_rq);

	/*
	 * Ensure that runnable average is periodically updated.
	 */
	update_load_avg(cfs_rq, curr, UPDATE_TG);
	update_cfs_group(curr);

#ifdef CONFIG_SCHED_HRTICK
	/*
	 * queued ticks are scheduled to match the slice, so don't bother
	 * validating it and just reschedule.
	 */
	if (queued) {
		resched_curr_lazy(rq_of(cfs_rq));
		return;
	}
#endif
}
----

update_curr():
[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/sched/fair.c
----
/*
 * Update the current task's runtime statistics.
 */
static void update_curr(struct cfs_rq *cfs_rq)
{
	struct sched_entity *curr = cfs_rq->curr;
	struct rq *rq = rq_of(cfs_rq);
	s64 delta_exec;
	bool resched;

	if (unlikely(!curr))
		return;

	delta_exec = update_curr_se(rq, curr);
	if (unlikely(delta_exec <= 0))
		return;

	curr->vruntime += calc_delta_fair(delta_exec, curr);
	resched = update_deadline(cfs_rq, curr);
	update_min_vruntime(cfs_rq);

	if (entity_is_task(curr)) {
		struct task_struct *p = task_of(curr);

		update_curr_task(p, delta_exec);

		/*
		 * If the fair_server is active, we need to account for the
		 * fair_server time whether or not the task is running on
		 * behalf of fair_server or not:
		 *  - If the task is running on behalf of fair_server, we need
		 *    to limit its time based on the assigned runtime.
		 *  - Fair task that runs outside of fair_server should account
		 *    against fair_server such that it can account for this time
		 *    and possibly avoid running this period.
		 */
		if (dl_server_active(&rq->fair_server))
			dl_server_update(&rq->fair_server, delta_exec);
	}

	account_cfs_rq_runtime(cfs_rq, delta_exec);

	if (cfs_rq->nr_queued == 1)
		return;

	if (resched || did_preempt_short(cfs_rq, curr)) {
		resched_curr_lazy(rq);
		clear_buddies(cfs_rq, curr);
	}
}
----

resched_curr_lazy(): 内部调用__resched_curr()，在thread_info里面标记需要重新调度TIF_NEED_RESCHED，SMP结构下如果不是当前CPU，还可能发送一个处理器间中断，最终让对方CPU进行调度。
[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/sched/core.c
----
/*
 * resched_curr - mark rq's current task 'to be rescheduled now'.
 *
 * On UP this means the setting of the need_resched flag, on SMP it
 * might also involve a cross-CPU call to trigger the scheduler on
 * the target CPU.
 */
static void __resched_curr(struct rq *rq, int tif)
{
	struct task_struct *curr = rq->curr;
	struct thread_info *cti = task_thread_info(curr);
	int cpu;

	lockdep_assert_rq_held(rq);

	/*
	 * Always immediately preempt the idle task; no point in delaying doing
	 * actual work.
	 */
	if (is_idle_task(curr) && tif == TIF_NEED_RESCHED_LAZY)
		tif = TIF_NEED_RESCHED;

	if (cti->flags & ((1 << tif) | _TIF_NEED_RESCHED))
		return;

	cpu = cpu_of(rq);

	if (cpu == smp_processor_id()) {
		set_ti_thread_flag(cti, tif);
		if (tif == TIF_NEED_RESCHED)
			set_preempt_need_resched();
		return;
	}

	if (set_nr_and_not_polling(cti, tif)) {
		if (tif == TIF_NEED_RESCHED)
			smp_send_reschedule(cpu);
	} else {
		trace_sched_wake_idle_without_ipi(cpu);
	}
}

void resched_curr(struct rq *rq)
{
	__resched_curr(rq, TIF_NEED_RESCHED);
}
//...
void resched_curr_lazy(struct rq *rq)
{
	__resched_curr(rq, get_lazy_tif_bit());
}
----
其主要流程:

	if (cpu == smp_processor_id()): 目标CPU为当前CPU
		set_ti_thread_flag(cti, tif): 设置TIF_NEED_RESCHED
		set_preempt_need_resched(): 设置PREEMPT_NEED_RESCHED

	if (set_nr_and_not_polling(cti, tif)): 是否为IDLE线程正在做轮询
		smp_send_reschedule(cpu): 在给定CPU上触发IPI，引起scheduler_ipi被执行，间接触发preemption

=== 抢占
==== 概念
- 什么是抢占
the act of involuntarily suspending a running process is called preemption
preemption(抢占)允许满足某些条件(如优先级)的任务打断当前正在CPU上运行的任务而得到调度执行，且这种打断不需要当前正在运行的任务的配合，同时被打断的程序可以在后来再次被调度以恢复执行。

- 为什么需要抢占
抢占是为了降低延迟，更好的响应性。例如桌面系统会配置抢占，服务器系统一般不会配置抢占。
对于低调度延迟或者实时等场景，支持抢占是必须的。

- 用户抢占(user preemption)
用户抢占是指，当内核即将从内核态返回用户态时，如果发现当前进程的 TIF_NEED_RESCHED 标志被设置（表示需要重新调度），内核会调用 schedule() 函数选择一个更合适的进程来运行，从而抢占当前进程的用户态执行。
触发用户抢占的常见时机:
从系统调用返回用户态时。
从中断处理程序返回用户态时。

- 内核抢占(kernel preemption)
内核抢占是指，一个进程在内核态执行时（例如正在执行一个系统调用），可以被另一个更高优先级的进程抢占，即使当前进程的内核代码还没有执行完、还没有返回到用户态。
触发内核抢占的常见时机:
中断处理程序完成时: 这是最常见的情况。当一个中断处理程序执行完毕，在返回到被中断的内核代码之前，内核会检查是否需要调度。如果被中断的进程是可抢占的（即没有持有锁），并且有更高优先级进程就绪，就会直接发生调度。
内核代码显式释放锁时: 当进程释放一个自旋锁（spinlock）时，内核会检查是否需要调度。
内核代码主动让出CPU时: 例如调用 cond_resched()。

==== 配置
===== config
https://elixir.bootlin.com/linux/latest/source/kernel/Kconfig.preempt

    choice
        prompt "Preemption Model"
        default PREEMPT_NONE

    config PREEMPT_NONE
        bool "No Forced Preemption (Server)"
        depends on !PREEMPT_RT
        select PREEMPT_NONE_BUILD if !PREEMPT_DYNAMIC
        help
        This is the traditional Linux preemption model, geared towards
        throughput. It will still provide good latencies most of the
        time, but there are no guarantees and occasional longer delays
        are possible.

        Select this option if you are building a kernel for a server or
        scientific/computation system, or if you want to maximize the
        raw processing power of the kernel, irrespective of scheduling
        latencies.

    config PREEMPT_VOLUNTARY
        bool "Voluntary Kernel Preemption (Desktop)"
        depends on !ARCH_NO_PREEMPT
        depends on !PREEMPT_RT
        select PREEMPT_VOLUNTARY_BUILD if !PREEMPT_DYNAMIC
        help
        This option reduces the latency of the kernel by adding more
        "explicit preemption points" to the kernel code. These new
        preemption points have been selected to reduce the maximum
        latency of rescheduling, providing faster application reactions,
        at the cost of slightly lower throughput.

        This allows reaction to interactive events by allowing a
        low priority process to voluntarily preempt itself even if it
        is in kernel mode executing a system call. This allows
        applications to run more 'smoothly' even when the system is
        under load.

        Select this if you are building a kernel for a desktop system.

    config PREEMPT
        bool "Preemptible Kernel (Low-Latency Desktop)"
        depends on !ARCH_NO_PREEMPT
        select PREEMPT_BUILD if !PREEMPT_DYNAMIC
        help
        This option reduces the latency of the kernel by making
        all kernel code (that is not executing in a critical section)
        preemptible.  This allows reaction to interactive events by
        permitting a low priority process to be preempted involuntarily
        even if it is in kernel mode executing a system call and would
        otherwise not be about to reach a natural preemption point.
        This allows applications to run more 'smoothly' even when the
        system is under load, at the cost of slightly lower throughput
        and a slight runtime overhead to kernel code.

        Select this if you are building a kernel for a desktop or
        embedded system with latency requirements in the milliseconds
        range.

    config PREEMPT_LAZY
        bool "Scheduler controlled preemption model"
        depends on !ARCH_NO_PREEMPT
        depends on ARCH_HAS_PREEMPT_LAZY
        select PREEMPT_BUILD if !PREEMPT_DYNAMIC
        help
        This option provides a scheduler driven preemption model that
        is fundamentally similar to full preemption, but is less
        eager to preempt SCHED_NORMAL tasks in an attempt to
        reduce lock holder preemption and recover some of the performance
        gains seen from using Voluntary preemption.

    endchoice

    config PREEMPT_RT
        bool "Fully Preemptible Kernel (Real-Time)"
        depends on EXPERT && ARCH_SUPPORTS_RT && !COMPILE_TEST
        select PREEMPTION
        help
        This option turns the kernel into a real-time kernel by replacing
        various locking primitives (spinlocks, rwlocks, etc.) with
        preemptible priority-inheritance aware variants, enforcing
        interrupt threading and introducing mechanisms to break up long
        non-preemptible sections. This makes the kernel, except for very
        low level and critical code paths (entry code, scheduler, low
        level interrupt handling) fully preemptible and brings most
        execution contexts under scheduler control.

        Select this if you are building a kernel for systems which
        require real-time guarantees.

===== CONFIG_PREEMPT_NONE
不允许内核抢占，一般用于server系统。

===== CONFIG_PREEMPT_VOLUNTARY
在一些耗时较长的内核代码中主动调用cond_resched()让出CPU，对吞吐量有轻微影响，但是响应会稍微快些。

===== CONFIG_PREEMPT
除了处于持有spinlock时的critical section，其它时候都允许内核抢占，响应速度进一步提升，吞吐量进一步下降，一般用于desktop/embedded系统。

要不处于临界区（主要是由自旋锁保护的区域），更高优先级的用户任务可以立即抢占当前的内核任务。
自旋锁是抢占边界。当一个内核线程持有自旋锁时，它是不可抢占的。这是导致 PREEMPT 模式延迟不确定性的主要来源。

===== PREEMPT_LAZY
在 PREEMPT 模式下，为了保持内核数据结构的完整性，内核使用了自旋锁。当一个线程持有自旋锁时，它是不可抢占的。如果有一个高优先级任务需要运行，但当前持有锁的是一个低优先级任务，高优先级任务就必须自旋等待（忙等），这会导致严重的优先级反转和延迟。

PREEMPT_LAZY: 当内核线程持有自旋锁时，如果有一个高优先级任务就绪了，系统不会立即抢占当前任务。相反，它会设置一个需要抢占的延迟标志（TIF_NEED_RESCHED_LAZY），然后允许当前任务继续执行。

抢占时机: 真正的抢占被推迟到当前任务释放自旋锁的那一刻。在释放锁的代码路径中，会检查这个延迟标志，如果被设置，则会在锁释放后立即发生调度，让高优先级任务运行。

===== CONFIG_PREEMPT_RT
spinlock换成了preemptable mutex，只剩下一些极其核心的地方仍然用禁止抢占的spinlock，基本可以认为是随时可被抢占。

- 工作原理
对内核进行深度改造，几乎消除了所有不可抢占的区域:
** 将自旋锁替换为可抢占的互斥锁
这是最核心的改变。大部分自旋锁被转换成可以睡眠的互斥锁，这意味着高优先级任务无需等待低优先级任务释放锁，而是可以抢占它。
** 中断线程化
将几乎所有的中断处理程序（ISR）变成内核线程，这样，中断可以被更高优先级的线程抢占，大大减少了中断对系统延迟的冲击。
** 更多代码路径变为可抢占

- 关键特性
极大地减少了由于锁导致的优先级反转问题。
提供了极低且可预测的延迟（通常在微秒级别）。

- 代价
降低了系统的整体吞吐量。
增加了内核的复杂性和开销。

==== 开关与preemptible()
preempt_enable()与preempt_disable():
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/preempt.h
----
#ifdef CONFIG_PREEMPT_COUNT

#define preempt_disable() \
do { \
	preempt_count_inc(); \
	barrier(); \
} while (0)
//...
#define preemptible()	(preempt_count() == 0 && !irqs_disabled())

#ifdef CONFIG_PREEMPTION
#define preempt_enable() \
do { \
	barrier(); \
	if (unlikely(preempt_count_dec_and_test())) \
		__preempt_schedule(); \
} while (0)
//...
#else /* !CONFIG_PREEMPTION */
#define preempt_enable() \
do { \
	barrier(); \
	preempt_count_dec(); \
} while (0)
#endif /* CONFIG_PREEMPTION */
//...
#else /* !CONFIG_PREEMPT_COUNT */

/*
 * Even if we don't have any preemption, we need preempt disable/enable
 * to be barriers, so that we don't have things like get_user/put_user
 * that can cause faults and scheduling migrate into our preempt-protected
 * region.
 */
#define preempt_disable()			barrier()
//...
#define preempt_enable()			barrier()
//...
#define preemptible()				0

#endif /* CONFIG_PREEMPT_COUNT */
----

==== preempt_count变量与函数
preempt_count是一个32bit的变量。
对于x86，是一个per-cpu变量，对于ARM和通用平台，通常保存在current_thread_info中。

▪ x86:
[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/preempt.h
----
DECLARE_PER_CPU_CACHE_HOT(int, __preempt_count);
//...
static __always_inline int preempt_count(void)
{
	return raw_cpu_read_4(__preempt_count) & ~PREEMPT_NEED_RESCHED;
}
----

▪ ARM64:
[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/arm64/include/asm/preempt.h
----
static inline int preempt_count(void)
{
	return READ_ONCE(current_thread_info()->preempt.count);
}
----

▪ generic:
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/asm-generic/preempt.h
----
static __always_inline int preempt_count(void)
{
	return READ_ONCE(current_thread_info()->preempt_count);
}
----

▪ 32位组成:
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/preempt.h
----
/*
 * We put the hardirq and softirq counter into the preemption
 * counter. The bitmask has the following meaning:
 *
 * - bits 0-7 are the preemption count (max preemption depth: 256)
 * - bits 8-15 are the softirq count (max # of softirqs: 256)
 *
 * The hardirq count could in theory be the same as the number of
 * interrupts in the system, but we run all interrupt handlers with
 * interrupts disabled, so we cannot have nesting interrupts. Though
 * there are a few palaeontologic drivers which reenable interrupts in
 * the handler, so we need more than one bit here.
 *
 *         PREEMPT_MASK:	0x000000ff
 *         SOFTIRQ_MASK:	0x0000ff00
 *         HARDIRQ_MASK:	0x000f0000
 *             NMI_MASK:	0x00f00000
 * PREEMPT_NEED_RESCHED:	0x80000000
 */
----

参考: https://lwn.net/Articles/831678/
参考: https://zhuanlan.zhihu.com/p/88883239

==== preempt_schedule()
▪ generic:
[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/asm-generic/preempt.h
----
#ifdef CONFIG_PREEMPTION
extern asmlinkage void preempt_schedule(void);
#define __preempt_schedule() preempt_schedule()
extern asmlinkage void preempt_schedule_notrace(void);
#define __preempt_schedule_notrace() preempt_schedule_notrace()
#endif /* CONFIG_PREEMPTION */
----

▪ x86:
[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/preempt.h
----
#ifdef CONFIG_PREEMPTION

extern asmlinkage void preempt_schedule(void);
extern asmlinkage void preempt_schedule_thunk(void);

#define preempt_schedule_dynamic_enabled	preempt_schedule_thunk
#define preempt_schedule_dynamic_disabled	NULL

extern asmlinkage void preempt_schedule_notrace(void);
extern asmlinkage void preempt_schedule_notrace_thunk(void);

#define preempt_schedule_notrace_dynamic_enabled	preempt_schedule_notrace_thunk
#define preempt_schedule_notrace_dynamic_disabled	NULL

#ifdef CONFIG_PREEMPT_DYNAMIC

DECLARE_STATIC_CALL(preempt_schedule, preempt_schedule_dynamic_enabled);

#define __preempt_schedule() \
do { \
	__STATIC_CALL_MOD_ADDRESSABLE(preempt_schedule); \
	asm volatile ("call " STATIC_CALL_TRAMP_STR(preempt_schedule) : ASM_CALL_CONSTRAINT); \
} while (0)

DECLARE_STATIC_CALL(preempt_schedule_notrace, preempt_schedule_notrace_dynamic_enabled);

#define __preempt_schedule_notrace() \
do { \
	__STATIC_CALL_MOD_ADDRESSABLE(preempt_schedule_notrace); \
	asm volatile ("call " STATIC_CALL_TRAMP_STR(preempt_schedule_notrace) : ASM_CALL_CONSTRAINT); \
} while (0)

#else /* PREEMPT_DYNAMIC */

#define __preempt_schedule() \
	asm volatile ("call preempt_schedule_thunk" : ASM_CALL_CONSTRAINT);

#define __preempt_schedule_notrace() \
	asm volatile ("call preempt_schedule_notrace_thunk" : ASM_CALL_CONSTRAINT);

#endif /* PREEMPT_DYNAMIC */

#endif /* PREEMPTION */
----

▪ ARM64:
[source, c]
.https://elixir.bootlin.com/linux/latest/source/arch/arm64/include/asm/preempt.h
----
#ifdef CONFIG_PREEMPTION

void preempt_schedule(void);
void preempt_schedule_notrace(void);

#ifdef CONFIG_PREEMPT_DYNAMIC

DECLARE_STATIC_KEY_TRUE(sk_dynamic_irqentry_exit_cond_resched);
void dynamic_preempt_schedule(void);
#define __preempt_schedule()		dynamic_preempt_schedule()
void dynamic_preempt_schedule_notrace(void);
#define __preempt_schedule_notrace()	dynamic_preempt_schedule_notrace()

#else /* CONFIG_PREEMPT_DYNAMIC */

#define __preempt_schedule()		preempt_schedule()
#define __preempt_schedule_notrace()	preempt_schedule_notrace()

#endif /* CONFIG_PREEMPT_DYNAMIC */
#endif /* CONFIG_PREEMPTION */
----

可见对于__preempt_schedule()，x86进行了重载，其他体系结构则使用preempt_schedule()

▪ 通用:
[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/sched/core.c
----
static void __sched notrace preempt_schedule_common(void)
{
	do {
		/*
		 * Because the function tracer can trace preempt_count_sub()
		 * and it also uses preempt_enable/disable_notrace(), if
		 * NEED_RESCHED is set, the preempt_enable_notrace() called
		 * by the function tracer will call this function again and
		 * cause infinite recursion.
		 *
		 * Preemption must be disabled here before the function
		 * tracer can trace. Break up preempt_disable() into two
		 * calls. One to disable preemption without fear of being
		 * traced. The other to still record the preemption latency,
		 * which can also be traced by the function tracer.
		 */
		preempt_disable_notrace();
		preempt_latency_start(1);
		__schedule(SM_PREEMPT);
		preempt_latency_stop(1);
		preempt_enable_no_resched_notrace();

		/*
		 * Check again in case we missed a preemption opportunity
		 * between schedule and now.
		 */
	} while (need_resched());
}

#ifdef CONFIG_PREEMPTION
/*
 * This is the entry point to schedule() from in-kernel preemption
 * off of preempt_enable.
 */
asmlinkage __visible void __sched notrace preempt_schedule(void)
{
	/*
	 * If there is a non-zero preempt_count or interrupts are disabled,
	 * we do not want to preempt the current task. Just return..
	 */
	if (likely(!preemptible()))
		return;
	preempt_schedule_common();
}
NOKPROBE_SYMBOL(preempt_schedule);
EXPORT_SYMBOL(preempt_schedule);

#ifdef CONFIG_PREEMPT_DYNAMIC
#if defined(CONFIG_HAVE_PREEMPT_DYNAMIC_CALL)
#ifndef preempt_schedule_dynamic_enabled
#define preempt_schedule_dynamic_enabled	preempt_schedule
#define preempt_schedule_dynamic_disabled	NULL
#endif
DEFINE_STATIC_CALL(preempt_schedule, preempt_schedule_dynamic_enabled);
EXPORT_STATIC_CALL_TRAMP(preempt_schedule);
#elif defined(CONFIG_HAVE_PREEMPT_DYNAMIC_KEY)
static DEFINE_STATIC_KEY_TRUE(sk_dynamic_preempt_schedule);
void __sched notrace dynamic_preempt_schedule(void)
{
	if (!static_branch_unlikely(&sk_dynamic_preempt_schedule))
		return;
	preempt_schedule();
}
NOKPROBE_SYMBOL(dynamic_preempt_schedule);
EXPORT_SYMBOL(dynamic_preempt_schedule);
#endif
#endif

/**
 * preempt_schedule_notrace - preempt_schedule called by tracing
 *
 * The tracing infrastructure uses preempt_enable_notrace to prevent
 * recursion and tracing preempt enabling caused by the tracing
 * infrastructure itself. But as tracing can happen in areas coming
 * from userspace or just about to enter userspace, a preempt enable
 * can occur before user_exit() is called. This will cause the scheduler
 * to be called when the system is still in usermode.
 *
 * To prevent this, the preempt_enable_notrace will use this function
 * instead of preempt_schedule() to exit user context if needed before
 * calling the scheduler.
 */
asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)
{
	enum ctx_state prev_ctx;

	if (likely(!preemptible()))
		return;

	do {
		/*
		 * Because the function tracer can trace preempt_count_sub()
		 * and it also uses preempt_enable/disable_notrace(), if
		 * NEED_RESCHED is set, the preempt_enable_notrace() called
		 * by the function tracer will call this function again and
		 * cause infinite recursion.
		 *
		 * Preemption must be disabled here before the function
		 * tracer can trace. Break up preempt_disable() into two
		 * calls. One to disable preemption without fear of being
		 * traced. The other to still record the preemption latency,
		 * which can also be traced by the function tracer.
		 */
		preempt_disable_notrace();
		preempt_latency_start(1);
		/*
		 * Needs preempt disabled in case user_exit() is traced
		 * and the tracer calls preempt_enable_notrace() causing
		 * an infinite recursion.
		 */
		prev_ctx = exception_enter();
		__schedule(SM_PREEMPT);
		exception_exit(prev_ctx);

		preempt_latency_stop(1);
		preempt_enable_no_resched_notrace();
	} while (need_resched());
}
----

▪ x86:

==== Tick Preemption
参考: <<周期性调度, 周期性调度>>

==== Wakeup Preemption
很多同步原语都会触发唤醒:
如semaphore，mutex，futex等锁退出时，会调用wake_up_process()/wake_up_state()来唤醒等待该锁的任务列表里的第一个等待任务。
等待队列(wait queue)/completion的机制里，也会唤醒其它等待在指定等待队列或者completion上的一个或者多个其它任务。

这些唤醒任务最终都会进入一个共同的入口: try_to_wake_up()
[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/sched/core.c
----
/*
 * Notes on Program-Order guarantees on SMP systems.
 *
 *  MIGRATION
 *
 * The basic program-order guarantee on SMP systems is that when a task [t]
 * migrates, all its activity on its old CPU [c0] happens-before any subsequent
 * execution on its new CPU [c1].
 *
 * For migration (of runnable tasks) this is provided by the following means:
 *
 *  A) UNLOCK of the rq(c0)->lock scheduling out task t
 *  B) migration for t is required to synchronize *both* rq(c0)->lock and
 *     rq(c1)->lock (if not at the same time, then in that order).
 *  C) LOCK of the rq(c1)->lock scheduling in task
 *
 * Release/acquire chaining guarantees that B happens after A and C after B.
 * Note: the CPU doing B need not be c0 or c1
 *
 * Example:
 *
 *   CPU0            CPU1            CPU2
 *
 *   LOCK rq(0)->lock
 *   sched-out X
 *   sched-in Y
 *   UNLOCK rq(0)->lock
 *
 *                                   LOCK rq(0)->lock // orders against CPU0
 *                                   dequeue X
 *                                   UNLOCK rq(0)->lock
 *
 *                                   LOCK rq(1)->lock
 *                                   enqueue X
 *                                   UNLOCK rq(1)->lock
 *
 *                   LOCK rq(1)->lock // orders against CPU2
 *                   sched-out Z
 *                   sched-in X
 *                   UNLOCK rq(1)->lock
 *
 *
 *  BLOCKING -- aka. SLEEP + WAKEUP
 *
 * For blocking we (obviously) need to provide the same guarantee as for
 * migration. However the means are completely different as there is no lock
 * chain to provide order. Instead we do:
 *
 *   1) smp_store_release(X->on_cpu, 0)   -- finish_task()
 *   2) smp_cond_load_acquire(!X->on_cpu) -- try_to_wake_up()
 *
 * Example:
 *
 *   CPU0 (schedule)  CPU1 (try_to_wake_up) CPU2 (schedule)
 *
 *   LOCK rq(0)->lock LOCK X->pi_lock
 *   dequeue X
 *   sched-out X
 *   smp_store_release(X->on_cpu, 0);
 *
 *                    smp_cond_load_acquire(&X->on_cpu, !VAL);
 *                    X->state = WAKING
 *                    set_task_cpu(X,2)
 *
 *                    LOCK rq(2)->lock
 *                    enqueue X
 *                    X->state = RUNNING
 *                    UNLOCK rq(2)->lock
 *
 *                                          LOCK rq(2)->lock // orders against CPU1
 *                                          sched-out Z
 *                                          sched-in X
 *                                          UNLOCK rq(2)->lock
 *
 *                    UNLOCK X->pi_lock
 *   UNLOCK rq(0)->lock
 *
 *
 * However, for wakeups there is a second guarantee we must provide, namely we
 * must ensure that CONDITION=1 done by the caller can not be reordered with
 * accesses to the task state; see try_to_wake_up() and set_current_state().
 */

/**
 * try_to_wake_up - wake up a thread
 * @p: the thread to be awakened
 * @state: the mask of task states that can be woken
 * @wake_flags: wake modifier flags (WF_*)
 *
 * Conceptually does:
 *
 *   If (@state & @p->state) @p->state = TASK_RUNNING.
 *
 * If the task was not queued/runnable, also place it back on a runqueue.
 *
 * This function is atomic against schedule() which would dequeue the task.
 *
 * It issues a full memory barrier before accessing @p->state, see the comment
 * with set_current_state().
 *
 * Uses p->pi_lock to serialize against concurrent wake-ups.
 *
 * Relies on p->pi_lock stabilizing:
 *  - p->sched_class
 *  - p->cpus_ptr
 *  - p->sched_task_group
 * in order to do migration, see its use of select_task_rq()/set_task_cpu().
 *
 * Tries really hard to only take one task_rq(p)->lock for performance.
 * Takes rq->lock in:
 *  - ttwu_runnable()    -- old rq, unavoidable, see comment there;
 *  - ttwu_queue()       -- new rq, for enqueue of the task;
 *  - psi_ttwu_dequeue() -- much sadness :-( accounting will kill us.
 *
 * As a consequence we race really badly with just about everything. See the
 * many memory barriers and their comments for details.
 *
 * Return: %true if @p->state changes (an actual wakeup was done),
 *	   %false otherwise.
 */
int try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)
{
	guard(preempt)();
	int cpu, success = 0;

	wake_flags |= WF_TTWU;

	if (p == current) {
		/*
		 * We're waking current, this means 'p->on_rq' and 'task_cpu(p)
		 * == smp_processor_id()'. Together this means we can special
		 * case the whole 'p->on_rq && ttwu_runnable()' case below
		 * without taking any locks.
		 *
		 * Specifically, given current runs ttwu() we must be before
		 * schedule()'s block_task(), as such this must not observe
		 * sched_delayed.
		 *
		 * In particular:
		 *  - we rely on Program-Order guarantees for all the ordering,
		 *  - we're serialized against set_special_state() by virtue of
		 *    it disabling IRQs (this allows not taking ->pi_lock).
		 */
		WARN_ON_ONCE(p->se.sched_delayed);
		if (!ttwu_state_match(p, state, &success))
			goto out;

		trace_sched_waking(p);
		ttwu_do_wakeup(p);
		goto out;
	}

	/*
	 * If we are going to wake up a thread waiting for CONDITION we
	 * need to ensure that CONDITION=1 done by the caller can not be
	 * reordered with p->state check below. This pairs with smp_store_mb()
	 * in set_current_state() that the waiting thread does.
	 */
	scoped_guard (raw_spinlock_irqsave, &p->pi_lock) {
		smp_mb__after_spinlock();
		if (!ttwu_state_match(p, state, &success))
			break;

		trace_sched_waking(p);

		/*
		 * Ensure we load p->on_rq _after_ p->state, otherwise it would
		 * be possible to, falsely, observe p->on_rq == 0 and get stuck
		 * in smp_cond_load_acquire() below.
		 *
		 * sched_ttwu_pending()			try_to_wake_up()
		 *   STORE p->on_rq = 1			  LOAD p->state
		 *   UNLOCK rq->lock
		 *
		 * __schedule() (switch to task 'p')
		 *   LOCK rq->lock			  smp_rmb();
		 *   smp_mb__after_spinlock();
		 *   UNLOCK rq->lock
		 *
		 * [task p]
		 *   STORE p->state = UNINTERRUPTIBLE	  LOAD p->on_rq
		 *
		 * Pairs with the LOCK+smp_mb__after_spinlock() on rq->lock in
		 * __schedule().  See the comment for smp_mb__after_spinlock().
		 *
		 * A similar smp_rmb() lives in __task_needs_rq_lock().
		 */
		smp_rmb();
		if (READ_ONCE(p->on_rq) && ttwu_runnable(p, wake_flags))
			break;

#ifdef CONFIG_SMP
		/*
		 * Ensure we load p->on_cpu _after_ p->on_rq, otherwise it would be
		 * possible to, falsely, observe p->on_cpu == 0.
		 *
		 * One must be running (->on_cpu == 1) in order to remove oneself
		 * from the runqueue.
		 *
		 * __schedule() (switch to task 'p')	try_to_wake_up()
		 *   STORE p->on_cpu = 1		  LOAD p->on_rq
		 *   UNLOCK rq->lock
		 *
		 * __schedule() (put 'p' to sleep)
		 *   LOCK rq->lock			  smp_rmb();
		 *   smp_mb__after_spinlock();
		 *   STORE p->on_rq = 0			  LOAD p->on_cpu
		 *
		 * Pairs with the LOCK+smp_mb__after_spinlock() on rq->lock in
		 * __schedule().  See the comment for smp_mb__after_spinlock().
		 *
		 * Form a control-dep-acquire with p->on_rq == 0 above, to ensure
		 * schedule()'s deactivate_task() has 'happened' and p will no longer
		 * care about it's own p->state. See the comment in __schedule().
		 */
		smp_acquire__after_ctrl_dep();

		/*
		 * We're doing the wakeup (@success == 1), they did a dequeue (p->on_rq
		 * == 0), which means we need to do an enqueue, change p->state to
		 * TASK_WAKING such that we can unlock p->pi_lock before doing the
		 * enqueue, such as ttwu_queue_wakelist().
		 */
		WRITE_ONCE(p->__state, TASK_WAKING);

		/*
		 * If the owning (remote) CPU is still in the middle of schedule() with
		 * this task as prev, considering queueing p on the remote CPUs wake_list
		 * which potentially sends an IPI instead of spinning on p->on_cpu to
		 * let the waker make forward progress. This is safe because IRQs are
		 * disabled and the IPI will deliver after on_cpu is cleared.
		 *
		 * Ensure we load task_cpu(p) after p->on_cpu:
		 *
		 * set_task_cpu(p, cpu);
		 *   STORE p->cpu = @cpu
		 * __schedule() (switch to task 'p')
		 *   LOCK rq->lock
		 *   smp_mb__after_spin_lock()		smp_cond_load_acquire(&p->on_cpu)
		 *   STORE p->on_cpu = 1		LOAD p->cpu
		 *
		 * to ensure we observe the correct CPU on which the task is currently
		 * scheduling.
		 */
		if (smp_load_acquire(&p->on_cpu) &&
		    ttwu_queue_wakelist(p, task_cpu(p), wake_flags))
			break;

		/*
		 * If the owning (remote) CPU is still in the middle of schedule() with
		 * this task as prev, wait until it's done referencing the task.
		 *
		 * Pairs with the smp_store_release() in finish_task().
		 *
		 * This ensures that tasks getting woken will be fully ordered against
		 * their previous state and preserve Program Order.
		 */
		smp_cond_load_acquire(&p->on_cpu, !VAL);

		cpu = select_task_rq(p, p->wake_cpu, &wake_flags);
		if (task_cpu(p) != cpu) {
			if (p->in_iowait) {
				delayacct_blkio_end(p);
				atomic_dec(&task_rq(p)->nr_iowait);
			}

			wake_flags |= WF_MIGRATED;
			psi_ttwu_dequeue(p);
			set_task_cpu(p, cpu);
		}
#else
		cpu = task_cpu(p);
#endif /* CONFIG_SMP */

		ttwu_queue(p, cpu, wake_flags);
	}
out:
	if (success)
		ttwu_stat(p, task_cpu(p), wake_flags);

	return success;
}
----

步骤 1: 状态检查与状态转换
快速检查，避免不必要的唤醒操作。
检查 p->state：函数会检查目标进程 p 的当前状态是否包含在调用者传入的 state 参数中（例如 TASK_INTERRUPTIBLE）。如果进程的状态已经改变（比如已经被其它路径唤醒），函数会立即失败返回。
如果状态匹配，函数会尝试将 p->state 从 TASK_INTERRUPTIBLE 等状态设置为 TASK_RUNNING。

步骤 2: 选择目标运行队列
select_task_rq(p, p->wake_cpu, &wake_flags) 决定了唤醒的进程应该被放到哪个CPU的运行队列上。
选择策略：
缓存亲和性：首选进程最后运行的那个CPU（p->wake_cpu）。因为该CPU的缓存中可能还存有该进程的数据，在其上运行可以获得最好的性能。
负载均衡：如果首选CPU的负载过高，或者由于调度域的限制，调度器会选择一个负载较轻且符合要求的其他CPU。
唤醒标志：wake_flags 如 WF_SYNC（同步唤醒）等会影响选择策略。

步骤 3: 核心唤醒与调度触发
这是实际将进程加入运行队列并可能触发抢占的逻辑，由 ttwu_queue(p, cpu, wake_flags) 完成，它进一步调用 ttwu_do_activate。
激活任务：将进程的状态正式激活，并将其加入到步骤2中选择的目标CPU的运行队列中。
检查抢占：在进程入队后，函数会调用 check_preempt_curr：
    它比较被唤醒的进程 p 和当前正在该CPU上运行的进程 curr 的优先级。
    如果 p 的优先级高于 curr，则会设置当前进程的 TIF_NEED_RESCHED 标志。
触发调度：如果设置了 TIF_NEED_RESCHED 标志，并且当前CPU不在临界区内（preempt_count == 0），try_to_wake_up 会通过 ttwu_do_wakeup 最终可能触发一个 IPI（处理器间中断） 发送到目标CPU（如果 p 和 curr 不在同一CPU上），强制该CPU尽快进行调度。这样，高优先级的进程就能几乎立即被调度执行。

==== 不可抢占内核的低延迟处理
一些比较耗时的处理中如文件系统和内存回收的一些路径会调用cond_resched()。
抢占式内核中(CONFIG_PREEMPTION=y)cond_resched宏的_cond_resched为空，并没有主动判断重新调度的功能，只有非抢占式内核才会调用_cond_resched来执行主动检查可抢占性。

cond_resched():
在不可抢占内核中： 在 CONFIG_PREEMPT=n（不可抢占）的内核中，这是唯一能在长的、不睡眠的内核循环中触发调度的机制。开发者必须手动在循环中插入它。
在可抢占内核中： 在 CONFIG_PREEMPT_VOLUNTARY=y 的内核中，它仍然非常有用，可以作为对完全自动抢占的一种低开销补充。但在 CONFIG_PREEMPT 全可抢占内核中，它的作用变小了，因为调度器可以在几乎所有地方自动介入。

[source, c]
.https://elixir.bootlin.com/linux/latest/source/include/linux/sched.h
----
#if !defined(CONFIG_PREEMPTION) || defined(CONFIG_PREEMPT_DYNAMIC)
extern int __cond_resched(void);

#if defined(CONFIG_PREEMPT_DYNAMIC) && defined(CONFIG_HAVE_PREEMPT_DYNAMIC_CALL)

DECLARE_STATIC_CALL(cond_resched, __cond_resched);

static __always_inline int _cond_resched(void)
{
	return static_call_mod(cond_resched)();
}

#elif defined(CONFIG_PREEMPT_DYNAMIC) && defined(CONFIG_HAVE_PREEMPT_DYNAMIC_KEY)

extern int dynamic_cond_resched(void);

static __always_inline int _cond_resched(void)
{
	return dynamic_cond_resched();
}

#else /* !CONFIG_PREEMPTION */

static inline int _cond_resched(void)
{
	return __cond_resched();
}

#endif /* PREEMPT_DYNAMIC && CONFIG_HAVE_PREEMPT_DYNAMIC_CALL */

#else /* CONFIG_PREEMPTION && !CONFIG_PREEMPT_DYNAMIC */

static inline int _cond_resched(void)
{
	return 0;
}

#endif /* !CONFIG_PREEMPTION || CONFIG_PREEMPT_DYNAMIC */

#define cond_resched() ({			\
	__might_resched(__FILE__, __LINE__, 0);	\
	_cond_resched();			\
})
----
也就是说，在CONFIG_PREEMPTION && !CONFIG_PREEMPT_DYNAMIC的情况下， 是空实现；否则，最终都是调用__cond_resched()函数:

[source, c]
.https://elixir.bootlin.com/linux/latest/source/kernel/sched/core.c
----
#if !defined(CONFIG_PREEMPTION) || defined(CONFIG_PREEMPT_DYNAMIC)
int __sched __cond_resched(void)
{
	if (should_resched(0) && !irqs_disabled()) {
		preempt_schedule_common();
		return 1;
	}
	/*
	 * In PREEMPT_RCU kernels, ->rcu_read_lock_nesting tells the tick
	 * whether the current CPU is in an RCU read-side critical section,
	 * so the tick can report quiescent states even for CPUs looping
	 * in kernel context.  In contrast, in non-preemptible kernels,
	 * RCU readers leave no in-memory hints, which means that CPU-bound
	 * processes executing in kernel context might never report an
	 * RCU quiescent state.  Therefore, the following code causes
	 * cond_resched() to report a quiescent state, but only when RCU
	 * is in urgent need of one.
	 * A third case, preemptible, but non-PREEMPT_RCU provides for
	 * urgently needed quiescent states via rcu_flavor_sched_clock_irq().
	 */
#ifndef CONFIG_PREEMPT_RCU
	rcu_all_qs();
#endif
	return 0;
}
EXPORT_SYMBOL(__cond_resched);
#endif
----
preempt_schedule_common(): 执行抢占调度
rcu_all_qs(): 在可能的情况下，向RCU（Read-Copy-Update）子系统报告一个静默状态，这有助于RCU垃圾回收，减少系统延迟。

==== 参考
https://lwn.net/Kernel/Index/#Preemption
https://zhuanlan.zhihu.com/p/339378819

=== 均衡
个体均衡: 选择到一个相对清闲的CPU上去运行。
总体均衡: 从其它CPU上拉取一些进程到自己这来运行。

个体均衡的时机：
新进程刚创建时
进程要执行新程序时
进程被唤醒时
在这几个时机上进程可以选择去哪个CPU的运行队列上去等待执行。

总体均衡的时机：
CPU即将idle前会去找到最忙的CPU然后拉取一些任务过来；
定时器中断的周期性检测是否所有的CPU的忙闲成都，如果差别太大就会迁移进程；
在idle进程中如果CPU发现自己太忙而有的CPU在idle就会将其唤醒进行负载均衡。

load_balance(): https://elixir.bootlin.com/linux/latest/source/kernel/sched/fair.c

scheduler_tick(): https://elixir.bootlin.com/linux/latest/source/kernel/sched/core.c

https://docs.kernel.org/scheduler/sched-domains.html
https://zhuanlan.zhihu.com/p/499169668

=== 亲和性
系统调用:
https://man7.org/linux/man-pages/man2/sched_setaffinity.2.html
int sched_setaffinity(pid_t pid, size_t cpusetsize, const cpu_set_t *mask);
int sched_getaffinity(pid_t pid, size_t cpusetsize, cpu_set_t *mask);

实现:

    SYSCALL_DEFINE3(sched_setaffinity: https://elixir.bootlin.com/linux/latest/source/kernel/sched/core.c
        sched_setaffinity(pid, new_mask)
            __sched_setaffinity()
                __set_cpus_allowed_ptr()
                    __set_cpus_allowed_ptr_locked()

=== Real Time
https://kernelnewbies.org/Linux_6.12#Real_Time_support
https://lwn.net/Articles/146861/
https://wiki.linuxfoundation.org/realtime/start

=== v0.12
https://elixir.bootlin.com/linux/0.12/source/kernel/sched.c

=== 参考
https://github.com/torvalds/linux/tree/master/Documentation/scheduler
https://docs.kernel.org/scheduler/index.html
https://github.com/oska874/process-scheduling-in-linux/blob/master/linux_scheduler_notes_final.pdf
https://zhuanlan.zhihu.com/p/548740742
https://trepo.tuni.fi/bitstream/handle/10024/96864/GRADU-1428493916.pdf
https://z.itpub.net/article/detail/4D5A28D92B6F801DF6E4C7F793969996
https://lwn.net/Kernel/Index/#Realtime
https://lwn.net/Kernel/Index/#Scheduler
https://students.mimuw.edu.pl/ZSO/Wyklady/14_CPUschedulers1/ProcessScheduling1.pdf
https://students.mimuw.edu.pl/ZSO/Wyklady/15_CPUschedulers2/ProcessScheduling2.pdf